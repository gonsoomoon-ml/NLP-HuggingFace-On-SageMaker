{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Module 2.2] 로컬 모드 및 스크립트 모드로 훈련 (SageMaker 사용)\n",
    "\n",
    "### 본 워크샵의 모든 노트북은 `conda_python3` 여기에서 작업 합니다.\n",
    "\n",
    "이 노트북은 아래와 같은 작업을 합니다.\n",
    "- 1. 환경 셋업\n",
    "- 2. 세이지 메이크 로컬 모드 훈련\n",
    "- 3. SageMaker Host Mode 로 훈련\n",
    "- 4. 실험 결과 보기\n",
    "- 5. 모델 아티펙트 저장\n",
    "\n",
    "---\n",
    "\n",
    "참고:\n",
    "\n",
    "- 세이지 메이커로 파이토치 사용 \n",
    "    - [Use PyTorch with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html)\n",
    "\n",
    "\n",
    "- Use PyTorch with the SageMaker Python SDK\n",
    "    - https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html\n",
    "\n",
    "\n",
    "- Amazon SageMaker Local Mode Examples\n",
    "    - TF, Pytorch, SKLean, SKLearn Processing JOb에 대한 로컬 모드 샘플\n",
    "        - https://github.com/aws-samples/amazon-sagemaker-local-mode\n",
    "    - Pytorch 로컬 모드\n",
    "        - https://github.com/aws-samples/amazon-sagemaker-local-mode/blob/main/pytorch_script_mode_local_training_and_serving/pytorch_script_mode_local_training_and_serving.py    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 환경 셋업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 세팅\n",
    "사용하는 패키지는 import 시점에 다시 재로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 커스텀 라이브러리\n",
    "import config "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "버킷 및 폴더(prefix) 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "use_default_bucket = True\n",
    "if use_default_bucket:\n",
    "    bucket = sagemaker_session.default_bucket()\n",
    "else:\n",
    "    bucket = '<Type your bucket>'\n",
    "    \n",
    "prefix = \"KoElectra-HF\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 세이지 메이크 로컬 모드 훈련\n",
    "#### 로컬의 GPU, CPU 여부로 instance_type 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance type = local_gpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    if subprocess.call(\"nvidia-smi\") == 0:\n",
    "        ## Set type to GPU if one is present\n",
    "        instance_type = \"local_gpu\"\n",
    "    else:\n",
    "        instance_type = \"local\"        \n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Instance type = \" + instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. 스크립트 모드의 코드 작성 방법\n",
    "- ![script_mode_example](img/script_mode_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.훈련 코드 확인\n",
    "- 아래의 코드는 전형적인 스크립트 모드의 코드 작성 방법을 따르고 있습니다.\n",
    "- 훈련 함수는 `from train_lib import train` 로서 이전 노트북의 **[세이지 메이커 없이]** 작성한 스크래치 버전에서 사용한 훈련 함수와 동일 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtrain_lib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m train\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparser_args\u001b[39;49;00m():\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# Default Setting\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m32\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--eval_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m128\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m256\u001b[39;49;00m)    \n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m5e-5\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--warmup_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m0\u001b[39;49;00m)    \n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--fp16\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[34mlambda\u001b[39;49;00m x: (\u001b[36mstr\u001b[39;49;00m(x).lower() == \u001b[33m'\u001b[39;49;00m\u001b[33mtrue\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), default=\u001b[34mTrue\u001b[39;49;00m)    \n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--tokenizer_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mmonologg/koelectra-small-v3-discriminator\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mmonologg/koelectra-small-v3-discriminator\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# SageMaker Container environment\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output_data_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_data_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--checkpoint_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/checkpoints\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)   \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--is_evaluation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[34mlambda\u001b[39;49;00m x: (\u001b[36mstr\u001b[39;49;00m(x).lower() == \u001b[33m'\u001b[39;49;00m\u001b[33mtrue\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), default=\u001b[34mTrue\u001b[39;49;00m)    \n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--eval_ratio\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.2\u001b[39;49;00m)       \n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--is_test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[34mlambda\u001b[39;49;00m x: (\u001b[36mstr\u001b[39;49;00m(x).lower() == \u001b[33m'\u001b[39;49;00m\u001b[33mtrue\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), default=\u001b[34mTrue\u001b[39;49;00m) \n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test_data_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])    \n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--use_subset_train_sampler\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, default=\u001b[34mTrue\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[34mlambda\u001b[39;49;00m x: (\u001b[36mstr\u001b[39;49;00m(x).lower() == \u001b[33m'\u001b[39;49;00m\u001b[33mtrue\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--log_interval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m50\u001b[39;49;00m)      \n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--n_gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]) \n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m42\u001b[39;49;00m)  \n",
      "\n",
      "    args = parser.parse_args()\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m args\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    args = parser_args()\n",
      "    train(args)    \n"
     ]
    }
   ],
   "source": [
    "train_code = 'src/train.py'\n",
    "!pygmentize {train_code}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. 로컬에 있는 데이타 세트의 위치를 지정 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_local_inputs:  data/nsmc/train\n",
      "test_local_inputs:  data/nsmc/test\n"
     ]
    }
   ],
   "source": [
    "train_local_inputs = config.train_data_dir\n",
    "test_local_inputs = config.test_data_dir\n",
    "print(\"train_local_inputs: \", train_local_inputs)\n",
    "print(\"test_local_inputs: \", test_local_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_inputs = {\n",
    "                'train': f'file://{train_local_inputs}',\n",
    "                'test': f'file://{test_local_inputs}',    \n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. 로컬 모드로 훈련 실행\n",
    "- 아래의 두 라인이 로컬모드로 훈련을 지시 합니다.\n",
    "```python\n",
    "    instance_type=instance_type, # local_gpu or local 지정\n",
    "    session = sagemaker.LocalSession(), # 로컬 세션을 사용합니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'epochs': 1, \n",
    "                   'train_batch_size' : 256,\n",
    "                   'eval_batch_size' : 256,                   \n",
    "                   'test_batch_size' : 256,                                      \n",
    "                   'learning_rate': 5e-5,\n",
    "                   'warmup_steps' : 0,\n",
    "                   'tokenizer_id' : 'monologg/koelectra-small-v3-discriminator',\n",
    "                   'model_id' : 'monologg/koelectra-small-v3-discriminator',     \n",
    "                   'is_evaluation' : \"True\",\n",
    "                   'eval_ratio' : 0.2,\n",
    "                   'is_test' : \"True\",\n",
    "                   'use_subset_train_sampler' : \"True\",\n",
    "                   'log_interval' : 50,\n",
    "                    }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating zfodu33hed-algo-1-697fw ... \n",
      "Creating zfodu33hed-algo-1-697fw ... done\n",
      "Attaching to zfodu33hed-algo-1-697fw\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m 2022-06-08 14:51:58,952 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m 2022-06-08 14:51:58,974 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m 2022-06-08 14:51:58,977 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m 2022-06-08 14:51:59,206 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m /opt/conda/bin/python -m pip install -r requirements.txt\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Requirement already satisfied: torch==1.9.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.9.1)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Collecting transformers==4.11.0\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading transformers-4.11.0-py3-none-any.whl (2.9 MB)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/2.9 MB 57.7 MB/s eta 0:00:00\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Collecting datasets==2.2.2\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading datasets-2.2.2-py3-none-any.whl (346 kB)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 346.8/346.8 kB 36.3 MB/s eta 0:00:00\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.9.1->-r requirements.txt (line 1)) (3.10.0.2)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers==4.11.0->-r requirements.txt (line 2)) (2.27.1)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.11.0->-r requirements.txt (line 2)) (5.4.1)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Collecting regex!=2019.12.17\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading regex-2022.6.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 764.9/764.9 kB 50.7 MB/s eta 0:00:00\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.11.0->-r requirements.txt (line 2)) (1.22.3)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Collecting sacremoses\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 880.6/880.6 kB 57.7 MB/s eta 0:00:00\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Preparing metadata (setup.py): started\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Preparing metadata (setup.py): finished with status 'done'\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/3.3 MB 67.2 MB/s eta 0:00:00\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers==4.11.0->-r requirements.txt (line 2)) (4.61.2)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers==4.11.0->-r requirements.txt (line 2)) (21.2)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Collecting filelock\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Collecting huggingface-hub>=0.0.17\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.2/86.2 kB 13.0 MB/s eta 0:00:00\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.2.2->-r requirements.txt (line 3)) (2022.5.0)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Collecting responses<0.19\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Collecting aiohttp\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 61.5 MB/s eta 0:00:00\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Collecting tqdm>=4.27\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.4/78.4 kB 12.5 MB/s eta 0:00:00\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.2.2->-r requirements.txt (line 3)) (6.0.0)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Collecting dill<0.3.5\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.9/86.9 kB 14.6 MB/s eta 0:00:00\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets==2.2.2->-r requirements.txt (line 3)) (0.70.13)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets==2.2.2->-r requirements.txt (line 3)) (1.2.4)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Collecting xxhash\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading xxhash-3.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.1/212.1 kB 32.4 MB/s eta 0:00:00\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Requirement already satisfied: pyparsing<3,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.11.0->-r requirements.txt (line 2)) (2.4.7)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.11.0->-r requirements.txt (line 2)) (2.10)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.11.0->-r requirements.txt (line 2)) (2022.5.18.1)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.11.0->-r requirements.txt (line 2)) (2.0.4)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.11.0->-r requirements.txt (line 2)) (1.26.6)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Collecting yarl<2.0,>=1.0\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading yarl-1.7.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (308 kB)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 308.6/308.6 kB 37.8 MB/s eta 0:00:00\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Collecting multidict<7.0,>=4.5\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading multidict-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.3/121.3 kB 16.4 MB/s eta 0:00:00\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Collecting frozenlist>=1.1.1\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.7/158.7 kB 23.1 MB/s eta 0:00:00\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Collecting async-timeout<5.0,>=4.0.0a3\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.2.2->-r requirements.txt (line 3)) (21.4.0)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Collecting aiosignal>=1.1.2\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Collecting multiprocess\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading multiprocess-0.70.12.2-py38-none-any.whl (128 kB)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.3/128.3 kB 20.6 MB/s eta 0:00:00\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.2.2->-r requirements.txt (line 3)) (2.8.2)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.2.2->-r requirements.txt (line 3)) (2021.3)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.11.0->-r requirements.txt (line 2)) (1.16.0)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.11.0->-r requirements.txt (line 2)) (8.1.3)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.11.0->-r requirements.txt (line 2)) (1.1.0)\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Building wheels for collected packages: sacremoses\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Building wheel for sacremoses (setup.py): started\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895254 sha256=6d0176e84536746e7ee9ac501816fe24824c315692ac7b482a4b479db68c4dfe\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Successfully built sacremoses\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Installing collected packages: tokenizers, xxhash, tqdm, regex, multidict, frozenlist, filelock, dill, async-timeout, yarl, sacremoses, responses, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, datasets\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Attempting uninstall: tqdm\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Found existing installation: tqdm 4.61.2\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Uninstalling tqdm-4.61.2:\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Successfully uninstalled tqdm-4.61.2\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Attempting uninstall: dill\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Found existing installation: dill 0.3.5.1\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Uninstalling dill-0.3.5.1:\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Successfully uninstalled dill-0.3.5.1\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Attempting uninstall: multiprocess\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Found existing installation: multiprocess 0.70.13\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Uninstalling multiprocess-0.70.13:\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Successfully uninstalled multiprocess-0.70.13\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m pathos 0.2.9 requires dill>=0.3.5.1, but you have dill 0.3.4 which is incompatible.\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m pathos 0.2.9 requires multiprocess>=0.70.13, but you have multiprocess 0.70.12.2 which is incompatible.\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 datasets-2.2.2 dill-0.3.4 filelock-3.7.1 frozenlist-1.3.0 huggingface-hub-0.7.0 multidict-6.0.2 multiprocess-0.70.12.2 regex-2022.6.2 responses-0.18.0 sacremoses-0.0.53 tokenizers-0.10.3 tqdm-4.64.0 transformers-4.11.0 xxhash-3.0.0 yarl-1.7.2\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m 2022-06-08 14:52:09,783 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m 2022-06-08 14:52:09,783 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m 2022-06-08 14:52:09,853 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Training Env:\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m {\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m         \"train\": \"/opt/ml/input/data/train\",\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m         \"test\": \"/opt/ml/input/data/test\"\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     },\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     \"current_host\": \"algo-1-697fw\",\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     \"hosts\": [\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m         \"algo-1-697fw\"\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     ],\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m         \"epochs\": 1,\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m         \"train_batch_size\": 256,\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m         \"eval_batch_size\": 256,\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m         \"test_batch_size\": 256,\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m         \"learning_rate\": 5e-05,\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m         \"warmup_steps\": 0,\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m         \"tokenizer_id\": \"monologg/koelectra-small-v3-discriminator\",\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m         \"model_id\": \"monologg/koelectra-small-v3-discriminator\",\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m         \"is_evaluation\": \"True\",\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m         \"eval_ratio\": 0.2,\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m         \"is_test\": \"True\",\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m         \"use_subset_train_sampler\": \"True\",\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m         \"log_interval\": 50\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     },\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m         \"train\": {\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m         },\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m         \"test\": {\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m         }\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     },\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     \"job_name\": \"pytorch-training-2022-06-08-14-51-55-813\",\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     \"master_hostname\": \"algo-1-697fw\",\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     \"module_dir\": \"s3://sagemaker-us-east-1-154364950293/pytorch-training-2022-06-08-14-51-55-813/source/sourcedir.tar.gz\",\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     \"module_name\": \"train\",\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     \"num_cpus\": 8,\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     \"num_gpus\": 1,\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m         \"current_host\": \"algo-1-697fw\",\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m         \"hosts\": [\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m             \"algo-1-697fw\"\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m         ]\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     },\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m     \"user_entry_point\": \"train.py\"\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m }\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Environment variables:\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_HOSTS=[\"algo-1-697fw\"]\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_HPS={\"epochs\":1,\"eval_batch_size\":256,\"eval_ratio\":0.2,\"is_evaluation\":\"True\",\"is_test\":\"True\",\"learning_rate\":5e-05,\"log_interval\":50,\"model_id\":\"monologg/koelectra-small-v3-discriminator\",\"test_batch_size\":256,\"tokenizer_id\":\"monologg/koelectra-small-v3-discriminator\",\"train_batch_size\":256,\"use_subset_train_sampler\":\"True\",\"warmup_steps\":0}\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_USER_ENTRY_POINT=train.py\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-697fw\",\"hosts\":[\"algo-1-697fw\"]}\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_INPUT_DATA_CONFIG={\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_CHANNELS=[\"test\",\"train\"]\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_CURRENT_HOST=algo-1-697fw\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_MODULE_NAME=train\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_NUM_CPUS=8\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_NUM_GPUS=1\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_MODULE_DIR=s3://sagemaker-us-east-1-154364950293/pytorch-training-2022-06-08-14-51-55-813/source/sourcedir.tar.gz\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1-697fw\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-697fw\"],\"hyperparameters\":{\"epochs\":1,\"eval_batch_size\":256,\"eval_ratio\":0.2,\"is_evaluation\":\"True\",\"is_test\":\"True\",\"learning_rate\":5e-05,\"log_interval\":50,\"model_id\":\"monologg/koelectra-small-v3-discriminator\",\"test_batch_size\":256,\"tokenizer_id\":\"monologg/koelectra-small-v3-discriminator\",\"train_batch_size\":256,\"use_subset_train_sampler\":\"True\",\"warmup_steps\":0},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-06-08-14-51-55-813\",\"log_level\":20,\"master_hostname\":\"algo-1-697fw\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-154364950293/pytorch-training-2022-06-08-14-51-55-813/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-697fw\",\"hosts\":[\"algo-1-697fw\"]},\"user_entry_point\":\"train.py\"}\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_USER_ARGS=[\"--epochs\",\"1\",\"--eval_batch_size\",\"256\",\"--eval_ratio\",\"0.2\",\"--is_evaluation\",\"True\",\"--is_test\",\"True\",\"--learning_rate\",\"5e-05\",\"--log_interval\",\"50\",\"--model_id\",\"monologg/koelectra-small-v3-discriminator\",\"--test_batch_size\",\"256\",\"--tokenizer_id\",\"monologg/koelectra-small-v3-discriminator\",\"--train_batch_size\",\"256\",\"--use_subset_train_sampler\",\"True\",\"--warmup_steps\",\"0\"]\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_HP_EPOCHS=1\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_HP_TRAIN_BATCH_SIZE=256\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_HP_EVAL_BATCH_SIZE=256\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_HP_TEST_BATCH_SIZE=256\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_HP_LEARNING_RATE=5e-05\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_HP_WARMUP_STEPS=0\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_HP_TOKENIZER_ID=monologg/koelectra-small-v3-discriminator\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_HP_MODEL_ID=monologg/koelectra-small-v3-discriminator\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_HP_IS_EVALUATION=True\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_HP_EVAL_RATIO=0.2\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_HP_IS_TEST=True\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_HP_USE_SUBSET_TRAIN_SAMPLER=True\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m SM_HP_LOG_INTERVAL=50\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m /opt/conda/bin/python train.py --epochs 1 --eval_batch_size 256 --eval_ratio 0.2 --is_evaluation True --is_test True --learning_rate 5e-05 --log_interval 50 --model_id monologg/koelectra-small-v3-discriminator --test_batch_size 256 --tokenizer_id monologg/koelectra-small-v3-discriminator --train_batch_size 256 --use_subset_train_sampler True --warmup_steps 0\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m =====> Load Input Arguemtn <===========\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m ##### Args: \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m  {'epochs': 1, 'train_batch_size': 256, 'eval_batch_size': 256, 'test_batch_size': 256, 'learning_rate': 5e-05, 'warmup_steps': 0, 'fp16': True, 'tokenizer_id': 'monologg/koelectra-small-v3-discriminator', 'model_id': 'monologg/koelectra-small-v3-discriminator', 'output_data_dir': '/opt/ml/output/data', 'model_dir': '/opt/ml/model', 'train_data_dir': '/opt/ml/input/data/train', 'checkpoint_dir': '/opt/ml/checkpoints', 'is_evaluation': True, 'eval_ratio': 0.2, 'is_test': True, 'test_data_dir': '/opt/ml/input/data/test', 'use_subset_train_sampler': True, 'log_interval': 50, 'n_gpus': '1', 'seed': 42}\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m device: cuda\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m =====> data loading <===========\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m train_data_filenames ['/opt/ml/input/data/train/ratings_train.txt']\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m train_file_path /opt/ml/input/data/train/ratings_train.txt\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m len: 149552 \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Sample: ['흠   포스터보고 초딩영화줄    오버연기조차 가볍지 않구나', '너무재밓었다그래서보는것을추천한다', '교도소 이야기구먼   솔직히 재미는 없다  평점 조정', '사이몬페그의 익살스런 연기가 돋보였던 영화 스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다', '막 걸음마 뗀 세부터 초등학교 학년생인 살용영화 ㅋㅋㅋ   별반개도 아까움']\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m len: 149552 \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Sample: [1, 0, 0, 1, 0]\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m =====> Loading Train Dataset <===========\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading:   0%|          | 0.00/257k [00:00<?, ?B/s]\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading: 100%|██████████| 257k/257k [00:00<00:00, 30.5MB/s]\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading:   0%|          | 0.00/61.0 [00:00<?, ?B/s]\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading: 100%|██████████| 61.0/61.0 [00:00<00:00, 85.4kB/s]\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading:   0%|          | 0.00/458 [00:00<?, ?B/s]\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading: 100%|██████████| 458/458 [00:00<00:00, 429kB/s]\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m /opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m   warnings.warn(\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m size of train_dataset : 119641\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m dataset size with frac: 0.01 ==> 1196\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m =====> Loading Test Dataset <===========\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m test_data_filenames ['/opt/ml/input/data/test/ratings_test.txt']\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m test_file_path /opt/ml/input/data/test/ratings_test.txt\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m len: 49832 \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Sample: ['뭐야 이 평점들은     나쁘진 않지만 점 짜리는 더더욱 아니잖아', '지루하지는 않은데 완전 막장임    돈주고 보기에는', '만 아니었어도 별 다섯 개 줬을텐데   왜 로 나와서 제 심기를 불편하게 하죠', '음악이 주가 된  최고의 음악영화', '진정한 쓰레기']\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m len: 49832 \n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Sample: [0, 0, 0, 1, 0]\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m size of test_dataset : 49832\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m dataset size with frac: 1 ==> 49832\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m =====> model loading <===========\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m num_labels: 2\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m label2id: {'negative': '0', 'positive': '1'}\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m id2label: {'0': 'negative', '1': 'positive'}\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading:   0%|          | 0.00/54.0M [00:00<?, ?B/s]\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading:   9%|▊         | 4.61M/54.0M [00:00<00:01, 48.3MB/s]\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading:  20%|██        | 10.9M/54.0M [00:00<00:00, 59.0MB/s]\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading:  33%|███▎      | 18.0M/54.0M [00:00<00:00, 65.8MB/s]\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading:  46%|████▋     | 25.1M/54.0M [00:00<00:00, 69.1MB/s]\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading:  60%|█████▉    | 32.3M/54.0M [00:00<00:00, 71.4MB/s]\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading:  73%|███████▎  | 39.4M/54.0M [00:00<00:00, 72.7MB/s]\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading:  86%|████████▌ | 46.5M/54.0M [00:00<00:00, 73.2MB/s]\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading: 100%|█████████▉| 53.8M/54.0M [00:00<00:00, 74.3MB/s]\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Downloading: 100%|██████████| 54.0M/54.0M [00:00<00:00, 70.4MB/s]\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Some weights of the model checkpoint at monologg/koelectra-small-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias']\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m - This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m - This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Some weights of the model checkpoint at monologg/koelectra-small-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias']\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m - This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m - This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m =====> Training Loop <===========\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m =====> Loading Validation Dataset <===========\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m size of val_dataset : 29911\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m dataset size with frac: 1 ==> 29911\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m [2022-06-08 14:53:04.977 algo-1-697fw:50 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m [2022-06-08 14:53:05.016 algo-1-697fw:50 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m The time elapse of epoch 000 is: 00: 00: 02\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Train Epoch: 0 Acc=0.501014;\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m INFO:train_lib:Train Epoch: 0 Acc=0.501014;\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m the model is saved at /opt/ml/model/sentimental-electro-hf.pth\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m INFO:train_lib:the model is saved at /opt/ml/model/sentimental-electro-hf.pth\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m =====> test model performance <===========\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m INFO:train_lib:=====> test model performance <===========\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m /opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m   warnings.warn(\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m size of test_dataset : 49832\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m INFO:train_lib:size of test_dataset : 49832\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m dataset size with frac: 1 ==> 49832\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m INFO:train_lib:dataset size with frac: 1 ==> 49832\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m Test Accuracy: Acc=0.496369;\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m INFO:train_lib:Test Accuracy: Acc=0.496369;\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m 2022-06-08 14:53:49,371 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m 2022-06-08 14:53:49,371 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "\u001b[36mzfodu33hed-algo-1-697fw |\u001b[0m 2022-06-08 14:53:49,372 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\u001b[36mzfodu33hed-algo-1-697fw exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "local_estimator = PyTorch(\n",
    "    entry_point=\"train.py\",    \n",
    "    source_dir='src',    \n",
    "    role=role,\n",
    "    framework_version='1.9.1',\n",
    "    py_version='py38',\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type, # local_gpu or local 지정\n",
    "    session = sagemaker.LocalSession(), # 로컬 세션을 사용합니다.\n",
    "    hyperparameters= hyperparameters               \n",
    "    \n",
    ")\n",
    "local_estimator.fit(local_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SageMaker Host Mode 로 훈련\n",
    "- instance_type, session 을 수정 합니다.\n",
    "- 입력 데이터를 inputs로서 S3 의 경로를 제공합니다.\n",
    "- wait=False 로 지정해서 async 모드로 훈련을 실행합니다. \n",
    "- 실행 경과는 아래의 cifar10_estimator.logs() 에서 확인 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. 데이터 세트를 S3에 업로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_train_data_loc:  s3://sagemaker-us-east-1-154364950293/KoElectra-HF/data/nsmc/train\n",
      "s3_test_data_loc:  s3://sagemaker-us-east-1-154364950293/KoElectra-HF/data/nsmc/test\n"
     ]
    }
   ],
   "source": [
    "s3_train_data_loc = sagemaker_session.upload_data(path=config.train_data_dir, bucket=bucket, \n",
    "                                       key_prefix=f\"{prefix}/{config.train_data_dir}\")\n",
    "\n",
    "s3_test_data_loc = sagemaker_session.upload_data(path=config.test_data_dir, bucket=bucket, \n",
    "                                       key_prefix=f\"{prefix}/{config.test_data_dir}\")\n",
    "\n",
    "print(\"s3_train_data_loc: \", s3_train_data_loc)\n",
    "print(\"s3_test_data_loc: \", s3_test_data_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-08 14:53:54   13296169 KoElectra-HF/data/nsmc/train/ratings_train.txt\n",
      "2022-06-08 14:53:55    4449062 KoElectra-HF/data/nsmc/test/ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls {s3_train_data_loc} --recursive\n",
    "! aws s3 ls {s3_test_data_loc} --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. 훈련 및 테스트 데이터를 S3 로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_inputs: \n",
      " {'train': 's3://sagemaker-us-east-1-154364950293/KoElectra-HF/data/nsmc/train', 'test': 's3://sagemaker-us-east-1-154364950293/KoElectra-HF/data/nsmc/test'}\n"
     ]
    }
   ],
   "source": [
    "s3_inputs = {\n",
    "            'train': f'{s3_train_data_loc}',\n",
    "            'test': f'{s3_test_data_loc}'\n",
    "            }\n",
    "\n",
    "print(\"s3_inputs: \\n\", s3_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. 실험 세팅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실험(Experiment) 세팅\n",
    "- Amazon SageMaker 실험은 기계 학습 실험을 구성, 추적, 비교 및 평가할 수 있는 Amazon SageMaker 의 기능입니다\n",
    "- 상세 사항은 개발자 가이드 참조 하세요. --> [Amazon SageMaker 실험을 통한 Machine Learning 관리](https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/experiments.html)\n",
    "- sagemaker experiment는 추가적인 패키지를 설치하여야 합니다. 1_Setup_Environment 가 실행이 안되었다고 하면, `!pip install --upgrade sagemaker-experiments` 를 통해 설치 해주세요.\n",
    "- 여기서는 boto3 API를 통해서 실험을 생성합니다. SageMaker Python SDK를 통해서도 가능합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실험 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment:KoElectra-HF is created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade sagemaker-experiments\n",
    "import boto3\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "\n",
    "# 설험에 대한 이름을 생성 합니다.\n",
    "experiment_name = prefix \n",
    "\n",
    "# 실험이 존재하지 않으면 생성하고, 그렇지 않으면 지나갑니다.\n",
    "try:\n",
    "    response = sm_client.describe_experiment(ExperimentName=experiment_name)\n",
    "    print(f\"Experiment:{experiment_name} already exists\")    \n",
    "    \n",
    "except:\n",
    "    response = sm_client.create_experiment(\n",
    "        ExperimentName = experiment_name,\n",
    "        Description = 'Experiment for NCF',\n",
    "    )\n",
    "    print(f\"Experiment:{experiment_name} is created\")        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [중요] 하이퍼 파라미터 \n",
    "- epochs \n",
    "    - 값을 조절해서 실행 시간을 조정 하세요.\n",
    "- `'use_subset_train_sampler' : False,`\n",
    "    - False 이면 전체 훈련 데이터를 사용\n",
    "    - True 이면 전체 훈련 데이터의 0.01 만큼만 사용 (빠른 코드 디버깅시 사용)\n",
    "- `'is_evaluation' : True,`\n",
    "    - True 이면 전체 훈련 데이터에서 `eval_ratio` (에: 0.2) 데이타를 쪼개어서 사용 \n",
    "        - `eval_ratio` 가 0.2 이면 train --> train 80% : val 20 % 로 쪼개짐.\n",
    "    - False 이면 전체 훈련 데이터 사용\n",
    "- `is_test` 가 True 이면 테스트 데이터로 모델 평가 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_hyperparameters = {'epochs': 5, \n",
    "                   'train_batch_size' : 256,\n",
    "                   'eval_batch_size' : 256,  \n",
    "                   'test_batch_size' : 256,                                                              \n",
    "                   'learning_rate': 5e-5,\n",
    "                   'warmup_steps' : 0,\n",
    "                   'tokenizer_id' : 'monologg/koelectra-small-v3-discriminator',\n",
    "                   'model_id' : 'monologg/koelectra-small-v3-discriminator',     \n",
    "                   'is_evaluation' : \"False\",\n",
    "                   'is_test' : \"True\",                        \n",
    "                   'eval_ratio' : 0.2,\n",
    "                   'use_subset_train_sampler' : \"False\",\n",
    "                   'log_interval' : 50,\n",
    "                    }  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시도(Trial) 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# 시도 이름 생성\n",
    "ts = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "trial_name = experiment_name + f\"-{ts}\"\n",
    "\n",
    "# 1개의 실험 안에 시도를 생성함.\n",
    "response = sm_client.create_trial(\n",
    "    ExperimentName = experiment_name,\n",
    "    TrialName = trial_name,\n",
    ")    \n",
    "\n",
    "# 실험 설정: 실험 이름, 시도 이름으로 구성\n",
    "experiment_config = {\n",
    "    'ExperimentName' : experiment_name,\n",
    "    'TrialName' : trial_name,\n",
    "    \"TrialComponentDisplayName\" : 'Training',\n",
    "}    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 훈련 실행\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 메트릭을 CloudWatch 에서 보기\n",
    "- 개발자 가이드\n",
    "    - [Monitor and Analyze Training Jobs Using Amazon CloudWatch ](https://docs.amazonaws.cn/en_us/sagemaker/latest/dg/training-metrics.html#define-train-metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions=[\n",
    "       {'Name': 'Accuracy', 'Regex': 'Acc=(.*?);'},\n",
    "       {'Name': 'Loss', 'Regex': 'Loss=(.*?);'}        \n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2022-06-08-14-54-26-322\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "instance_type = 'ml.p3.2xlarge'\n",
    "\n",
    "host_estimator = PyTorch(\n",
    "    entry_point=\"train.py\",    \n",
    "    source_dir='src',    \n",
    "    role=role,\n",
    "    framework_version='1.9.1',\n",
    "    py_version='py38',\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    session = sagemaker.Session(), # 세이지 메이커 세션\n",
    "    hyperparameters=host_hyperparameters,\n",
    "    metric_definitions = metric_definitions\n",
    "    \n",
    ")\n",
    "host_estimator.fit(s3_inputs, \n",
    "                   experiment_config = experiment_config, # 실험 설정 제공                   \n",
    "                   wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-08 14:54:26 Starting - Starting the training job...ProfilerReport-1654700066: InProgress\n",
      "...\n",
      "2022-06-08 14:55:17 Starting - Preparing the instances for training......\n",
      "2022-06-08 14:56:28 Downloading - Downloading input data\n",
      "2022-06-08 14:56:28 Training - Downloading the training image..............................\n",
      "2022-06-08 15:01:18 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-06-08 15:01:18,251 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-06-08 15:01:18,273 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-06-08 15:01:18,279 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-06-08 15:01:18,790 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch==1.9.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.9.1)\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.11.0\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.11.0-py3-none-any.whl (2.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/2.9 MB 59.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets==2.2.2\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.2.2-py3-none-any.whl (346 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 346.8/346.8 kB 32.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.9.1->-r requirements.txt (line 1)) (3.10.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers==4.11.0->-r requirements.txt (line 2)) (21.2)\u001b[0m\n",
      "\u001b[34mCollecting filelock\u001b[0m\n",
      "\u001b[34mDownloading filelock-3.7.1-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers==4.11.0->-r requirements.txt (line 2)) (2.27.1)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.0.17\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.2/86.2 kB 8.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.11.0->-r requirements.txt (line 2)) (1.22.3)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.11,>=0.10.1\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/3.3 MB 71.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers==4.11.0->-r requirements.txt (line 2)) (4.61.2)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\u001b[0m\n",
      "\u001b[34mDownloading regex-2022.6.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 764.9/764.9 kB 54.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\u001b[0m\n",
      "\u001b[34mDownloading sacremoses-0.0.53.tar.gz (880 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 880.6/880.6 kB 60.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.11.0->-r requirements.txt (line 2)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting xxhash\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.1/212.1 kB 29.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mCollecting dill<0.3.5\u001b[0m\n",
      "\u001b[34mDownloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.9/86.9 kB 5.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tqdm>=4.27\u001b[0m\n",
      "\u001b[34mDownloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.4/78.4 kB 14.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets==2.2.2->-r requirements.txt (line 3)) (0.70.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.2.2->-r requirements.txt (line 3)) (2022.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets==2.2.2->-r requirements.txt (line 3)) (1.2.4)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 65.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.2.2->-r requirements.txt (line 3)) (6.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing<3,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.11.0->-r requirements.txt (line 2)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.11.0->-r requirements.txt (line 2)) (1.26.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.11.0->-r requirements.txt (line 2)) (2.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.11.0->-r requirements.txt (line 2)) (2022.5.18.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.11.0->-r requirements.txt (line 2)) (2.10)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.7.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (308 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 308.6/308.6 kB 31.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.2.2->-r requirements.txt (line 3)) (21.4.0)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.3/121.3 kB 23.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.7/158.7 kB 11.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting multiprocess\u001b[0m\n",
      "\u001b[34mDownloading multiprocess-0.70.12.2-py38-none-any.whl (128 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.3/128.3 kB 25.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.2.2->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.2.2->-r requirements.txt (line 3)) (2021.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.11.0->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.11.0->-r requirements.txt (line 2)) (8.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.11.0->-r requirements.txt (line 2)) (1.1.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sacremoses\u001b[0m\n",
      "\u001b[34mBuilding wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for sacremoses (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895254 sha256=efafd912ccb8553136f8a892e607580ec0a99e933e57207c1a81bfba1bb32c46\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\u001b[0m\n",
      "\u001b[34mSuccessfully built sacremoses\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, xxhash, tqdm, regex, multidict, frozenlist, filelock, dill, async-timeout, yarl, sacremoses, responses, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, datasets\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tqdm\u001b[0m\n",
      "\u001b[34mFound existing installation: tqdm 4.61.2\u001b[0m\n",
      "\u001b[34mUninstalling tqdm-4.61.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tqdm-4.61.2\u001b[0m\n",
      "\u001b[34mAttempting uninstall: dill\u001b[0m\n",
      "\u001b[34mFound existing installation: dill 0.3.5.1\u001b[0m\n",
      "\u001b[34mUninstalling dill-0.3.5.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled dill-0.3.5.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: multiprocess\u001b[0m\n",
      "\u001b[34mFound existing installation: multiprocess 0.70.13\u001b[0m\n",
      "\u001b[34mUninstalling multiprocess-0.70.13:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled multiprocess-0.70.13\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mpathos 0.2.9 requires dill>=0.3.5.1, but you have dill 0.3.4 which is incompatible.\u001b[0m\n",
      "\u001b[34mpathos 0.2.9 requires multiprocess>=0.70.13, but you have multiprocess 0.70.12.2 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 datasets-2.2.2 dill-0.3.4 filelock-3.7.1 frozenlist-1.3.0 huggingface-hub-0.7.0 multidict-6.0.2 multiprocess-0.70.12.2 regex-2022.6.2 responses-0.18.0 sacremoses-0.0.53 tokenizers-0.10.3 tqdm-4.64.0 transformers-4.11.0 xxhash-3.0.0 yarl-1.7.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-06-08 15:01:29,778 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-06-08 15:01:29,779 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-06-08 15:01:29,850 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 5,\n",
      "        \"eval_batch_size\": 256,\n",
      "        \"eval_ratio\": 0.2,\n",
      "        \"is_evaluation\": \"False\",\n",
      "        \"is_test\": \"True\",\n",
      "        \"learning_rate\": 5e-05,\n",
      "        \"log_interval\": 50,\n",
      "        \"model_id\": \"monologg/koelectra-small-v3-discriminator\",\n",
      "        \"test_batch_size\": 256,\n",
      "        \"tokenizer_id\": \"monologg/koelectra-small-v3-discriminator\",\n",
      "        \"train_batch_size\": 256,\n",
      "        \"use_subset_train_sampler\": \"False\",\n",
      "        \"warmup_steps\": 0\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2022-06-08-14-54-26-322\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-154364950293/pytorch-training-2022-06-08-14-54-26-322/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":5,\"eval_batch_size\":256,\"eval_ratio\":0.2,\"is_evaluation\":\"False\",\"is_test\":\"True\",\"learning_rate\":5e-05,\"log_interval\":50,\"model_id\":\"monologg/koelectra-small-v3-discriminator\",\"test_batch_size\":256,\"tokenizer_id\":\"monologg/koelectra-small-v3-discriminator\",\"train_batch_size\":256,\"use_subset_train_sampler\":\"False\",\"warmup_steps\":0}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-154364950293/pytorch-training-2022-06-08-14-54-26-322/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":5,\"eval_batch_size\":256,\"eval_ratio\":0.2,\"is_evaluation\":\"False\",\"is_test\":\"True\",\"learning_rate\":5e-05,\"log_interval\":50,\"model_id\":\"monologg/koelectra-small-v3-discriminator\",\"test_batch_size\":256,\"tokenizer_id\":\"monologg/koelectra-small-v3-discriminator\",\"train_batch_size\":256,\"use_subset_train_sampler\":\"False\",\"warmup_steps\":0},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-06-08-14-54-26-322\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-154364950293/pytorch-training-2022-06-08-14-54-26-322/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"5\",\"--eval_batch_size\",\"256\",\"--eval_ratio\",\"0.2\",\"--is_evaluation\",\"False\",\"--is_test\",\"True\",\"--learning_rate\",\"5e-05\",\"--log_interval\",\"50\",\"--model_id\",\"monologg/koelectra-small-v3-discriminator\",\"--test_batch_size\",\"256\",\"--tokenizer_id\",\"monologg/koelectra-small-v3-discriminator\",\"--train_batch_size\",\"256\",\"--use_subset_train_sampler\",\"False\",\"--warmup_steps\",\"0\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=5\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=256\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_RATIO=0.2\u001b[0m\n",
      "\u001b[34mSM_HP_IS_EVALUATION=False\u001b[0m\n",
      "\u001b[34mSM_HP_IS_TEST=True\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=5e-05\u001b[0m\n",
      "\u001b[34mSM_HP_LOG_INTERVAL=50\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=monologg/koelectra-small-v3-discriminator\u001b[0m\n",
      "\u001b[34mSM_HP_TEST_BATCH_SIZE=256\u001b[0m\n",
      "\u001b[34mSM_HP_TOKENIZER_ID=monologg/koelectra-small-v3-discriminator\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=256\u001b[0m\n",
      "\u001b[34mSM_HP_USE_SUBSET_TRAIN_SAMPLER=False\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_STEPS=0\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train.py --epochs 5 --eval_batch_size 256 --eval_ratio 0.2 --is_evaluation False --is_test True --learning_rate 5e-05 --log_interval 50 --model_id monologg/koelectra-small-v3-discriminator --test_batch_size 256 --tokenizer_id monologg/koelectra-small-v3-discriminator --train_batch_size 256 --use_subset_train_sampler False --warmup_steps 0\u001b[0m\n",
      "\u001b[34m=====> Load Input Arguemtn <===========\u001b[0m\n",
      "\u001b[34m##### Args: \n",
      " {'epochs': 5, 'train_batch_size': 256, 'eval_batch_size': 256, 'test_batch_size': 256, 'learning_rate': 5e-05, 'warmup_steps': 0, 'fp16': True, 'tokenizer_id': 'monologg/koelectra-small-v3-discriminator', 'model_id': 'monologg/koelectra-small-v3-discriminator', 'output_data_dir': '/opt/ml/output/data', 'model_dir': '/opt/ml/model', 'train_data_dir': '/opt/ml/input/data/train', 'checkpoint_dir': '/opt/ml/checkpoints', 'is_evaluation': False, 'eval_ratio': 0.2, 'is_test': True, 'test_data_dir': '/opt/ml/input/data/test', 'use_subset_train_sampler': False, 'log_interval': 50, 'n_gpus': '1', 'seed': 42}\u001b[0m\n",
      "\u001b[34mdevice: cuda\u001b[0m\n",
      "\u001b[34m=====> data loading <===========\u001b[0m\n",
      "\u001b[34mtrain_data_filenames ['/opt/ml/input/data/train/ratings_train.txt']\u001b[0m\n",
      "\u001b[34mtrain_file_path /opt/ml/input/data/train/ratings_train.txt\u001b[0m\n",
      "\u001b[34mlen: 149552 \u001b[0m\n",
      "\u001b[34mSample: ['흠   포스터보고 초딩영화줄    오버연기조차 가볍지 않구나', '너무재밓었다그래서보는것을추천한다', '교도소 이야기구먼   솔직히 재미는 없다  평점 조정', '사이몬페그의 익살스런 연기가 돋보였던 영화 스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다', '막 걸음마 뗀 세부터 초등학교 학년생인 살용영화 ㅋㅋㅋ   별반개도 아까움']\u001b[0m\n",
      "\u001b[34mlen: 149552 \u001b[0m\n",
      "\u001b[34mSample: [1, 0, 0, 1, 0]\u001b[0m\n",
      "\u001b[34m=====> Loading Train Dataset <===========\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/257k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 257k/257k [00:00<00:00, 23.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/61.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 61.0/61.0 [00:00<00:00, 80.1kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/458 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 458/458 [00:00<00:00, 673kB/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34msize of train_dataset : 149552\u001b[0m\n",
      "\u001b[34mdataset size with frac: 1 ==> 149552\u001b[0m\n",
      "\u001b[34m=====> Loading Test Dataset <===========\u001b[0m\n",
      "\u001b[34mtest_data_filenames ['/opt/ml/input/data/test/ratings_test.txt']\u001b[0m\n",
      "\u001b[34mtest_file_path /opt/ml/input/data/test/ratings_test.txt\u001b[0m\n",
      "\u001b[34mlen: 49832 \u001b[0m\n",
      "\u001b[34mSample: ['뭐야 이 평점들은     나쁘진 않지만 점 짜리는 더더욱 아니잖아', '지루하지는 않은데 완전 막장임    돈주고 보기에는', '만 아니었어도 별 다섯 개 줬을텐데   왜 로 나와서 제 심기를 불편하게 하죠', '음악이 주가 된  최고의 음악영화', '진정한 쓰레기']\u001b[0m\n",
      "\u001b[34mlen: 49832 \u001b[0m\n",
      "\u001b[34mSample: [0, 0, 0, 1, 0]\u001b[0m\n",
      "\u001b[34msize of test_dataset : 49832\u001b[0m\n",
      "\u001b[34mdataset size with frac: 1 ==> 49832\u001b[0m\n",
      "\u001b[34m=====> model loading <===========\u001b[0m\n",
      "\u001b[34mnum_labels: 2\u001b[0m\n",
      "\u001b[34mlabel2id: {'negative': '0', 'positive': '1'}\u001b[0m\n",
      "\u001b[34mid2label: {'0': 'negative', '1': 'positive'}\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/54.0M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:  10%|█         | 5.55M/54.0M [00:00<00:00, 58.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██        | 11.1M/54.0M [00:00<00:00, 45.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  30%|██▉       | 16.0M/54.0M [00:00<00:00, 45.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  41%|████▏     | 22.3M/54.0M [00:00<00:00, 37.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  48%|████▊     | 26.2M/54.0M [00:00<00:00, 32.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  56%|█████▌    | 30.3M/54.0M [00:00<00:00, 35.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  63%|██████▎   | 34.0M/54.0M [00:01<00:00, 28.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  73%|███████▎  | 39.6M/54.0M [00:01<00:00, 35.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  80%|████████  | 43.4M/54.0M [00:01<00:00, 32.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  91%|█████████ | 49.0M/54.0M [00:01<00:00, 38.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 54.0M/54.0M [00:01<00:00, 38.3MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at monologg/koelectra-small-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at monologg/koelectra-small-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m=====> Training Loop <===========\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:31.369 algo-1:50 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:31.497 algo-1:50 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:31.498 algo-1:50 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:31.498 algo-1:50 INFO hook.py:200] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:31.499 algo-1:50 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:31.499 algo-1:50 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.510 algo-1:50 INFO hook.py:591] name:electra.embeddings.word_embeddings.weight count_params:4480000\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.510 algo-1:50 INFO hook.py:591] name:electra.embeddings.position_embeddings.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.510 algo-1:50 INFO hook.py:591] name:electra.embeddings.token_type_embeddings.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.510 algo-1:50 INFO hook.py:591] name:electra.embeddings.LayerNorm.weight count_params:128\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.510 algo-1:50 INFO hook.py:591] name:electra.embeddings.LayerNorm.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.510 algo-1:50 INFO hook.py:591] name:electra.embeddings_project.weight count_params:32768\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.510 algo-1:50 INFO hook.py:591] name:electra.embeddings_project.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.510 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.0.attention.self.query.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.510 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.0.attention.self.query.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.510 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.0.attention.self.key.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.510 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.0.attention.self.key.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.510 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.0.attention.self.value.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.511 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.0.attention.self.value.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.511 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.0.attention.output.dense.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.511 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.0.attention.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.511 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.0.attention.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.511 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.0.attention.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.511 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.0.intermediate.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.511 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.0.intermediate.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.511 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.0.output.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.511 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.0.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.511 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.0.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.511 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.0.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.511 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.1.attention.self.query.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.511 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.1.attention.self.query.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.512 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.1.attention.self.key.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.512 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.1.attention.self.key.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.512 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.1.attention.self.value.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.512 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.1.attention.self.value.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.512 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.1.attention.output.dense.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.512 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.1.attention.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.512 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.1.attention.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.512 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.1.attention.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.512 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.1.intermediate.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.512 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.1.intermediate.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.512 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.1.output.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.512 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.1.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.512 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.1.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.512 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.1.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.513 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.2.attention.self.query.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.513 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.2.attention.self.query.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.513 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.2.attention.self.key.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.513 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.2.attention.self.key.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.513 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.2.attention.self.value.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.513 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.2.attention.self.value.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.513 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.2.attention.output.dense.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.513 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.2.attention.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.513 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.2.attention.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.513 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.2.attention.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.513 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.2.intermediate.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.513 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.2.intermediate.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.514 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.2.output.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.514 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.2.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.514 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.2.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.514 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.2.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.514 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.3.attention.self.query.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.514 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.3.attention.self.query.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.514 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.3.attention.self.key.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.514 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.3.attention.self.key.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.514 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.3.attention.self.value.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.514 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.3.attention.self.value.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.514 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.3.attention.output.dense.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.514 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.3.attention.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.514 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.3.attention.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.514 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.3.attention.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.515 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.3.intermediate.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.515 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.3.intermediate.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.515 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.3.output.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.515 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.3.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.515 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.3.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.515 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.3.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.515 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.4.attention.self.query.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.515 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.4.attention.self.query.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.515 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.4.attention.self.key.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.515 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.4.attention.self.key.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.515 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.4.attention.self.value.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.515 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.4.attention.self.value.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.515 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.4.attention.output.dense.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.515 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.4.attention.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.516 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.4.attention.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.516 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.4.attention.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.516 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.4.intermediate.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.516 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.4.intermediate.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.516 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.4.output.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.516 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.4.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.516 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.4.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.516 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.4.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.516 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.5.attention.self.query.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.516 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.5.attention.self.query.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.516 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.5.attention.self.key.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.516 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.5.attention.self.key.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.516 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.5.attention.self.value.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.516 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.5.attention.self.value.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.517 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.5.attention.output.dense.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.517 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.5.attention.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.517 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.5.attention.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.517 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.5.attention.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.517 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.5.intermediate.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.517 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.5.intermediate.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.517 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.5.output.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.517 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.5.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.517 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.5.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.517 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.5.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.517 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.6.attention.self.query.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.517 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.6.attention.self.query.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.517 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.6.attention.self.key.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.517 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.6.attention.self.key.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.517 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.6.attention.self.value.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.518 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.6.attention.self.value.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.518 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.6.attention.output.dense.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.518 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.6.attention.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.518 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.6.attention.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.518 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.6.attention.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.518 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.6.intermediate.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.518 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.6.intermediate.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.518 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.6.output.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.518 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.6.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.518 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.6.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.518 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.6.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.518 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.7.attention.self.query.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.518 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.7.attention.self.query.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.518 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.7.attention.self.key.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.518 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.7.attention.self.key.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.518 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.7.attention.self.value.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.518 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.7.attention.self.value.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.519 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.7.attention.output.dense.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.519 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.7.attention.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.519 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.7.attention.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.519 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.7.attention.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.519 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.7.intermediate.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.519 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.7.intermediate.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.519 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.7.output.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.519 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.7.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.519 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.7.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.519 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.7.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.519 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.8.attention.self.query.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.519 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.8.attention.self.query.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.519 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.8.attention.self.key.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.519 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.8.attention.self.key.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.520 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.8.attention.self.value.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.520 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.8.attention.self.value.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.520 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.8.attention.output.dense.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.520 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.8.attention.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.520 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.8.attention.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.520 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.8.attention.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.520 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.8.intermediate.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.520 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.8.intermediate.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.520 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.8.output.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.520 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.8.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.520 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.8.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.520 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.8.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.520 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.9.attention.self.query.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.520 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.9.attention.self.query.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.521 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.9.attention.self.key.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.521 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.9.attention.self.key.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.521 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.9.attention.self.value.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.521 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.9.attention.self.value.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.521 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.9.attention.output.dense.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.521 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.9.attention.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.521 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.9.attention.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.521 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.9.attention.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.521 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.9.intermediate.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.521 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.9.intermediate.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.521 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.9.output.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.521 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.9.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.521 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.9.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.521 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.9.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.522 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.10.attention.self.query.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.522 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.10.attention.self.query.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.522 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.10.attention.self.key.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.522 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.10.attention.self.key.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.522 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.10.attention.self.value.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.522 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.10.attention.self.value.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.522 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.10.attention.output.dense.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.522 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.10.attention.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.522 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.10.attention.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.522 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.10.attention.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.522 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.10.intermediate.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.522 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.10.intermediate.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.522 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.10.output.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.522 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.10.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.522 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.10.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.522 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.10.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.522 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.11.attention.self.query.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.523 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.11.attention.self.query.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.523 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.11.attention.self.key.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.523 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.11.attention.self.key.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.523 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.11.attention.self.value.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.523 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.11.attention.self.value.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.523 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.11.attention.output.dense.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.523 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.11.attention.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.523 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.11.attention.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.523 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.11.attention.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.523 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.11.intermediate.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.523 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.11.intermediate.dense.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.523 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.11.output.dense.weight count_params:262144\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.523 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.11.output.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.523 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.11.output.LayerNorm.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.524 algo-1:50 INFO hook.py:591] name:electra.encoder.layer.11.output.LayerNorm.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.524 algo-1:50 INFO hook.py:591] name:classifier.dense.weight count_params:65536\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.524 algo-1:50 INFO hook.py:591] name:classifier.dense.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.524 algo-1:50 INFO hook.py:591] name:classifier.out_proj.weight count_params:512\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.524 algo-1:50 INFO hook.py:591] name:classifier.out_proj.bias count_params:2\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.524 algo-1:50 INFO hook.py:593] Total Trainable Params: 14122498\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.524 algo-1:50 INFO hook.py:424] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-06-08 15:02:32.527 algo-1:50 INFO hook.py:488] Hook is writing from the hook with pid: 50\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [200/149552 (9%)] Loss=0.652161;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [200/149552 (9%)] Loss=0.652161;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [400/149552 (17%)] Loss=0.516232;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [400/149552 (17%)] Loss=0.516232;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [600/149552 (26%)] Loss=0.383998;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [600/149552 (26%)] Loss=0.383998;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [800/149552 (34%)] Loss=0.428840;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [800/149552 (34%)] Loss=0.428840;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [1000/149552 (43%)] Loss=0.409271;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [1000/149552 (43%)] Loss=0.409271;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [1200/149552 (51%)] Loss=0.353532;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [1200/149552 (51%)] Loss=0.353532;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [1400/149552 (60%)] Loss=0.377877;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [1400/149552 (60%)] Loss=0.377877;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [1600/149552 (68%)] Loss=0.322960;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [1600/149552 (68%)] Loss=0.322960;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [1800/149552 (77%)] Loss=0.297783;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [1800/149552 (77%)] Loss=0.297783;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [2000/149552 (85%)] Loss=0.323697;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [2000/149552 (85%)] Loss=0.323697;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 0 [2200/149552 (94%)] Loss=0.330932;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 0 [2200/149552 (94%)] Loss=0.330932;\u001b[0m\n",
      "\u001b[34mThe time elapse of epoch 000 is: 00: 02: 47\u001b[0m\n",
      "\u001b[34mthe model is saved at /opt/ml/model/sentimental-electro-hf.pth\u001b[0m\n",
      "\u001b[34mINFO:train_lib:the model is saved at /opt/ml/model/sentimental-electro-hf.pth\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [200/149552 (9%)] Loss=0.342058;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 1 [200/149552 (9%)] Loss=0.342058;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [400/149552 (17%)] Loss=0.304759;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 1 [400/149552 (17%)] Loss=0.304759;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [600/149552 (26%)] Loss=0.379464;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 1 [600/149552 (26%)] Loss=0.379464;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [800/149552 (34%)] Loss=0.251422;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 1 [800/149552 (34%)] Loss=0.251422;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [1000/149552 (43%)] Loss=0.266142;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 1 [1000/149552 (43%)] Loss=0.266142;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [1200/149552 (51%)] Loss=0.257324;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 1 [1200/149552 (51%)] Loss=0.257324;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [1400/149552 (60%)] Loss=0.244520;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 1 [1400/149552 (60%)] Loss=0.244520;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [1600/149552 (68%)] Loss=0.238771;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 1 [1600/149552 (68%)] Loss=0.238771;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [1800/149552 (77%)] Loss=0.295865;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 1 [1800/149552 (77%)] Loss=0.295865;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [2000/149552 (85%)] Loss=0.313045;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 1 [2000/149552 (85%)] Loss=0.313045;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [2200/149552 (94%)] Loss=0.320136;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 1 [2200/149552 (94%)] Loss=0.320136;\u001b[0m\n",
      "\u001b[34mThe time elapse of epoch 001 is: 00: 02: 46\u001b[0m\n",
      "\u001b[34mthe model is saved at /opt/ml/model/sentimental-electro-hf.pth\u001b[0m\n",
      "\u001b[34mINFO:train_lib:the model is saved at /opt/ml/model/sentimental-electro-hf.pth\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [200/149552 (9%)] Loss=0.251493;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 2 [200/149552 (9%)] Loss=0.251493;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [400/149552 (17%)] Loss=0.292026;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 2 [400/149552 (17%)] Loss=0.292026;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [600/149552 (26%)] Loss=0.277421;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 2 [600/149552 (26%)] Loss=0.277421;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [800/149552 (34%)] Loss=0.268231;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 2 [800/149552 (34%)] Loss=0.268231;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [1000/149552 (43%)] Loss=0.231787;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 2 [1000/149552 (43%)] Loss=0.231787;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [1200/149552 (51%)] Loss=0.301299;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 2 [1200/149552 (51%)] Loss=0.301299;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [1400/149552 (60%)] Loss=0.295750;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 2 [1400/149552 (60%)] Loss=0.295750;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [1600/149552 (68%)] Loss=0.212977;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 2 [1600/149552 (68%)] Loss=0.212977;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [1800/149552 (77%)] Loss=0.249581;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 2 [1800/149552 (77%)] Loss=0.249581;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [2000/149552 (85%)] Loss=0.234277;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 2 [2000/149552 (85%)] Loss=0.234277;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [2200/149552 (94%)] Loss=0.310357;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 2 [2200/149552 (94%)] Loss=0.310357;\u001b[0m\n",
      "\u001b[34mThe time elapse of epoch 002 is: 00: 02: 44\u001b[0m\n",
      "\u001b[34mthe model is saved at /opt/ml/model/sentimental-electro-hf.pth\u001b[0m\n",
      "\u001b[34mINFO:train_lib:the model is saved at /opt/ml/model/sentimental-electro-hf.pth\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [200/149552 (9%)] Loss=0.244383;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 3 [200/149552 (9%)] Loss=0.244383;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [400/149552 (17%)] Loss=0.241023;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 3 [400/149552 (17%)] Loss=0.241023;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [600/149552 (26%)] Loss=0.233935;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 3 [600/149552 (26%)] Loss=0.233935;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [800/149552 (34%)] Loss=0.220911;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 3 [800/149552 (34%)] Loss=0.220911;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [1000/149552 (43%)] Loss=0.230587;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 3 [1000/149552 (43%)] Loss=0.230587;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [1200/149552 (51%)] Loss=0.233022;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 3 [1200/149552 (51%)] Loss=0.233022;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [1400/149552 (60%)] Loss=0.210249;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 3 [1400/149552 (60%)] Loss=0.210249;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [1600/149552 (68%)] Loss=0.285461;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 3 [1600/149552 (68%)] Loss=0.285461;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [1800/149552 (77%)] Loss=0.203659;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 3 [1800/149552 (77%)] Loss=0.203659;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [2000/149552 (85%)] Loss=0.228504;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 3 [2000/149552 (85%)] Loss=0.228504;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [2200/149552 (94%)] Loss=0.263409;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 3 [2200/149552 (94%)] Loss=0.263409;\u001b[0m\n",
      "\u001b[34mThe time elapse of epoch 003 is: 00: 02: 44\u001b[0m\n",
      "\u001b[34mthe model is saved at /opt/ml/model/sentimental-electro-hf.pth\u001b[0m\n",
      "\u001b[34mINFO:train_lib:the model is saved at /opt/ml/model/sentimental-electro-hf.pth\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [200/149552 (9%)] Loss=0.147092;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 4 [200/149552 (9%)] Loss=0.147092;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [400/149552 (17%)] Loss=0.227054;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 4 [400/149552 (17%)] Loss=0.227054;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [600/149552 (26%)] Loss=0.183650;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 4 [600/149552 (26%)] Loss=0.183650;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [800/149552 (34%)] Loss=0.170879;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 4 [800/149552 (34%)] Loss=0.170879;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [1000/149552 (43%)] Loss=0.194011;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 4 [1000/149552 (43%)] Loss=0.194011;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [1200/149552 (51%)] Loss=0.239693;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 4 [1200/149552 (51%)] Loss=0.239693;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [1400/149552 (60%)] Loss=0.209829;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 4 [1400/149552 (60%)] Loss=0.209829;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [1600/149552 (68%)] Loss=0.199816;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 4 [1600/149552 (68%)] Loss=0.199816;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [1800/149552 (77%)] Loss=0.279431;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 4 [1800/149552 (77%)] Loss=0.279431;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [2000/149552 (85%)] Loss=0.242477;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 4 [2000/149552 (85%)] Loss=0.242477;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [2200/149552 (94%)] Loss=0.210633;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Train Epoch: 4 [2200/149552 (94%)] Loss=0.210633;\u001b[0m\n",
      "\u001b[34mThe time elapse of epoch 004 is: 00: 02: 45\u001b[0m\n",
      "\u001b[34mthe model is saved at /opt/ml/model/sentimental-electro-hf.pth\u001b[0m\n",
      "\u001b[34mINFO:train_lib:the model is saved at /opt/ml/model/sentimental-electro-hf.pth\u001b[0m\n",
      "\u001b[34m=====> test model performance <===========\u001b[0m\n",
      "\u001b[34mINFO:train_lib:=====> test model performance <===========\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34msize of test_dataset : 49832\u001b[0m\n",
      "\u001b[34mINFO:train_lib:size of test_dataset : 49832\u001b[0m\n",
      "\u001b[34mdataset size with frac: 1 ==> 49832\u001b[0m\n",
      "\u001b[34mINFO:train_lib:dataset size with frac: 1 ==> 49832\u001b[0m\n",
      "\n",
      "2022-06-08 15:17:02 Uploading - Uploading generated training model\u001b[34mTest Accuracy: Acc=0.885250;\u001b[0m\n",
      "\u001b[34mINFO:train_lib:Test Accuracy: Acc=0.885250;\u001b[0m\n",
      "\u001b[34m2022-06-08 15:16:52,249 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-06-08 15:16:52,249 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-06-08 15:16:52,250 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-06-08 15:17:22 Completed - Training job completed\n",
      "ProfilerReport-1654700066: IssuesFound\n",
      "Training seconds: 1257\n",
      "Billable seconds: 1257\n",
      "CPU times: user 2.85 s, sys: 93.8 ms, total: 2.94 s\n",
      "Wall time: 23min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "host_estimator.logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 실험 결과 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "위의 실험한 결과를 확인 합니다.\n",
    "- 각각의 훈련잡의 시도에 대한 훈련 사용 데이터, 모델 입력 하이퍼 파라미터, 모델 평가 지표, 모델 아티펙트 결과 위치 등의 확인이 가능합니다.\n",
    "- **아래의 모든 내용은 SageMaker Studio 를 통해서 직관적으로 확인이 가능합니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrialComponentName</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>SourceArn</th>\n",
       "      <th>SageMaker.ImageUri</th>\n",
       "      <th>SageMaker.InstanceCount</th>\n",
       "      <th>SageMaker.InstanceType</th>\n",
       "      <th>SageMaker.VolumeSizeInGB</th>\n",
       "      <th>epochs</th>\n",
       "      <th>eval_batch_size</th>\n",
       "      <th>eval_ratio</th>\n",
       "      <th>is_evaluation</th>\n",
       "      <th>is_test</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>log_interval</th>\n",
       "      <th>model_id</th>\n",
       "      <th>sagemaker_container_log_level</th>\n",
       "      <th>sagemaker_job_name</th>\n",
       "      <th>sagemaker_program</th>\n",
       "      <th>sagemaker_region</th>\n",
       "      <th>sagemaker_submit_directory</th>\n",
       "      <th>test_batch_size</th>\n",
       "      <th>tokenizer_id</th>\n",
       "      <th>train_batch_size</th>\n",
       "      <th>use_subset_train_sampler</th>\n",
       "      <th>warmup_steps</th>\n",
       "      <th>...</th>\n",
       "      <th>Accuracy - StdDev</th>\n",
       "      <th>Accuracy - Last</th>\n",
       "      <th>Accuracy - Count</th>\n",
       "      <th>Loss - Min</th>\n",
       "      <th>Loss - Max</th>\n",
       "      <th>Loss - Avg</th>\n",
       "      <th>Loss - StdDev</th>\n",
       "      <th>Loss - Last</th>\n",
       "      <th>Loss - Count</th>\n",
       "      <th>cross_entropy_loss_output_0_GLOBAL - Min</th>\n",
       "      <th>cross_entropy_loss_output_0_GLOBAL - Max</th>\n",
       "      <th>cross_entropy_loss_output_0_GLOBAL - Avg</th>\n",
       "      <th>cross_entropy_loss_output_0_GLOBAL - StdDev</th>\n",
       "      <th>cross_entropy_loss_output_0_GLOBAL - Last</th>\n",
       "      <th>cross_entropy_loss_output_0_GLOBAL - Count</th>\n",
       "      <th>test - MediaType</th>\n",
       "      <th>test - Value</th>\n",
       "      <th>train - MediaType</th>\n",
       "      <th>train - Value</th>\n",
       "      <th>SageMaker.DebugHookOutput - MediaType</th>\n",
       "      <th>SageMaker.DebugHookOutput - Value</th>\n",
       "      <th>SageMaker.ModelArtifact - MediaType</th>\n",
       "      <th>SageMaker.ModelArtifact - Value</th>\n",
       "      <th>Trials</th>\n",
       "      <th>Experiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pytorch-training-2022-06-08-14-54-26-322-aws-t...</td>\n",
       "      <td>Training</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:154364950293:train...</td>\n",
       "      <td>763104351884.dkr.ecr.us-east-1.amazonaws.com/p...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ml.p3.2xlarge</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>\"False\"</td>\n",
       "      <td>\"True\"</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>50.0</td>\n",
       "      <td>\"monologg/koelectra-small-v3-discriminator\"</td>\n",
       "      <td>20.0</td>\n",
       "      <td>\"pytorch-training-2022-06-08-14-54-26-322\"</td>\n",
       "      <td>\"train.py\"</td>\n",
       "      <td>\"us-east-1\"</td>\n",
       "      <td>\"s3://sagemaker-us-east-1-154364950293/pytorch...</td>\n",
       "      <td>256.0</td>\n",
       "      <td>\"monologg/koelectra-small-v3-discriminator\"</td>\n",
       "      <td>256.0</td>\n",
       "      <td>\"False\"</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.88525</td>\n",
       "      <td>1</td>\n",
       "      <td>0.147092</td>\n",
       "      <td>0.652161</td>\n",
       "      <td>0.294415</td>\n",
       "      <td>0.09952</td>\n",
       "      <td>0.210633</td>\n",
       "      <td>63</td>\n",
       "      <td>0.223876</td>\n",
       "      <td>0.694538</td>\n",
       "      <td>0.323007</td>\n",
       "      <td>0.16708</td>\n",
       "      <td>0.223876</td>\n",
       "      <td>7</td>\n",
       "      <td>None</td>\n",
       "      <td>s3://sagemaker-us-east-1-154364950293/KoElectr...</td>\n",
       "      <td>None</td>\n",
       "      <td>s3://sagemaker-us-east-1-154364950293/KoElectr...</td>\n",
       "      <td>None</td>\n",
       "      <td>s3://sagemaker-us-east-1-154364950293/</td>\n",
       "      <td>None</td>\n",
       "      <td>s3://sagemaker-us-east-1-154364950293/pytorch-...</td>\n",
       "      <td>[KoElectra-HF-2022-06-08-14-53-56-437716]</td>\n",
       "      <td>[KoElectra-HF]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  TrialComponentName DisplayName  \\\n",
       "0  pytorch-training-2022-06-08-14-54-26-322-aws-t...    Training   \n",
       "\n",
       "                                           SourceArn  \\\n",
       "0  arn:aws:sagemaker:us-east-1:154364950293:train...   \n",
       "\n",
       "                                  SageMaker.ImageUri  SageMaker.InstanceCount  \\\n",
       "0  763104351884.dkr.ecr.us-east-1.amazonaws.com/p...                      1.0   \n",
       "\n",
       "  SageMaker.InstanceType  SageMaker.VolumeSizeInGB  epochs  eval_batch_size  \\\n",
       "0          ml.p3.2xlarge                      30.0     5.0            256.0   \n",
       "\n",
       "   eval_ratio is_evaluation is_test  learning_rate  log_interval  \\\n",
       "0         0.2       \"False\"  \"True\"        0.00005          50.0   \n",
       "\n",
       "                                      model_id  sagemaker_container_log_level  \\\n",
       "0  \"monologg/koelectra-small-v3-discriminator\"                           20.0   \n",
       "\n",
       "                           sagemaker_job_name sagemaker_program  \\\n",
       "0  \"pytorch-training-2022-06-08-14-54-26-322\"        \"train.py\"   \n",
       "\n",
       "  sagemaker_region                         sagemaker_submit_directory  \\\n",
       "0      \"us-east-1\"  \"s3://sagemaker-us-east-1-154364950293/pytorch...   \n",
       "\n",
       "   test_batch_size                                 tokenizer_id  \\\n",
       "0            256.0  \"monologg/koelectra-small-v3-discriminator\"   \n",
       "\n",
       "   train_batch_size use_subset_train_sampler  warmup_steps  ...  \\\n",
       "0             256.0                  \"False\"           0.0  ...   \n",
       "\n",
       "   Accuracy - StdDev  Accuracy - Last  Accuracy - Count  Loss - Min  \\\n",
       "0                0.0          0.88525                 1    0.147092   \n",
       "\n",
       "   Loss - Max  Loss - Avg  Loss - StdDev  Loss - Last  Loss - Count  \\\n",
       "0    0.652161    0.294415        0.09952     0.210633            63   \n",
       "\n",
       "   cross_entropy_loss_output_0_GLOBAL - Min  \\\n",
       "0                                  0.223876   \n",
       "\n",
       "   cross_entropy_loss_output_0_GLOBAL - Max  \\\n",
       "0                                  0.694538   \n",
       "\n",
       "   cross_entropy_loss_output_0_GLOBAL - Avg  \\\n",
       "0                                  0.323007   \n",
       "\n",
       "   cross_entropy_loss_output_0_GLOBAL - StdDev  \\\n",
       "0                                      0.16708   \n",
       "\n",
       "   cross_entropy_loss_output_0_GLOBAL - Last  \\\n",
       "0                                   0.223876   \n",
       "\n",
       "   cross_entropy_loss_output_0_GLOBAL - Count  test - MediaType  \\\n",
       "0                                           7              None   \n",
       "\n",
       "                                        test - Value  train - MediaType  \\\n",
       "0  s3://sagemaker-us-east-1-154364950293/KoElectr...               None   \n",
       "\n",
       "                                       train - Value  \\\n",
       "0  s3://sagemaker-us-east-1-154364950293/KoElectr...   \n",
       "\n",
       "  SageMaker.DebugHookOutput - MediaType  \\\n",
       "0                                  None   \n",
       "\n",
       "        SageMaker.DebugHookOutput - Value SageMaker.ModelArtifact - MediaType  \\\n",
       "0  s3://sagemaker-us-east-1-154364950293/                                None   \n",
       "\n",
       "                     SageMaker.ModelArtifact - Value  \\\n",
       "0  s3://sagemaker-us-east-1-154364950293/pytorch-...   \n",
       "\n",
       "                                      Trials     Experiments  \n",
       "0  [KoElectra-HF-2022-06-08-14-53-56-437716]  [KoElectra-HF]  \n",
       "\n",
       "[1 rows x 53 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_rows = 5\n",
    "pd.options.display.max_colwidth = 50\n",
    "\n",
    "search_expression = {\n",
    "    \"Filters\": [\n",
    "        {\n",
    "            \"Name\": \"DisplayName\",\n",
    "            \"Operator\": \"Equals\",\n",
    "            \"Value\": \"Training\",\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "trial_component_analytics = ExperimentAnalytics(\n",
    "    sagemaker_session= sagemaker_session,\n",
    "    experiment_name= experiment_name,\n",
    "    search_expression=search_expression,\n",
    ")\n",
    "\n",
    "trial_component_analytics.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 평가 지표에 순서에 따른 시도 보기\n",
    "- 아래는 모델 평가 지표에 따른 순서로 보여주기 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrialComponentName</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>SourceArn</th>\n",
       "      <th>epochs</th>\n",
       "      <th>train_batch_size</th>\n",
       "      <th>Accuracy - Min</th>\n",
       "      <th>Accuracy - Max</th>\n",
       "      <th>Accuracy - Avg</th>\n",
       "      <th>Accuracy - StdDev</th>\n",
       "      <th>Accuracy - Last</th>\n",
       "      <th>Accuracy - Count</th>\n",
       "      <th>test - MediaType</th>\n",
       "      <th>test - Value</th>\n",
       "      <th>train - MediaType</th>\n",
       "      <th>train - Value</th>\n",
       "      <th>SageMaker.DebugHookOutput - MediaType</th>\n",
       "      <th>SageMaker.DebugHookOutput - Value</th>\n",
       "      <th>SageMaker.ModelArtifact - MediaType</th>\n",
       "      <th>SageMaker.ModelArtifact - Value</th>\n",
       "      <th>Trials</th>\n",
       "      <th>Experiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pytorch-training-2022-06-08-14-54-26-322-aws-t...</td>\n",
       "      <td>Training</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:154364950293:train...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.88525</td>\n",
       "      <td>0.88525</td>\n",
       "      <td>0.88525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.88525</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>s3://sagemaker-us-east-1-154364950293/KoElectr...</td>\n",
       "      <td>None</td>\n",
       "      <td>s3://sagemaker-us-east-1-154364950293/KoElectr...</td>\n",
       "      <td>None</td>\n",
       "      <td>s3://sagemaker-us-east-1-154364950293/</td>\n",
       "      <td>None</td>\n",
       "      <td>s3://sagemaker-us-east-1-154364950293/pytorch-...</td>\n",
       "      <td>[KoElectra-HF-2022-06-08-14-53-56-437716]</td>\n",
       "      <td>[KoElectra-HF]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  TrialComponentName DisplayName  \\\n",
       "0  pytorch-training-2022-06-08-14-54-26-322-aws-t...    Training   \n",
       "\n",
       "                                           SourceArn  epochs  \\\n",
       "0  arn:aws:sagemaker:us-east-1:154364950293:train...     5.0   \n",
       "\n",
       "   train_batch_size  Accuracy - Min  Accuracy - Max  Accuracy - Avg  \\\n",
       "0             256.0         0.88525         0.88525         0.88525   \n",
       "\n",
       "   Accuracy - StdDev  Accuracy - Last  Accuracy - Count test - MediaType  \\\n",
       "0                0.0          0.88525                 1             None   \n",
       "\n",
       "                                        test - Value train - MediaType  \\\n",
       "0  s3://sagemaker-us-east-1-154364950293/KoElectr...              None   \n",
       "\n",
       "                                       train - Value  \\\n",
       "0  s3://sagemaker-us-east-1-154364950293/KoElectr...   \n",
       "\n",
       "  SageMaker.DebugHookOutput - MediaType  \\\n",
       "0                                  None   \n",
       "\n",
       "        SageMaker.DebugHookOutput - Value SageMaker.ModelArtifact - MediaType  \\\n",
       "0  s3://sagemaker-us-east-1-154364950293/                                None   \n",
       "\n",
       "                     SageMaker.ModelArtifact - Value  \\\n",
       "0  s3://sagemaker-us-east-1-154364950293/pytorch-...   \n",
       "\n",
       "                                      Trials     Experiments  \n",
       "0  [KoElectra-HF-2022-06-08-14-53-56-437716]  [KoElectra-HF]  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trial_component_training_analytics = ExperimentAnalytics(\n",
    "    sagemaker_session= sagemaker_session,\n",
    "    experiment_name= experiment_name,\n",
    "    search_expression=search_expression,\n",
    "    sort_by=\"metrics.Accuracy.max\",        \n",
    "    sort_order=\"Descending\",\n",
    "    metric_names=[\"Accuracy\"],    \n",
    "    parameter_names=[\"epochs\", \"train_batch_size\",\n",
    "                    ],\n",
    ")\n",
    "\n",
    "trial_component_training_analytics.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 모델 아티펙트 저장\n",
    "- S3 에 저장된 모델 아티펙트를 저장하여 추론시 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifact_path:  s3://sagemaker-us-east-1-154364950293/pytorch-training-2022-06-08-14-54-26-322/output/model.tar.gz\n",
      "Stored 'artifact_path' (str)\n"
     ]
    }
   ],
   "source": [
    "artifact_path = host_estimator.model_data\n",
    "print(\"artifact_path: \", artifact_path)\n",
    "\n",
    "%store artifact_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기타 변수 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'bucket' (str)\n",
      "Stored 'prefix' (str)\n"
     ]
    }
   ],
   "source": [
    "%store bucket \n",
    "%store prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
