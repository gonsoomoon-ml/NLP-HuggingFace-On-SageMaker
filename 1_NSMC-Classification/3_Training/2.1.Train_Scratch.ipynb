{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [모듈 2.1] 모델 훈련 스크래치\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 노트북은 아래와 같은 주요 작업을 합니다.\n",
    "- 1. 환경 설정\n",
    "- 2. 데이터 로딩\n",
    "- 3. Hugging Face Electra tokenizer 및 pre-trained model 사용\n",
    "- 4. torch custome Dataset 생성 및 훈련 준비\n",
    "- 5. 모델 Fine-Tuning\n",
    "    - 5.1. Fine-tuning with Trainer\n",
    "    - 5.2. 파이썬 스크립트로 훈련    \n",
    "    - 5.3. Fine-tuning with native PyTorch    \n",
    "\n",
    "    \n",
    "---\n",
    "### 참고:\n",
    "- 커스텀 데이터 셋으로 파인 튜닝을 위한 참조 자료\n",
    "    - [Fine-tuning with custom datasets](https://huggingface.co/transformers/v3.2.0/custom_datasets.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# src 폴더 경로 설정\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "import config\n",
    "from  data_util import read_nsmc_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "# logger.setLevel(logging.WARNING)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r local_train_output_path\n",
    "%store -r local_test_output_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 로딩\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. 학습 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels = read_nsmc_split(local_train_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 149552 \n",
      "Sample: ['흠   포스터보고 초딩영화줄    오버연기조차 가볍지 않구나', '너무재밓었다그래서보는것을추천한다', '교도소 이야기구먼   솔직히 재미는 없다  평점 조정', '사이몬페그의 익살스런 연기가 돋보였던 영화 스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다', '막 걸음마 뗀 세부터 초등학교 학년생인 살용영화 ㅋㅋㅋ   별반개도 아까움']\n",
      "len: 149552 \n",
      "Sample: [1, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"len: {len(train_texts)} \\nSample: {train_texts[0:5]}\")\n",
    "logger.info(f\"len: {len(train_labels)} \\nSample: {train_labels[0:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. 검증 데이터 셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hugging Face Electra tokenizer 및 pre-trained model 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Electra 라이브러리 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "from transformers import (\n",
    "    ElectraModel, \n",
    "    ElectraTokenizer, \n",
    "    ElectraForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    set_seed\n",
    ")\n",
    "# from transformers.trainer_utils import get_last_checkpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Pre-trained model_id, tokenizer_id 지정\n",
    "- [KoElectra Git](https://github.com/monologg/KoELECTRA)\n",
    "- KoElectra Model\n",
    "    - Small:\n",
    "        - \"monologg/koelectra-small-v3-discriminator\n",
    "    - Base: \n",
    "        - monologg/koelectra-base-v3-discriminator\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_id = 'monologg/koelectra-small-v3-discriminator'\n",
    "model_id = \"monologg/koelectra-small-v3-discriminator\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Electra Model 입력 인코딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e39e27227441ff8e0cd7ee8c8343b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/257k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc790d3b10c44b3bb8db92001de7dcc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/61.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66a52083a5a4ab29303e2875dbbde18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/458 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.1 s, sys: 316 ms, total: 33.4 s\n",
      "Wall time: 33.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "tokenizer = ElectraTokenizer.from_pretrained(tokenizer_id)\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "# test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of train_encoding: <class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'type of train_encoding: {type(val_encodings)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. torch custome Dataset 생성 및 훈련 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. torch custome dataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_util import NSMCDataset\n",
    "\n",
    "train_dataset = NSMCDataset(train_encodings, train_labels)\n",
    "val_dataset = NSMCDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_dataset) : 119641\n",
      "len(val_dataset) : 29911\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"len(train_dataset) : {len(train_dataset)}\")\n",
    "logger.info(f\"len(val_dataset) : {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. 데이터 셋 부가 정보 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_util import create_train_meta\n",
    "# Prepare model labels - useful in inference API\n",
    "seed = 100\n",
    "\n",
    "# Set seed before initializing model\n",
    "set_seed(seed)\n",
    "    \n",
    "num_labels, label2id, id2label = create_train_meta(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5. 모델 Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## 5.1. Fine-tuning with Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e4828db7b6402a8581f4d791a4839c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/54.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-small-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 119641\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 468\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='468' max='468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [468/468 02:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.692300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.597700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.443400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.388800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 52s, sys: 36.3 s, total: 2min 28s\n",
      "Wall time: 2min 26s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=468, training_loss=0.5070040490892198, metrics={'train_runtime': 140.3045, 'train_samples_per_second': 852.724, 'train_steps_per_second': 3.336, 'total_flos': 845575811718948.0, 'train_loss': 0.5070040490892198, 'epoch': 1.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # total number of training epochs\n",
    "    per_device_train_batch_size=256,  # batch size per device during training\n",
    "    per_device_eval_batch_size=256,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "model = ElectraForSequenceClassification.from_pretrained(\n",
    "    model_id, num_labels=num_labels, label2id=label2id, id2label=id2label\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5.2. 파이썬 스크립트 및 Trainer 훈련¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of epochs:  1\n"
     ]
    }
   ],
   "source": [
    "class ParamsScript:\n",
    "    def __init__(self):\n",
    "        self.epochs = 1        \n",
    "        self.train_batch_size = 256\n",
    "        self.eval_batch_size = 256\n",
    "        self.test_batch_size = 256         \n",
    "        self.learning_rate = 5e-5\n",
    "        self.warmup_steps = 0      \n",
    "        self.weight_decay = 0.01\n",
    "        self.fp16 = True\n",
    "        self.tokenizer_id = 'monologg/koelectra-small-v3-discriminator'\n",
    "        self.model_id = 'monologg/koelectra-small-v3-discriminator'     \n",
    "        # SageMaker Container environment        \n",
    "        self.output_data_dir = f\"{config.output_data_dir}\"                                            \n",
    "        self.model_dir = f\"{config.model_dir}\"                                       \n",
    "        self.train_data_dir = f\"{config.train_data_dir}\"               \n",
    "        self.checkpoint_dir = f\"{config.checkpoint_dir}\"                                               \n",
    "        self.is_evaluation = True\n",
    "        self.is_test = True\n",
    "        self.test_data_dir = f\"{config.test_data_dir}\"                               \n",
    "        self.eval_ratio = 0.5\n",
    "        self.use_subset_train_sampler = False\n",
    "        self.disable_tqdm = True        \n",
    "        self.logging_steps = 50\n",
    "        self.seed = 100\n",
    "                        \n",
    "script_args = ParamsScript()\n",
    "print(\"# of epochs: \", script_args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> Load Input Arguemtn <===========\n",
      "##### Args: \n",
      " {'epochs': 1, 'train_batch_size': 256, 'eval_batch_size': 256, 'test_batch_size': 256, 'learning_rate': 5e-05, 'warmup_steps': 0, 'weight_decay': 0.01, 'fp16': True, 'tokenizer_id': 'monologg/koelectra-small-v3-discriminator', 'model_id': 'monologg/koelectra-small-v3-discriminator', 'output_data_dir': 'output/nsmc', 'model_dir': 'models/nsmc', 'train_data_dir': 'data/nsmc/train', 'checkpoint_dir': 'checkpoint/nsmc', 'is_evaluation': True, 'is_test': True, 'test_data_dir': 'data/nsmc/test', 'eval_ratio': 0.5, 'use_subset_train_sampler': False, 'disable_tqdm': True, 'logging_steps': 50, 'seed': 100}\n",
      "device: cuda\n",
      "=====> data loading <===========\n",
      "train_data_filenames ['data/nsmc/train/ratings_train.txt']\n",
      "train_file_path data/nsmc/train/ratings_train.txt\n",
      "len: 149552 \n",
      "Sample: ['흠   포스터보고 초딩영화줄    오버연기조차 가볍지 않구나', '너무재밓었다그래서보는것을추천한다', '교도소 이야기구먼   솔직히 재미는 없다  평점 조정', '사이몬페그의 익살스런 연기가 돋보였던 영화 스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다', '막 걸음마 뗀 세부터 초등학교 학년생인 살용영화 ㅋㅋㅋ   별반개도 아까움']\n",
      "len: 149552 \n",
      "Sample: [1, 0, 0, 1, 0]\n",
      "=====> Loading Train Dataset <===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/vocab.txt from cache at /home/ec2-user/.cache/huggingface/transformers/32dc9196217c0cc26c7dd705168e8615ea2d82613aa5b672d7647b8e8d58545f.541023ff50f833a9bab3e48e78ae1856cf6744bdb336c86e797eaf675b62b2b8\n",
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/tokenizer_config.json from cache at /home/ec2-user/.cache/huggingface/transformers/a6c32c62ff893fb2aa32dabc5722c9f9eb7243cc1cb4514b9ba3fdb1b52704d6.35f013c4fd3572cfdddbbdf6223ef162dd4fb536bf83007533f201addf3287b7\n",
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/bd0f09888c5a5619ddb9de81d4a9936a94e5f45064f9a23ba6d39241ceebce02.d2485d28e5c07ca60bfa4fe84af673e0df83401e5c56bcdd991878cb4966eb34\n",
      "Model config ElectraConfig {\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 35000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train_dataset : 74776\n",
      "dataset size with frac: 1 ==> 74776\n",
      "=====> model loading <===========\n",
      "num_labels: 2\n",
      "label2id: {'negative': '0', 'positive': '1'}\n",
      "id2label: {'0': 'negative', '1': 'positive'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/bd0f09888c5a5619ddb9de81d4a9936a94e5f45064f9a23ba6d39241ceebce02.d2485d28e5c07ca60bfa4fe84af673e0df83401e5c56bcdd991878cb4966eb34\n",
      "Model config ElectraConfig {\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": {\n",
      "    \"negative\": \"0\",\n",
      "    \"positive\": \"1\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 35000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/pytorch_model.bin from cache at /home/ec2-user/.cache/huggingface/transformers/052bc3ecf8c8484f1519650794b32b6d2c70750bcba71d1f763951514b5cf0c8.84bb33167d2e89d46f4cde129b2f4c447618ac33ac46f2012ee9e5e706fec112\n",
      "Some weights of the model checkpoint at monologg/koelectra-small-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> Define Trainer <===========\n",
      "=====> Loading Validation Dataset <===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/vocab.txt from cache at /home/ec2-user/.cache/huggingface/transformers/32dc9196217c0cc26c7dd705168e8615ea2d82613aa5b672d7647b8e8d58545f.541023ff50f833a9bab3e48e78ae1856cf6744bdb336c86e797eaf675b62b2b8\n",
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/tokenizer_config.json from cache at /home/ec2-user/.cache/huggingface/transformers/a6c32c62ff893fb2aa32dabc5722c9f9eb7243cc1cb4514b9ba3fdb1b52704d6.35f013c4fd3572cfdddbbdf6223ef162dd4fb536bf83007533f201addf3287b7\n",
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/bd0f09888c5a5619ddb9de81d4a9936a94e5f45064f9a23ba6d39241ceebce02.d2485d28e5c07ca60bfa4fe84af673e0df83401e5c56bcdd991878cb4966eb34\n",
      "Model config ElectraConfig {\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 35000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of val_dataset : 74776\n",
      "dataset size with frac: 1 ==> 74776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp fp16 backend\n",
      "***** Running training *****\n",
      "  Num examples = 74776\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6625, 'learning_rate': 4.1467576791808876e-05, 'epoch': 0.17}\n",
      "{'loss': 0.496, 'learning_rate': 3.293515358361775e-05, 'epoch': 0.34}\n",
      "{'loss': 0.4314, 'learning_rate': 2.4402730375426623e-05, 'epoch': 0.51}\n",
      "{'loss': 0.405, 'learning_rate': 1.5870307167235497e-05, 'epoch': 0.68}\n",
      "{'loss': 0.4003, 'learning_rate': 7.337883959044369e-06, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 74776\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to checkpoint/nsmc/checkpoint-293\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in checkpoint/nsmc/checkpoint-293/config.json\n",
      "Model weights saved in checkpoint/nsmc/checkpoint-293/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3717063069343567, 'eval_accuracy': 0.8420482507756499, 'eval_f1': 0.8415715416292202, 'eval_precision': 0.8426226866152731, 'eval_recall': 0.8405230159155458, 'eval_runtime': 28.193, 'eval_samples_per_second': 2652.286, 'eval_steps_per_second': 10.393, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from checkpoint/nsmc/checkpoint-293 (score: 0.8420482507756499).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 87.1084, 'train_samples_per_second': 858.424, 'train_steps_per_second': 3.364, 'train_loss': 0.4650391067661116, 'epoch': 1.0}\n",
      "=====> Loading Test Files <===========\n",
      "test_data_filenames ['data/nsmc/test/ratings_test.txt']\n",
      "test_file_path data/nsmc/test/ratings_test.txt\n",
      "len: 49832 \n",
      "Sample: ['뭐야 이 평점들은     나쁘진 않지만 점 짜리는 더더욱 아니잖아', '지루하지는 않은데 완전 막장임    돈주고 보기에는', '만 아니었어도 별 다섯 개 줬을텐데   왜 로 나와서 제 심기를 불편하게 하죠', '음악이 주가 된  최고의 음악영화', '진정한 쓰레기']\n",
      "len: 49832 \n",
      "Sample: [0, 0, 0, 1, 0]\n",
      "=====> Loading Test Dataset <===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/vocab.txt from cache at /home/ec2-user/.cache/huggingface/transformers/32dc9196217c0cc26c7dd705168e8615ea2d82613aa5b672d7647b8e8d58545f.541023ff50f833a9bab3e48e78ae1856cf6744bdb336c86e797eaf675b62b2b8\n",
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/tokenizer_config.json from cache at /home/ec2-user/.cache/huggingface/transformers/a6c32c62ff893fb2aa32dabc5722c9f9eb7243cc1cb4514b9ba3fdb1b52704d6.35f013c4fd3572cfdddbbdf6223ef162dd4fb536bf83007533f201addf3287b7\n",
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/bd0f09888c5a5619ddb9de81d4a9936a94e5f45064f9a23ba6d39241ceebce02.d2485d28e5c07ca60bfa4fe84af673e0df83401e5c56bcdd991878cb4966eb34\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Model config ElectraConfig {\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 35000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of test_dataset : 49832\n",
      "dataset size with frac: 1 ==> 49832\n",
      "=====> Prediction on Test Dataset <===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 49832\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics: {'accuracy': 0.841, 'f1': 0.842, 'precision': 0.844, 'recall': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to models/nsmc\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Configuration saved in models/nsmc/config.json\n",
      "Model weights saved in models/nsmc/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 12s, sys: 21.1 s, total: 2min 33s\n",
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from train_lib import train_Trainer\n",
    "train_Trainer(script_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5.3. Fine-tuning with native PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train data loader 생성\n",
    "- 디버깅을 위해 일부 데이터 셋 사용시\n",
    "    - train_sample_loader\n",
    "    - eval_sample_loader\n",
    "- 풀 데이터 셋 사용시\n",
    "    - train_loader\n",
    "    - eval_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size with frac: 0.01 ==> 1196\n",
      "dataset size with frac: 1 ==> 119641\n",
      "dataset size with frac: 0.001 ==> 29\n",
      "dataset size with frac: 1 ==> 29911\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "\n",
    "from train_util import create_random_sampler\n",
    "    \n",
    "subset_train_sampler = create_random_sampler(train_dataset, frac=0.01, is_shuffle=True, logger=logger)\n",
    "train_sampler = create_random_sampler(train_dataset, frac=1, is_shuffle=True, logger=logger)\n",
    "\n",
    "subset_eval_sampler = create_random_sampler(val_dataset, frac=0.001, is_shuffle=False, logger=logger)\n",
    "eval_sampler = create_random_sampler(val_dataset, frac=1, is_shuffle=False, logger=logger)\n",
    "\n",
    "# subset_test_sampler = create_random_sampler(test_dataset, frac=0.001, is_shuffle=False, logger=logger)\n",
    "# test_sampler = create_random_sampler(test_dataset, frac=1, is_shuffle=False, logger=logger)\n",
    "    \n",
    "train_sample_loader = DataLoader(dataset=train_dataset, \n",
    "                          shuffle=False, \n",
    "                          batch_size=16, \n",
    "                          sampler=subset_train_sampler)    \n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          shuffle=False, \n",
    "                          batch_size=16, \n",
    "                          sampler=train_sampler)    \n",
    "\n",
    "eval_sample_loader = DataLoader(dataset=val_dataset, \n",
    "                          shuffle=False, \n",
    "                          batch_size=16, \n",
    "                          sampler=subset_eval_sampler)    \n",
    "\n",
    "eval_loader = DataLoader(dataset=val_dataset, \n",
    "                          shuffle=False, \n",
    "                          batch_size=16, \n",
    "                          sampler=eval_sampler)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파라미터 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of epochs:  1\n"
     ]
    }
   ],
   "source": [
    "class Params:\n",
    "    def __init__(self):\n",
    "        self.epochs = 1        \n",
    "        self.batch_size = 256\n",
    "        self.lr = 0.001\n",
    "        self.log_interval = 50\n",
    "        self.model_dir = config.model_dir\n",
    "                        \n",
    "args = Params()\n",
    "print(\"# of epochs: \", args.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/bd0f09888c5a5619ddb9de81d4a9936a94e5f45064f9a23ba6d39241ceebce02.d2485d28e5c07ca60bfa4fe84af673e0df83401e5c56bcdd991878cb4966eb34\n",
      "Model config ElectraConfig {\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": {\n",
      "    \"negative\": \"0\",\n",
      "    \"positive\": \"1\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 35000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/pytorch_model.bin from cache at /home/ec2-user/.cache/huggingface/transformers/052bc3ecf8c8484f1519650794b32b6d2c70750bcba71d1f763951514b5cf0c8.84bb33167d2e89d46f4cde129b2f4c447618ac33ac46f2012ee9e5e706fec112\n",
      "Some weights of the model checkpoint at monologg/koelectra-small-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = ElectraForSequenceClassification.from_pretrained(\n",
    "    model_id, num_labels=num_labels, label2id=label2id, id2label=id2label\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 루프 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [200/1196 (67%)] Loss=0.684317;\n",
      "The time elapse of epoch 000 is: 00: 00: 03\n",
      "Train Epoch: 0 Acc=0.814904;\n",
      "the model is saved at models/nsmc/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "from train_util import train_epoch, eval_epoch, save_best_model\n",
    "import time\n",
    "\n",
    "epochs = 1\n",
    "best_acc = 0\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_epoch(args, \n",
    "                model, \n",
    "                train_sample_loader, \n",
    "                optimizer, \n",
    "                epoch, \n",
    "                device, \n",
    "                logger,\n",
    "                sampler=None, \n",
    "                )            \n",
    "\n",
    "    elapsed_time = time.time() - start_time    \n",
    "    print(\"The time elapse of epoch {:03d}\".format(epoch) + \" is: \" + \n",
    "                time.strftime(\"%H: %M: %S\", time.gmtime(elapsed_time)))\n",
    "\n",
    "    acc = eval_epoch(args, \n",
    "               model, \n",
    "               epoch, \n",
    "               device, \n",
    "               logger,\n",
    "               eval_sample_loader)\n",
    "    \n",
    "    best_acc = save_best_model(model, \n",
    "                               acc, \n",
    "                               epoch, \n",
    "                               best_acc,\n",
    "                               args.model_dir,\n",
    "                               logger)            \n",
    "    # best_hr, best_ndcg, best_epoch = test(args, NCF_model, epoch, test_loader, best_hr, model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5.4. 파이썬 스크립트 및 Pytorch 로 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of epochs:  1\n"
     ]
    }
   ],
   "source": [
    "class ParamsScript:\n",
    "    def __init__(self):\n",
    "        self.epochs = 1        \n",
    "        self.train_batch_size = 256\n",
    "        self.eval_batch_size = 256\n",
    "        self.test_batch_size = 256         \n",
    "        self.learning_rate = 5e-5\n",
    "        self.warmup_steps = 0      \n",
    "        self.fp16 = True\n",
    "        self.tokenizer_id = 'monologg/koelectra-small-v3-discriminator'\n",
    "        self.model_id = 'monologg/koelectra-small-v3-discriminator'     \n",
    "        # SageMaker Container environment        \n",
    "        self.output_data_dir = f\"{config.output_data_dir}\"                                            \n",
    "        self.model_dir = f\"{config.model_dir}\"                                       \n",
    "        self.train_data_dir = f\"{config.train_data_dir}\"               \n",
    "        self.checkpoint_dir = f\"{config.checkpoint_dir}\"                                               \n",
    "        self.is_evaluation = config.is_evaluation                               \n",
    "        self.is_test = True\n",
    "        self.test_data_dir = f\"{config.test_data_dir}\"                               \n",
    "        self.eval_ratio = 0.5\n",
    "        self.use_subset_train_sampler = True \n",
    "        self.log_interval = 50\n",
    "        self.n_gpus = 1                        \n",
    "        self.seed = 100\n",
    "                        \n",
    "script_args = ParamsScript()\n",
    "print(\"# of epochs: \", script_args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> Load Input Arguemtn <===========\n",
      "##### Args: \n",
      " {'epochs': 1, 'train_batch_size': 256, 'eval_batch_size': 256, 'test_batch_size': 256, 'learning_rate': 5e-05, 'warmup_steps': 0, 'fp16': True, 'tokenizer_id': 'monologg/koelectra-small-v3-discriminator', 'model_id': 'monologg/koelectra-small-v3-discriminator', 'output_data_dir': 'output/nsmc', 'model_dir': 'models/nsmc', 'train_data_dir': 'data/nsmc/train', 'checkpoint_dir': 'checkpoint/nsmc', 'is_evaluation': 'True', 'is_test': True, 'test_data_dir': 'data/nsmc/test', 'eval_ratio': 0.5, 'use_subset_train_sampler': True, 'log_interval': 50, 'n_gpus': 1, 'seed': 100}\n",
      "device: cuda\n",
      "=====> data loading <===========\n",
      "train_data_filenames ['data/nsmc/train/ratings_train.txt']\n",
      "train_file_path data/nsmc/train/ratings_train.txt\n",
      "len: 149552 \n",
      "Sample: ['흠   포스터보고 초딩영화줄    오버연기조차 가볍지 않구나', '너무재밓었다그래서보는것을추천한다', '교도소 이야기구먼   솔직히 재미는 없다  평점 조정', '사이몬페그의 익살스런 연기가 돋보였던 영화 스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다', '막 걸음마 뗀 세부터 초등학교 학년생인 살용영화 ㅋㅋㅋ   별반개도 아까움']\n",
      "len: 149552 \n",
      "Sample: [1, 0, 0, 1, 0]\n",
      "=====> Loading Train Dataset <===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/vocab.txt from cache at /home/ec2-user/.cache/huggingface/transformers/32dc9196217c0cc26c7dd705168e8615ea2d82613aa5b672d7647b8e8d58545f.541023ff50f833a9bab3e48e78ae1856cf6744bdb336c86e797eaf675b62b2b8\n",
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/tokenizer_config.json from cache at /home/ec2-user/.cache/huggingface/transformers/a6c32c62ff893fb2aa32dabc5722c9f9eb7243cc1cb4514b9ba3fdb1b52704d6.35f013c4fd3572cfdddbbdf6223ef162dd4fb536bf83007533f201addf3287b7\n",
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/bd0f09888c5a5619ddb9de81d4a9936a94e5f45064f9a23ba6d39241ceebce02.d2485d28e5c07ca60bfa4fe84af673e0df83401e5c56bcdd991878cb4966eb34\n",
      "Model config ElectraConfig {\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 35000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train_dataset : 74776\n",
      "dataset size with frac: 0.01 ==> 747\n",
      "=====> model loading <===========\n",
      "num_labels: 2\n",
      "label2id: {'negative': '0', 'positive': '1'}\n",
      "id2label: {'0': 'negative', '1': 'positive'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/bd0f09888c5a5619ddb9de81d4a9936a94e5f45064f9a23ba6d39241ceebce02.d2485d28e5c07ca60bfa4fe84af673e0df83401e5c56bcdd991878cb4966eb34\n",
      "Model config ElectraConfig {\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": {\n",
      "    \"negative\": \"0\",\n",
      "    \"positive\": \"1\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 35000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/pytorch_model.bin from cache at /home/ec2-user/.cache/huggingface/transformers/052bc3ecf8c8484f1519650794b32b6d2c70750bcba71d1f763951514b5cf0c8.84bb33167d2e89d46f4cde129b2f4c447618ac33ac46f2012ee9e5e706fec112\n",
      "Some weights of the model checkpoint at monologg/koelectra-small-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> Training Loop <===========\n",
      "=====> Loading Validation Dataset <===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/vocab.txt from cache at /home/ec2-user/.cache/huggingface/transformers/32dc9196217c0cc26c7dd705168e8615ea2d82613aa5b672d7647b8e8d58545f.541023ff50f833a9bab3e48e78ae1856cf6744bdb336c86e797eaf675b62b2b8\n",
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/tokenizer_config.json from cache at /home/ec2-user/.cache/huggingface/transformers/a6c32c62ff893fb2aa32dabc5722c9f9eb7243cc1cb4514b9ba3fdb1b52704d6.35f013c4fd3572cfdddbbdf6223ef162dd4fb536bf83007533f201addf3287b7\n",
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/bd0f09888c5a5619ddb9de81d4a9936a94e5f45064f9a23ba6d39241ceebce02.d2485d28e5c07ca60bfa4fe84af673e0df83401e5c56bcdd991878cb4966eb34\n",
      "Model config ElectraConfig {\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 35000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of val_dataset : 74776\n",
      "dataset size with frac: 1 ==> 74776\n",
      "The time elapse of epoch 000 is: 00: 00: 00\n",
      "Train Epoch: 0 Acc=0.502182;\n",
      "the model is saved at models/nsmc/pytorch_model.bin\n",
      "=====> test model performance <===========\n",
      "=====> Loading Test Dataset <===========\n",
      "test_data_filenames ['data/nsmc/test/ratings_test.txt']\n",
      "test_file_path data/nsmc/test/ratings_test.txt\n",
      "len: 49832 \n",
      "Sample: ['뭐야 이 평점들은     나쁘진 않지만 점 짜리는 더더욱 아니잖아', '지루하지는 않은데 완전 막장임    돈주고 보기에는', '만 아니었어도 별 다섯 개 줬을텐데   왜 로 나와서 제 심기를 불편하게 하죠', '음악이 주가 된  최고의 음악영화', '진정한 쓰레기']\n",
      "len: 49832 \n",
      "Sample: [0, 0, 0, 1, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/vocab.txt from cache at /home/ec2-user/.cache/huggingface/transformers/32dc9196217c0cc26c7dd705168e8615ea2d82613aa5b672d7647b8e8d58545f.541023ff50f833a9bab3e48e78ae1856cf6744bdb336c86e797eaf675b62b2b8\n",
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/tokenizer_config.json from cache at /home/ec2-user/.cache/huggingface/transformers/a6c32c62ff893fb2aa32dabc5722c9f9eb7243cc1cb4514b9ba3fdb1b52704d6.35f013c4fd3572cfdddbbdf6223ef162dd4fb536bf83007533f201addf3287b7\n",
      "loading file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/monologg/koelectra-small-v3-discriminator/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/bd0f09888c5a5619ddb9de81d4a9936a94e5f45064f9a23ba6d39241ceebce02.d2485d28e5c07ca60bfa4fe84af673e0df83401e5c56bcdd991878cb4966eb34\n",
      "Model config ElectraConfig {\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.11.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 35000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of test_dataset : 49832\n",
      "dataset size with frac: 1 ==> 49832\n",
      "Test Accuracy: Acc=0.496422;\n",
      "CPU times: user 1min 25s, sys: 10.7 s, total: 1min 36s\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from train_lib import train\n",
    "train(script_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. 커널 리스타팅\n",
    "\n",
    "- 위의 노트북을 다 실행하고 나면 아래의 그림과 같이 GPU의 메모리를 차지하고 있습니다. (터미널에서 `nvidia-smi` 입력) \n",
    "![before-nvidia-smi.png](../2_WarmingUp/img/before-nvidia-smi.png)\n",
    "\n",
    "- 아래 셀을 실행하면 이 노트북의 커널이 리스타트 되고 해제된 메모리를 확인 할 수 있습니다.\n",
    "![after-nvidia-smi.png](../2_WarmingUp/img/after-nvidia-smi.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
