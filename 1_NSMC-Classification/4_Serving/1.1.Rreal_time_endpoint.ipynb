{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b1b9946",
   "metadata": {},
   "source": [
    "# [모듈 1.1] Real-Time Endpoint 배포 및 추론\n",
    "---\n",
    "\n",
    "본 워크샵의 모든 노트북은 `conda_python3` 추가 패키지를 설치하고 모두 이 커널 에서 작업 합니다.\n",
    "\n",
    "이 노트북은 아래와 같은 작업을 합니다.\n",
    "\n",
    "- 1. 환경 셋업\n",
    "- 2. 모델 패키징\n",
    "- 3. SageMaker Endpoint Deployment\n",
    "- 4. Inference\n",
    "- 5. 테스트 데이터 추론 및 성능 평가\n",
    "- 6. 엔드 포인트 삭제\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2a1614",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 참고\n",
    "- 김대근님의 리포에서 SageMaker Hugging Face Inference Toolkit 등을 자세히 설명 함.\n",
    "    - [Lab 2-1: Deploy Hugging Face Transformers in SageMaker Real-time Endpoint](https://github.com/daekeun-ml/sm-huggingface-kornlp/blob/main/lab_2_serving/1_real_time_endpoint.ipynb)\n",
    "    \n",
    "    \n",
    "- HF 공식 사이트 에서 SM Endpint 를 배포하는 가이드\n",
    "    - [Deploy models to Amazon SageMaker](https://huggingface.co/docs/sagemaker/inference#deploy-a-🤗-transformers-model-trained-in-sagemaker)\n",
    "    \n",
    "    \n",
    "- SageMaker Hugging Face Inference Toolkit: \n",
    "    - https://github.com/aws/sagemaker-huggingface-inference-toolkit    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07419d60",
   "metadata": {},
   "source": [
    "# 1. 환경 셋업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e19caa",
   "metadata": {},
   "source": [
    "## 기본 세팅\n",
    "사용하는 패키지는 import 시점에 다시 재로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25aabe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "# 서빙 코드\n",
    "sys.path.append('./src')\n",
    "# 훈련 코드\n",
    "sys.path.append('../3_Training/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba0463fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r bucket\n",
    "%store -r prefix\n",
    "%store -r artifact_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fef7b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 커스텀 라이브러리\n",
    "import config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4096d9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33ea08e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import logging\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker import session\n",
    "from transformers import ElectraConfig\n",
    "from transformers import (\n",
    "    ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n",
    ")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='[{%(filename)s:%(lineno)d} %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(filename='tmp.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298ad45f",
   "metadata": {},
   "source": [
    "# 2. 모델 패키징\n",
    "- 모델 아티펙트를 다운로드 합니다.\n",
    "- 다운로드 받은 모델 아티펙트의 압축을 해제하고 모델 가중치인 model.pth 파일을 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a29f161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_data_dir:  models/nsmc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import config\n",
    "\n",
    "model_data_dir = config.model_dir\n",
    "os.makedirs(model_data_dir, exist_ok=True)\n",
    "print(\"model_data_dir: \", model_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adef4c7",
   "metadata": {},
   "source": [
    "### 모델 패키징 폴더 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6baed41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/nsmc\n"
     ]
    }
   ],
   "source": [
    "%%sh -s {model_data_dir}\n",
    "\n",
    "model_data_dir=$1\n",
    "\n",
    "echo $model_data_dir\n",
    "\n",
    "# 기존 데이터 삭제\n",
    "rm -rf $model_data_dir/*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b477bc68",
   "metadata": {},
   "source": [
    "### 모델 및 토큰나이저 아티펙트 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d060540e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer_id: monologg/koelectra-small-v3-discriminator\n",
      "model_id: monologg/koelectra-small-v3-discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n",
      "Some weights of the model checkpoint at monologg/koelectra-small-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-small-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/nsmc/tokenizer_config.json',\n",
       " 'models/nsmc/special_tokens_map.json',\n",
       " 'models/nsmc/vocab.txt',\n",
       " 'models/nsmc/added_tokens.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_id = config.tokenizer_id\n",
    "model_id = config.model_id\n",
    "\n",
    "print(f\"tokenizer_id: {tokenizer_id}\")\n",
    "print(f\"model_id: {model_id}\")\n",
    "\n",
    "# Download model and tokenizer\n",
    "model = ElectraForSequenceClassification.from_pretrained(model_id)\n",
    "tokenizer = ElectraTokenizer.from_pretrained(tokenizer_id)\n",
    "\n",
    "# os.makedirs(model_dir, exist_ok=True)\n",
    "model.save_pretrained(model_data_dir)\n",
    "tokenizer.save_pretrained(model_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc69398",
   "metadata": {},
   "source": [
    "### pre-trained 모델 가중치 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03a771b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/nsmc\n"
     ]
    }
   ],
   "source": [
    "%%sh -s {model_data_dir}\n",
    "\n",
    "model_data_dir=$1\n",
    "\n",
    "echo $model_data_dir\n",
    "\n",
    "# 기존 모델 삭제ㅁ\n",
    "rm -rf pytorch_model.bin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c17ddf",
   "metadata": {},
   "source": [
    "### fine-tunnig 된 모델 가중치 다운로드 및 이름 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b603290a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-154364950293/pytorch-training-2022-06-08-14-54-26-322/output/model.tar.gz\n",
      "models/nsmc\n",
      "download: s3://sagemaker-us-east-1-154364950293/pytorch-training-2022-06-08-14-54-26-322/output/model.tar.gz to models/nsmc/model.tar.gz\n",
      "sentimental-electro-hf.pth\n"
     ]
    }
   ],
   "source": [
    "%%sh -s {artifact_path} {model_data_dir}\n",
    "\n",
    "artifact_path=$1\n",
    "model_data_dir=$2\n",
    "\n",
    "echo $artifact_path\n",
    "echo $model_data_dir\n",
    "\n",
    "\n",
    "# 모델을 S3에서 로컬로 다운로드\n",
    "aws s3 cp $artifact_path $model_data_dir\n",
    "\n",
    "# 모델 다운로드 폴더로 이동\n",
    "cd $model_data_dir\n",
    "\n",
    "# 압축 해제\n",
    "tar -xvf model.tar.gz  \n",
    "\n",
    "rm -rf model.tar.gz  \n",
    "\n",
    "# 훈련된 가중치를 약속된 pytorch_model.bin 이름으로 변경ㅁ\n",
    "mv sentimental-electro-hf.pth pytorch_model.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b662e5c8",
   "metadata": {},
   "source": [
    "### 모델 아테펙트를 model.tar.gz 로 압축"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159361b3",
   "metadata": {},
   "source": [
    "모델 파라메터 및 토크나이저를 `model.tar.gz`으로 압축합니다. 압축 파일명은 자유롭게 지정할 수 있으나, 반드시 `tar.gz`로 압축해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fec142bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\n",
      "pytorch_model.bin\n",
      "special_tokens_map.json\n",
      "tokenizer_config.json\n",
      "vocab.txt\n"
     ]
    }
   ],
   "source": [
    "model_artifact_name = 'model.tar.gz'\n",
    "!cd {model_data_dir} && tar -czvf {model_artifact_name} *.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0044bcab",
   "metadata": {},
   "source": [
    "### model.tar.gz 를 S3 에 업로드ㅡ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0885cb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: models/nsmc/model.tar.gz to s3://sagemaker-us-east-1-154364950293/KoElectra-HF/model.tar.gz\n",
      "s3_model_path: \n",
      " s3://sagemaker-us-east-1-154364950293/KoElectra-HF\n"
     ]
    }
   ],
   "source": [
    "s3_prefix = prefix\n",
    "s3_model_path = f's3://{bucket}/{s3_prefix}'\n",
    "!aws s3 cp {model_data_dir}/{model_artifact_name} {s3_model_path}/{model_artifact_name}\n",
    "\n",
    "print(\"s3_model_path: \\n\", s3_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561d6535",
   "metadata": {},
   "source": [
    "모델 파라메터 및 토크나이저를 `model.tar.gz`으로 압축합니다. 압축 파일명은 자유롭게 지정할 수 있으나, 반드시 `tar.gz`로 압축해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0be12a",
   "metadata": {},
   "source": [
    "# 3. SageMaker Endpoint Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972a8bad",
   "metadata": {},
   "source": [
    "## 3.1. 앤드포인트 배포\n",
    " \n",
    "아래 코드를 보시면 아시겠지만, 지속적으로 업데이트되는 파이썬 버전&프레임워크 버전&트랜스포머 버전에 쉽게 대응할 수 있습니다. AWS에서 관리하고 있는 딥러닝 컨테이너(DLC) 목록을 아래 주소에서 확인해 보세요.\n",
    "\n",
    "- https://github.com/aws/deep-learning-containers/blob/master/available_images.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31e87e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_model_path = os.path.join(config.model_dir, 'model.tar.gz')\n",
    "# print(\"local_model_path: \", local_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38f5b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance_type = \"ml.m5.xlarge\"\n",
    "instance_type = 'ml.g4dn.xlarge' # $ 0.906 per Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9217547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{session.py:2668} INFO - Creating model with name: huggingface-pytorch-inference-2022-06-08-15-19-15-231\n",
      "[{session.py:3585} INFO - Creating endpoint-config with name huggingface-pytorch-inference-2022-06-08-15-19-15-700\n",
      "[{session.py:3053} INFO - Creating endpoint with name huggingface-pytorch-inference-2022-06-08-15-19-15-700\n"
     ]
    }
   ],
   "source": [
    "# create Hugging Face Model Class\n",
    "hf_model = HuggingFaceModel(\n",
    "    model_data=f\"{s3_model_path}/{model_artifact_name}\",  # path to your trained SageMaker model\n",
    "    role=role,                                            # IAM role with permissions to create an endpoint\n",
    "    transformers_version=\"4.11.0\",                        # Transformers version used\n",
    "    pytorch_version=\"1.9.0\",                              # PyTorch version used\n",
    "    py_version='py38',                                    # Python version used\n",
    "    \n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "hf_predictor = hf_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type= instance_type,\n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba7e66ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------!CPU times: user 277 ms, sys: 15.1 ms, total: 292 ms\n",
      "Wall time: 5min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'EndpointName': 'huggingface-pytorch-inference-2022-06-08-15-19-15-700',\n",
       " 'EndpointArn': 'arn:aws:sagemaker:us-east-1:154364950293:endpoint/huggingface-pytorch-inference-2022-06-08-15-19-15-700',\n",
       " 'EndpointConfigName': 'huggingface-pytorch-inference-2022-06-08-15-19-15-700',\n",
       " 'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "   'DeployedImages': [{'SpecifiedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:1.9.0-transformers4.11.0-gpu-py38-cu111-ubuntu20.04',\n",
       "     'ResolvedImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference@sha256:b1a3fd4369a4364e153f390d81bd11b49f2c0b8193ad565b4886f10326c2c6b1',\n",
       "     'ResolutionTime': datetime.datetime(2022, 6, 8, 15, 19, 16, 978000, tzinfo=tzlocal())}],\n",
       "   'CurrentWeight': 1.0,\n",
       "   'DesiredWeight': 1.0,\n",
       "   'CurrentInstanceCount': 1,\n",
       "   'DesiredInstanceCount': 1}],\n",
       " 'EndpointStatus': 'InService',\n",
       " 'CreationTime': datetime.datetime(2022, 6, 8, 15, 19, 15, 977000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2022, 6, 8, 15, 24, 16, 130000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '527c68c9-6526-4caa-97eb-80536cb07a5c',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '527c68c9-6526-4caa-97eb-80536cb07a5c',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '873',\n",
       "   'date': 'Wed, 08 Jun 2022 15:24:20 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "sess.wait_for_endpoint(hf_predictor.endpoint_name, poll=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b825c5f8",
   "metadata": {},
   "source": [
    "## 3.2. Wait for the endpoint jobs to complete\n",
    "\n",
    "엔드포인트가 생성될 때까지 기다립니다. 약 5-10분의 시간이 소요됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8da88fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b><a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpoints/huggingface-pytorch-inference-2022-06-08-15-19-15-700\">[Deploy model from S3] Review Endpoint</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def make_endpoint_link(region, endpoint_name, endpoint_task):\n",
    "    \n",
    "    endpoint_link = f'<b><a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={region}#/endpoints/{endpoint_name}\">{endpoint_task} Review Endpoint</a></b>'   \n",
    "    return endpoint_link \n",
    "        \n",
    "endpoint_link1 = make_endpoint_link(region, hf_predictor.endpoint_name, '[Deploy model from S3]')\n",
    "\n",
    "\n",
    "display(HTML(endpoint_link1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ba7713",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 4. Inference\n",
    "\n",
    "---\n",
    "\n",
    "두 개의 엔드포인트가 배포되었습니다. 샘플 데이터로 직접 추론을 수행해 봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cad31d",
   "metadata": {},
   "source": [
    "## 4.1. SageMaker Python SDK 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f3b6972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example request, you always need to define \"inputs\"\n",
    "payload = {\n",
    "   \"inputs\": [\n",
    "       \"정말 재미있습니다. 세 번 봐도 질리지 않아요.\",\n",
    "       \"시간이 아깝습니다. 다른 영화를 보세요.\"\n",
    "   ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfea3315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.994269847869873},\n",
       " {'label': 'LABEL_0', 'score': 0.9986905455589294}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_predictor.predict(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bf812b",
   "metadata": {},
   "source": [
    "## 4.2. Botoe3 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33aaa49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "runtime_client = boto3.Session().client('sagemaker-runtime')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "911aa268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.2862389087677002 seconds ---\n",
      "result:  [{'label': 'LABEL_1', 'score': 0.994269847869873}, {'label': 'LABEL_0', 'score': 0.9986905455589294}]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "from inference_utils import invoke_endpoint_hf\n",
    "payload_dump = json.dumps(payload)\n",
    "\n",
    "start_time = time.time()\n",
    "result = invoke_endpoint_hf(runtime_client, hf_predictor.endpoint_name, \n",
    "                         payload_dump,\n",
    "                         content_type='application/json'\n",
    "                        )\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('result: ', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c19f211",
   "metadata": {},
   "source": [
    "# 5. 테스트 데이터 추론 및 성능 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b78e2c",
   "metadata": {},
   "source": [
    "## 5.1. 테스트 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8fdbc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_path = '../3_Training/data/nsmc/test/ratings_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "916f895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_util import read_nsmc_split\n",
    "from inference_utils import inference_batch, plot_confusion_matrix\n",
    "\n",
    "# 훈련 Text, Label 로딩    \n",
    "test_texts, test_labels = read_nsmc_split(test_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64a5f683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_df shape:  (49832, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(49832, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test_df = pd.DataFrame(data=list(zip(test_texts, test_labels)), \n",
    "                       columns=['doc','label'])\n",
    "print(\"test_df shape: \", test_df.shape)\n",
    "sample_df = test_df[0:50000]\n",
    "sample_df.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a499ba4b",
   "metadata": {},
   "source": [
    "## 5.2. 배치 단위로 추론\n",
    "-- 약 10 분 정도 소요 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0230e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "chunk_size = 1024\n",
    "pred_list, score_list = inference_batch(sample_df, chunk_size, runtime_client, hf_predictor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2ec21",
   "metadata": {},
   "source": [
    "## 5.3. 기존 테스트 데이터에 실험 데이터 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16aabd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.insert(len(sample_df.columns), column='pred', value=pred_list)\n",
    "sample_df.insert(len(sample_df.columns), column='score', value=score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859157ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2bace2",
   "metadata": {},
   "source": [
    "## 5.4. 정확도 및 혼돈 행렬 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2ea7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = sample_df.label.tolist()\n",
    "y_pred = sample_df.pred.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8079dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cf = confusion_matrix(y_true, y_pred)\n",
    "plot_confusion_matrix(cf, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2581fcb",
   "metadata": {},
   "source": [
    "# 6. 엔드 포인트 삭제\n",
    "- 기존에 생성한 로컬 세이지 메이커 모델, 앤드포인트 컨피그, 앤드포인트 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34de24b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_utils import delete_endpoint\n",
    "\n",
    "client = boto3.Session().client('sagemaker')\n",
    "delete_endpoint(client, hf_predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a697a9d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
