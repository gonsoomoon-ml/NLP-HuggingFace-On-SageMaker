{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f449047-280d-461a-aaa8-4ffd83cfc90b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Warming Up - BERT 추론 입출력 구조 및 Sentence BERT 소개\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 참조\n",
    "- Jay Alammar, [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/)\n",
    "* How the Embedding Layers in BERT Were Implemented\n",
    "    * 입력이 입베팅으로 변환하는 것을 직관적으로 보여줌\n",
    "    * https://medium.com/@_init_/why-bert-has-3-embedding-layers-and-their-implementation-details-9c261108e28a\n",
    "- [자연어처리_BERT 기초개념(완전 초보용)](https://han-py.tistory.com/252)\n",
    "- [SentenceTransformers Documentation](https://www.sbert.net/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5359730f-ddfd-43e2-8575-51b78f123b56",
   "metadata": {},
   "source": [
    "# 1. 사전 지식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30261d3-650f-4ee9-bb01-b974fc082689",
   "metadata": {
    "tags": []
   },
   "source": [
    "## BERT 추론 입출력 구조 \n",
    "- 입력 문장 예시: \n",
    "    -  '아 배고픈데 짜장면 먹고싶다'\n",
    "    - BERT Tokeninzer 가 아래와 같이 변환\n",
    "        - 예시 입력: 총 토큰 갯수 8개\n",
    "            - ['아', '배고픈', '##데', '짜장면', '먹', '##고', '##싶', '##다']\n",
    "    - 이 토큰은 token_id (자연수) 로 변환\n",
    "        - Token\n",
    "            - ['아', '배고픈', '##데', '짜장면', '먹', '##고', '##싶', '##다'] 를 아래왁 같이 토큰 번호로 변환 합니다.\n",
    "        - token_id    \n",
    "            - [2, 3079, 31420, 4244, 26766, 2654, 4219, 4451, 4176, 3]\n",
    "            - 2: [CLS], '아' : 3079, '배고픈' : 31420, .... 3: 3: [SEP] \n",
    "- 준비된 token_id 를 버트 모델에 입력하면, 아웃풋이 아래의 그림과 같이 벡터들로 제공 됨        \n",
    "    - 출력: Contextual Representation of Token (총 아웃풋 벡터 )\n",
    "        - (디멘션: 768) 9개 (CLS 벡터(Class Label) + 8개 토큰 벡터)\n",
    "        - `CLS 벡터`, `아 의 토큰 벡터`, `배고픈의 토큰 벡터`, ...., `##다 의 토큰 벡터` \n",
    "\n",
    "\n",
    "\n",
    "### 버트 토큰의 변화 과정\n",
    "![BERT-Embedding.png](img/BERT-Embedding.png)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23d294a-4987-4daf-885c-03b2ff26801f",
   "metadata": {},
   "source": [
    "## BERT 구조\n",
    "\n",
    "![BERT_Structure.png](img/BERT_Structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f35296d-a74c-4f50-b70a-1b4a879b2f72",
   "metadata": {},
   "source": [
    "# 2. BERT 추론 입력 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "706825dc-ce49-4d63-9efa-ec3b9182c2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "from transformers import (\n",
    "    ElectraModel, \n",
    "    ElectraTokenizer, \n",
    "    ElectraForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    set_seed\n",
    ")\n",
    "tokenizer_id = 'monologg/koelectra-small-v3-discriminator'\n",
    "\n",
    "\n",
    "tokenizer = ElectraTokenizer.from_pretrained(tokenizer_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc57178b-59ee-4431-b530-62ecfa37ded2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아', '배고픈', '##데', '짜장면', '먹', '##고', '##싶', '##다']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc= '아 배고픈데 짜장면 먹고싶다'\n",
    "\n",
    "tokenizer.tokenize(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59eed899-7417-4c3e-b9a8-357b936d1aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3079, 31420, 4244, 26766, 2654, 4219, 4451, 4176, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3cb7d1a-20e8-4c25-8db6-3222622a19ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 3079, 31420, 4244, 26766, 2654, 4219, 4451, 4176, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d2dd2-fe26-4fb2-b6de-5515ef53640a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Sentence BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff737666-b3e6-488e-a38b-5bf117190298",
   "metadata": {},
   "source": [
    "## 3.1. 문장의 Embedding 보여 주기\n",
    "- [SentenceTransformers Quickstart](https://www.sbert.net/docs/quickstart.html)\n",
    "\n",
    "BERT (and other transformer networks) output for each token in our input text an embedding. In order to create a fixed-sized sentence embedding out of this, the model applies mean pooling, i.e., the output embeddings for all tokens are averaged to yield a fixed-sized vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dda7b2db-247a-4826-a8bf-41a6c532ad1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: This framework generates embeddings for each input sentence\n",
      "embedding shape:  (384,)\n",
      "Embedding: [-1.37173692e-02 -4.28515859e-02 -1.56286061e-02  1.40537536e-02\n",
      "  3.95537727e-02  1.21796280e-01  2.94333808e-02 -3.17524113e-02\n",
      "  3.54959741e-02 -7.93140009e-02  1.75878238e-02 -4.04369570e-02\n",
      "  4.97259647e-02  2.54912507e-02 -7.18699619e-02  8.14968422e-02\n",
      "  1.47072901e-03  4.79627401e-02 -4.50336002e-02 -9.92175043e-02\n",
      " -2.81769186e-02  6.45045862e-02  4.44670655e-02 -4.76217158e-02\n",
      " -3.52952406e-02  4.38671708e-02 -5.28565831e-02  4.33040957e-04\n",
      "  1.01921491e-01  1.64072439e-02  3.26996669e-02 -3.45986784e-02\n",
      "  1.21339234e-02  7.94871300e-02  4.58339043e-03  1.57778561e-02\n",
      " -9.68206581e-03  2.87626311e-02 -5.05806506e-02 -1.55793801e-02\n",
      " -2.87907217e-02 -9.62280016e-03  3.15556154e-02  2.27349699e-02\n",
      "  8.71449411e-02 -3.85027751e-02 -8.84718671e-02 -8.75495933e-03\n",
      " -2.12343428e-02  2.08923910e-02 -9.02078301e-02 -5.25732301e-02\n",
      " -1.05638690e-02  2.88310833e-02 -1.61455031e-02  6.17841398e-03\n",
      " -1.23234447e-02 -1.07336966e-02  2.83353738e-02 -5.28567731e-02\n",
      " -3.58618125e-02 -5.97989522e-02 -1.09055135e-02  2.91566812e-02\n",
      "  7.97979310e-02 -3.27898335e-04  6.83498522e-03  1.32718626e-02\n",
      " -4.24619652e-02  1.87657066e-02 -9.89234969e-02  2.09050030e-02\n",
      " -8.69605988e-02 -1.50151998e-02 -4.86202352e-02  8.04414824e-02\n",
      " -3.67699261e-03 -6.65044412e-02  1.14556774e-01 -3.04228626e-02\n",
      "  2.96631828e-02 -2.80694887e-02  4.64990735e-02 -2.25513745e-02\n",
      "  8.54223222e-02  3.15446667e-02  7.34542161e-02 -2.21861787e-02\n",
      " -5.29679246e-02  1.27130589e-02 -5.27339727e-02 -1.06188774e-01\n",
      "  7.04731718e-02  2.76736524e-02 -8.05531442e-02  2.39649508e-02\n",
      " -2.65125018e-02 -2.17331145e-02  4.35275622e-02  4.84712347e-02\n",
      " -2.37067044e-02  2.85768453e-02  1.11846149e-01 -6.34936094e-02\n",
      " -1.58318467e-02 -2.26169825e-02 -1.31028313e-02 -1.62071548e-03\n",
      " -3.60929109e-02 -9.78297219e-02 -4.67729233e-02  1.76272057e-02\n",
      " -3.97492237e-02 -1.76431393e-04  3.39627601e-02 -2.09633969e-02\n",
      "  6.33659307e-03 -2.59411559e-02  8.10410902e-02  6.14393577e-02\n",
      " -5.44596510e-03  6.48276210e-02 -1.16844043e-01  2.36861240e-02\n",
      " -1.32058514e-02 -1.12476416e-01  1.90049242e-02 -1.74659324e-34\n",
      "  5.58949672e-02  1.94244608e-02  4.65438776e-02  5.18645942e-02\n",
      "  3.89390215e-02  3.40540968e-02 -4.32114154e-02  7.90637359e-02\n",
      " -9.79530141e-02 -1.27441064e-02 -2.91870758e-02  1.02052502e-02\n",
      "  1.88116059e-02  1.08942561e-01  6.63465187e-02 -5.35295345e-02\n",
      " -3.29228900e-02  4.69826944e-02  2.28882954e-02  2.74114814e-02\n",
      " -2.91983616e-02  3.12706418e-02 -2.22850684e-02 -1.02282152e-01\n",
      " -2.79116593e-02  1.13793155e-02  9.06308815e-02 -4.75414544e-02\n",
      " -1.00718960e-01 -1.23231923e-02 -7.96928406e-02 -1.44636836e-02\n",
      " -7.76400641e-02 -7.66919414e-03  9.73956566e-03  2.24204864e-02\n",
      "  7.77267814e-02 -3.17158061e-03  2.11538281e-02 -3.30394171e-02\n",
      "  9.55247600e-03 -3.73011939e-02  2.61360258e-02 -9.79084242e-03\n",
      " -6.31505251e-02  5.77433826e-03 -3.80031317e-02  1.29684126e-02\n",
      " -1.82499103e-02 -1.56283192e-02 -1.23361184e-03  5.55579402e-02\n",
      "  1.13106798e-04 -5.61256930e-02  7.40165859e-02  1.84451789e-02\n",
      " -2.66368203e-02  1.31951720e-02  7.50086829e-02 -2.46797539e-02\n",
      " -3.24006304e-02 -1.57674924e-02 -8.03514197e-03 -5.61320316e-03\n",
      "  1.05687808e-02  3.26169911e-03 -3.91990133e-02 -9.38677117e-02\n",
      "  1.14227153e-01  6.57304749e-02 -4.72633503e-02  1.45088183e-02\n",
      " -3.54490615e-02 -3.37761752e-02 -5.15506007e-02 -3.80997895e-03\n",
      " -5.15036248e-02 -5.93429692e-02 -1.69410568e-03  7.42107481e-02\n",
      " -4.20091376e-02 -7.19975159e-02  3.17250043e-02 -1.66303534e-02\n",
      "  3.96980811e-03 -6.52750880e-02  2.77391430e-02 -7.51649663e-02\n",
      "  2.27456093e-02 -3.91368158e-02  1.54315885e-02 -5.54908663e-02\n",
      "  1.23318322e-02 -2.59520710e-02  6.66423291e-02 -6.91259233e-34\n",
      "  3.31628732e-02  8.47928822e-02 -6.65584132e-02  3.33541557e-02\n",
      "  4.71606432e-03  1.35361981e-02 -5.38694225e-02  9.20694023e-02\n",
      " -2.96876729e-02  3.16219553e-02 -2.37497240e-02  1.98770948e-02\n",
      "  1.03446186e-01 -9.06947404e-02  6.30630041e-03  1.42886322e-02\n",
      "  1.19293723e-02  6.43728301e-03  4.20104526e-02  1.25344582e-02\n",
      "  3.93019505e-02  5.35691530e-02 -4.30749804e-02  6.10432513e-02\n",
      " -5.39267821e-05  6.91682622e-02  1.05520207e-02  1.22111589e-02\n",
      " -7.23185614e-02  2.50469334e-02 -5.18370904e-02 -4.36562486e-02\n",
      " -6.71818256e-02  1.34827979e-02 -7.25888759e-02  7.04163685e-03\n",
      "  6.58939108e-02  1.08994292e-02 -2.60011782e-03  5.49969077e-02\n",
      "  5.06966859e-02  3.27948332e-02 -6.68832958e-02  6.45557269e-02\n",
      " -2.52076350e-02 -2.92572044e-02 -1.16696745e-01  3.24064419e-02\n",
      "  5.85858710e-02 -3.51756401e-02 -7.15239868e-02  2.24936102e-02\n",
      " -1.00786708e-01 -4.74545173e-02 -7.61962831e-02 -5.87166920e-02\n",
      "  4.21138369e-02 -7.47214109e-02  1.98467989e-02 -3.36504518e-03\n",
      " -5.29736504e-02  2.74729300e-02  3.45736668e-02 -6.11846782e-02\n",
      "  1.06364794e-01 -9.64120105e-02 -4.55945246e-02  1.51489899e-02\n",
      " -5.13530755e-03 -6.64447546e-02  4.31721285e-02 -1.10406000e-02\n",
      " -9.80255567e-03  7.53783286e-02 -1.49571057e-02 -4.80208360e-02\n",
      "  5.80726676e-02 -2.43896730e-02 -2.23138295e-02 -4.36992571e-02\n",
      "  5.12054227e-02 -3.28625925e-02  1.08763352e-01  6.08926266e-02\n",
      "  3.30791413e-03  5.53819947e-02  8.43201056e-02  1.27087254e-02\n",
      "  3.84465680e-02  6.52325824e-02 -2.94683930e-02  5.08005321e-02\n",
      " -2.09348332e-02  1.46135703e-01  2.25561764e-02 -1.77227779e-08\n",
      " -5.02673052e-02 -2.79227970e-04 -1.00328557e-01  2.42811274e-02\n",
      " -7.54043385e-02 -3.79139856e-02  3.96049954e-02  3.10079716e-02\n",
      " -9.05705430e-03 -6.50412142e-02  4.05453481e-02  4.83390391e-02\n",
      " -4.56962362e-02  4.76006651e-03  2.64365622e-03  9.35614258e-02\n",
      " -4.02599126e-02  3.27401944e-02  1.18298400e-02  5.54344952e-02\n",
      "  1.48052186e-01  7.21189529e-02  2.76972074e-04  1.68651156e-02\n",
      "  8.34881701e-03 -8.76151957e-03 -1.33649800e-02  6.14236966e-02\n",
      "  1.57168042e-02  6.94961101e-02  1.08621810e-02  6.08018711e-02\n",
      " -5.33421449e-02 -3.47924605e-02 -3.36272232e-02  6.93906993e-02\n",
      "  1.22987917e-02 -1.45237356e-01 -2.06970843e-03 -4.61133160e-02\n",
      "  3.72748682e-03 -5.59354899e-03 -1.00659870e-01 -4.45953012e-02\n",
      "  5.40921465e-02  4.98895580e-03  1.49534754e-02 -8.26059505e-02\n",
      "  6.26630709e-02 -5.01911854e-03 -4.81857769e-02 -3.53991240e-02\n",
      "  9.03387088e-03 -2.42337529e-02  5.66267334e-02  2.51529180e-02\n",
      " -1.70709323e-02 -1.24779986e-02  3.19518112e-02  1.38421040e-02\n",
      " -1.55814979e-02  1.00178286e-01  1.23657271e-01 -4.22967188e-02]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#Our sentences we like to encode\n",
    "sentences = ['This framework generates embeddings for each input sentence']\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, sentence_embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"embedding shape: \", embedding.shape)    \n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32dc696-1882-443d-8dd3-b7049a93b446",
   "metadata": {},
   "source": [
    "## 3.2. 문장 유사도 보여주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cc5940a-cf4a-481b-ac3d-ffc73c56fc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine-Similarity: tensor([[0.6153]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "emb1 = model.encode(\"This is a red cat with a hat.\") # 384\n",
    "emb2 = model.encode(\"Have you seen my red cat?\") # 384\n",
    "\n",
    "\n",
    "cos_sim = util.cos_sim(emb1, emb2)\n",
    "print(\"Cosine-Similarity:\", cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111401e4-941d-4d76-8a1b-4d4a957f4b49",
   "metadata": {},
   "source": [
    "아래 sentences 에서 유사한 것 끼리 Pair 를 구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8196e9b2-1fd0-48c1-9540-3aa4537ba32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 most similar pairs:\n",
      "A man is eating food. \t A man is eating a piece of bread. \t 0.7553\n",
      "A man is riding a horse. \t A man is riding a white horse on an enclosed ground. \t 0.7369\n",
      "A monkey is playing drums. \t Someone in a gorilla costume is playing a set of drums. \t 0.6433\n",
      "A woman is playing violin. \t Someone in a gorilla costume is playing a set of drums. \t 0.2564\n",
      "A man is eating food. \t A man is riding a horse. \t 0.2474\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "sentences = ['A man is eating food.',\n",
    "          'A man is eating a piece of bread.',\n",
    "          'The girl is carrying a baby.',\n",
    "          'A man is riding a horse.',\n",
    "          'A woman is playing violin.',\n",
    "          'Two men pushed carts through the woods.',\n",
    "          'A man is riding a white horse on an enclosed ground.',\n",
    "          'A monkey is playing drums.',\n",
    "          'Someone in a gorilla costume is playing a set of drums.'\n",
    "          ]\n",
    "\n",
    "#Encode all sentences\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Compute cosine similarity between all pairs\n",
    "cos_sim = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "#Add all pairs to a list with their cosine similarity score\n",
    "all_sentence_combinations = []\n",
    "for i in range(len(cos_sim)-1):\n",
    "    for j in range(i+1, len(cos_sim)):\n",
    "        all_sentence_combinations.append([cos_sim[i][j], i, j])\n",
    "\n",
    "#Sort list by the highest cosine similarity score\n",
    "all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print(\"Top-5 most similar pairs:\")\n",
    "for score, i, j in all_sentence_combinations[0:5]:\n",
    "    print(\"{} \\t {} \\t {:.4f}\".format(sentences[i], sentences[j], cos_sim[i][j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5befef5-f59f-4dbd-9c0a-0333416d331a",
   "metadata": {},
   "source": [
    "## 3.3. 시멘틱 서치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4da6b95-efc3-4137-a933-8a18ea13d5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: tensor([[0.5472, 0.6330]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "query_embedding = model.encode('How big is London')\n",
    "passage_embedding = model.encode(['London has 9,787,426 inhabitants at the 2011 census',\n",
    "                                  'London is known for its finacial district'])\n",
    "\n",
    "print(\"Similarity:\", util.dot_score(query_embedding, passage_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9ce500f-3faa-4655-9e1a-ab7870254ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: \n",
      " [ 7.1523666 -6.2870383]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "model = CrossEncoder('cross-encoder/ms-marco-TinyBERT-L-2-v2', max_length=512)\n",
    "scores = model.predict([('Query1', 'Paragraph1'), ('Query1', 'Paragraph2')])\n",
    "\n",
    "#For Example\n",
    "scores = model.predict([('How many people live in Berlin?', 'Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.'), \n",
    "                        ('How many people live in Berlin?', 'Berlin is well known for its museums.')])\n",
    "print(\"scores: \\n\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517611c8-2c11-4fc0-a745-94d32ac61ea6",
   "metadata": {},
   "source": [
    "## 3.4. Using Hugging Face models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94e5c32d-5fed-4d5e-8862-eaa7616fb993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages/transformers/configuration_utils.py:358: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'corpus_id': 0, 'score': 0.5646327137947083}, {'corpus_id': 2, 'score': 0.5142343044281006}, {'corpus_id': 1, 'score': 0.4730040431022644}]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "question = \"<Q>How many models can I host on HuggingFace?\"\n",
    "answer_1 = \"<A>All plans come with unlimited private models and datasets.\"\n",
    "answer_2 = \"<A>AutoNLP is an automatic way to train and deploy state-of-the-art NLP models, seamlessly integrated with the Hugging Face ecosystem.\"\n",
    "answer_3 = \"<A>Based on how much training data and model variants are created, we send you a compute cost and payment link - as low as $10 per job.\"\n",
    "\n",
    "model = SentenceTransformer('clips/mfaq')\n",
    "query_embedding = model.encode(question)\n",
    "corpus_embeddings = model.encode([answer_1, answer_2, answer_3])\n",
    "\n",
    "print(util.semantic_search(query_embedding, corpus_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95277aa-4983-4df2-85db-7d8ca387b5ae",
   "metadata": {},
   "source": [
    "# 4. 커널 리스타트\n",
    "\n",
    "\n",
    "- 커널 리스타트에 대한 내용이 있습니다. 클릭 후 가장 하단의 \"3.커널 리스타팅\" 을 참조 하세요.\n",
    "    - [리스타트 상세](https://github.com/gonsoomoon-ml/NLP-HuggingFace-On-SageMaker/blob/main/1_NSMC-Classification/2_WarmingUp/0.1.warming_up_yelp_review.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83c517fa-61ff-45f3-a9a5-3d9ecbcff283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69308836-6a53-464e-bc2f-61761880e35a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8ff495-5a7e-4a71-9eff-ac1f5f4a07f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e80ad12-3cfc-4bfa-a5ce-854f3698751e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
