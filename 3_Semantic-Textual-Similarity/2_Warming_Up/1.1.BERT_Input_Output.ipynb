{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f449047-280d-461a-aaa8-4ffd83cfc90b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Warming Up - BERT 추론 입출력 구조 및 Sentence BERT 소개\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 참조\n",
    "- Jay Alammar, [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](http://jalammar.github.io/illustrated-bert/)\n",
    "* How the Embedding Layers in BERT Were Implemented\n",
    "    * 입력이 입베팅으로 변환하는 것을 직관적으로 보여줌\n",
    "    * https://medium.com/@_init_/why-bert-has-3-embedding-layers-and-their-implementation-details-9c261108e28a\n",
    "- [자연어처리_BERT 기초개념(완전 초보용)](https://han-py.tistory.com/252)\n",
    "- [SentenceTransformers Documentation](https://www.sbert.net/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5359730f-ddfd-43e2-8575-51b78f123b56",
   "metadata": {},
   "source": [
    "# 1. 사전 지식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30261d3-650f-4ee9-bb01-b974fc082689",
   "metadata": {
    "tags": []
   },
   "source": [
    "## BERT 추론 입출력 구조 \n",
    "- 입력 문장 예시: \n",
    "    -  '아 배고픈데 짜장면 먹고싶다'\n",
    "    - BERT Tokeninzer 가 아래와 같이 변환\n",
    "        - 예시 입력: 총 토큰 갯수 8개\n",
    "            - ['아', '배고픈', '##데', '짜장면', '먹', '##고', '##싶', '##다']\n",
    "    - 이 토큰은 token_id (자연수) 로 변환\n",
    "        - Token\n",
    "            - ['아', '배고픈', '##데', '짜장면', '먹', '##고', '##싶', '##다'] 를 아래왁 같이 토큰 번호로 변환 합니다.\n",
    "        - token_id    \n",
    "            - [2, 3079, 31420, 4244, 26766, 2654, 4219, 4451, 4176, 3]\n",
    "            - 2: [CLS], '아' : 3079, '배고픈' : 31420, .... 3: 3: [SEP] \n",
    "- 준비된 token_id 를 버트 모델에 입력하면, 아웃풋이 아래의 그림과 같이 벡터들로 제공 됨        \n",
    "    - 출력: Contextual Representation of Token (총 아웃풋 벡터 )\n",
    "        - (디멘션: 768) 9개 (CLS 벡터(Class Label) + 8개 토큰 벡터)\n",
    "        - `CLS 벡터`, `아 의 토큰 벡터`, `배고픈의 토큰 벡터`, ...., `##다 의 토큰 벡터` \n",
    "\n",
    "\n",
    "\n",
    "### 버트 토큰의 변화 과정\n",
    "![BERT-Embedding.png](img/BERT-Embedding.png)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23d294a-4987-4daf-885c-03b2ff26801f",
   "metadata": {},
   "source": [
    "## BERT 구조\n",
    "\n",
    "![BERT_Structure.png](img/BERT_Structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f35296d-a74c-4f50-b70a-1b4a879b2f72",
   "metadata": {},
   "source": [
    "# 2. BERT 추론 입력 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "706825dc-ce49-4d63-9efa-ec3b9182c2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "from transformers import (\n",
    "    ElectraModel, \n",
    "    ElectraTokenizer, \n",
    "    ElectraForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    set_seed\n",
    ")\n",
    "tokenizer_id = 'monologg/koelectra-small-v3-discriminator'\n",
    "\n",
    "\n",
    "tokenizer = ElectraTokenizer.from_pretrained(tokenizer_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc57178b-59ee-4431-b530-62ecfa37ded2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아', '배고픈', '##데', '짜장면', '먹', '##고', '##싶', '##다']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc= '아 배고픈데 짜장면 먹고싶다'\n",
    "\n",
    "tokenizer.tokenize(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59eed899-7417-4c3e-b9a8-357b936d1aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3079, 31420, 4244, 26766, 2654, 4219, 4451, 4176, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3cb7d1a-20e8-4c25-8db6-3222622a19ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 3079, 31420, 4244, 26766, 2654, 4219, 4451, 4176, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d2dd2-fe26-4fb2-b6de-5515ef53640a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Sentence BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff737666-b3e6-488e-a38b-5bf117190298",
   "metadata": {},
   "source": [
    "## 3.1. 문장의 Embedding 보여 주기\n",
    "- [SentenceTransformers Quickstart](https://www.sbert.net/docs/quickstart.html)\n",
    "\n",
    "BERT (and other transformer networks) output for each token in our input text an embedding. In order to create a fixed-sized sentence embedding out of this, the model applies mean pooling, i.e., the output embeddings for all tokens are averaged to yield a fixed-sized vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dda7b2db-247a-4826-a8bf-41a6c532ad1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: This framework generates embeddings for each input sentence\n",
      "embedding shape:  (384,)\n",
      "Embedding: [-1.37173478e-02 -4.28515859e-02 -1.56286210e-02  1.40537620e-02\n",
      "  3.95537540e-02  1.21796317e-01  2.94333659e-02 -3.17523889e-02\n",
      "  3.54959592e-02 -7.93140158e-02  1.75878238e-02 -4.04369682e-02\n",
      "  4.97259647e-02  2.54912749e-02 -7.18699768e-02  8.14968348e-02\n",
      "  1.47072237e-03  4.79627214e-02 -4.50335667e-02 -9.92174894e-02\n",
      " -2.81769093e-02  6.45045787e-02  4.44670357e-02 -4.76217344e-02\n",
      " -3.52952369e-02  4.38671671e-02 -5.28565906e-02  4.33052715e-04\n",
      "  1.01921484e-01  1.64072420e-02  3.26996557e-02 -3.45986970e-02\n",
      "  1.21339411e-02  7.94871226e-02  4.58340487e-03  1.57778393e-02\n",
      " -9.68207326e-03  2.87626293e-02 -5.05806282e-02 -1.55793820e-02\n",
      " -2.87907142e-02 -9.62280110e-03  3.15556377e-02  2.27349531e-02\n",
      "  8.71449560e-02 -3.85027714e-02 -8.84718820e-02 -8.75496119e-03\n",
      " -2.12343633e-02  2.08924245e-02 -9.02078301e-02 -5.25732152e-02\n",
      " -1.05638672e-02  2.88311355e-02 -1.61454827e-02  6.17840467e-03\n",
      " -1.23234456e-02 -1.07337153e-02  2.83353478e-02 -5.28567806e-02\n",
      " -3.58618125e-02 -5.97989485e-02 -1.09054986e-02  2.91566495e-02\n",
      "  7.97979012e-02 -3.27874674e-04  6.83497125e-03  1.32718673e-02\n",
      " -4.24619615e-02  1.87656526e-02 -9.89234671e-02  2.09050030e-02\n",
      " -8.69605839e-02 -1.50152054e-02 -4.86202575e-02  8.04414749e-02\n",
      " -3.67701659e-03 -6.65044188e-02  1.14556760e-01 -3.04228626e-02\n",
      "  2.96631716e-02 -2.80694999e-02  4.64990586e-02 -2.25513596e-02\n",
      "  8.54222924e-02  3.15446816e-02  7.34541938e-02 -2.21861899e-02\n",
      " -5.29679209e-02  1.27130523e-02 -5.27339578e-02 -1.06188759e-01\n",
      "  7.04731718e-02  2.76736449e-02 -8.05531144e-02  2.39649452e-02\n",
      " -2.65124887e-02 -2.17331164e-02  4.35275361e-02  4.84712012e-02\n",
      " -2.37067100e-02  2.85768602e-02  1.11846142e-01 -6.34936020e-02\n",
      " -1.58318505e-02 -2.26169657e-02 -1.31028257e-02 -1.62070850e-03\n",
      " -3.60928997e-02 -9.78297219e-02 -4.67729457e-02  1.76272150e-02\n",
      " -3.97492200e-02 -1.76449917e-04  3.39627974e-02 -2.09633876e-02\n",
      "  6.33659633e-03 -2.59411335e-02  8.10410902e-02  6.14393465e-02\n",
      " -5.44599118e-03  6.48275986e-02 -1.16844073e-01  2.36861203e-02\n",
      " -1.32058468e-02 -1.12476423e-01  1.90049317e-02 -1.74659462e-34\n",
      "  5.58949485e-02  1.94244385e-02  4.65438701e-02  5.18645830e-02\n",
      "  3.89390215e-02  3.40540931e-02 -4.32114229e-02  7.90637434e-02\n",
      " -9.79530066e-02 -1.27441240e-02 -2.91870907e-02  1.02052437e-02\n",
      "  1.88115798e-02  1.08942553e-01  6.63465112e-02 -5.35295308e-02\n",
      " -3.29229049e-02  4.69826870e-02  2.28883084e-02  2.74114460e-02\n",
      " -2.91983429e-02  3.12706567e-02 -2.22850628e-02 -1.02282152e-01\n",
      " -2.79116612e-02  1.13793127e-02  9.06309113e-02 -4.75414619e-02\n",
      " -1.00718953e-01 -1.23232044e-02 -7.96928331e-02 -1.44636687e-02\n",
      " -7.76400417e-02 -7.66920345e-03  9.73956101e-03  2.24204715e-02\n",
      "  7.77267665e-02 -3.17155966e-03  2.11538617e-02 -3.30394171e-02\n",
      "  9.55248345e-03 -3.73012237e-02  2.61360332e-02 -9.79084428e-03\n",
      " -6.31505251e-02  5.77435037e-03 -3.80031131e-02  1.29684275e-02\n",
      " -1.82498936e-02 -1.56283118e-02 -1.23359391e-03  5.55579253e-02\n",
      "  1.13095819e-04 -5.61256781e-02  7.40165785e-02  1.84452031e-02\n",
      " -2.66368333e-02  1.31951738e-02  7.50086531e-02 -2.46797465e-02\n",
      " -3.24006006e-02 -1.57674793e-02 -8.03515222e-03 -5.61319292e-03\n",
      "  1.05687883e-02  3.26168560e-03 -3.91989872e-02 -9.38677192e-02\n",
      "  1.14227146e-01  6.57304600e-02 -4.72633392e-02  1.45087866e-02\n",
      " -3.54490355e-02 -3.37761454e-02 -5.15506081e-02 -3.80998827e-03\n",
      " -5.15036210e-02 -5.93429506e-02 -1.69412384e-03  7.42107481e-02\n",
      " -4.20091338e-02 -7.19975084e-02  3.17250043e-02 -1.66303571e-02\n",
      "  3.96985747e-03 -6.52750805e-02  2.77391076e-02 -7.51649737e-02\n",
      "  2.27456205e-02 -3.91368307e-02  1.54316062e-02 -5.54908514e-02\n",
      "  1.23318350e-02 -2.59520393e-02  6.66423365e-02 -6.91259050e-34\n",
      "  3.31628807e-02  8.47928971e-02 -6.65584058e-02  3.33541557e-02\n",
      "  4.71607596e-03  1.35361860e-02 -5.38694300e-02  9.20693874e-02\n",
      " -2.96876486e-02  3.16219516e-02 -2.37497557e-02  1.98771060e-02\n",
      "  1.03446193e-01 -9.06947181e-02  6.30627247e-03  1.42886145e-02\n",
      "  1.19293546e-02  6.43728394e-03  4.20104675e-02  1.25344796e-02\n",
      "  3.93019132e-02  5.35691381e-02 -4.30749767e-02  6.10432588e-02\n",
      " -5.39115936e-05  6.91682771e-02  1.05520692e-02  1.22111579e-02\n",
      " -7.23185390e-02  2.50469297e-02 -5.18370979e-02 -4.36562225e-02\n",
      " -6.71818107e-02  1.34828053e-02 -7.25888684e-02  7.04162195e-03\n",
      "  6.58939332e-02  1.08993948e-02 -2.60010315e-03  5.49969003e-02\n",
      "  5.06966710e-02  3.27948630e-02 -6.68833181e-02  6.45557195e-02\n",
      " -2.52076369e-02 -2.92571820e-02 -1.16696738e-01  3.24064419e-02\n",
      "  5.85858524e-02 -3.51756550e-02 -7.15240017e-02  2.24935841e-02\n",
      " -1.00786716e-01 -4.74545173e-02 -7.61962458e-02 -5.87166697e-02\n",
      "  4.21138369e-02 -7.47213885e-02  1.98468100e-02 -3.36503773e-03\n",
      " -5.29736765e-02  2.74729151e-02  3.45736779e-02 -6.11847080e-02\n",
      "  1.06364764e-01 -9.64119807e-02 -4.55945209e-02  1.51489889e-02\n",
      " -5.13532152e-03 -6.64447621e-02  4.31721397e-02 -1.10405823e-02\n",
      " -9.80253145e-03  7.53782913e-02 -1.49571104e-02 -4.80208583e-02\n",
      "  5.80726303e-02 -2.43896674e-02 -2.23138165e-02 -4.36992720e-02\n",
      "  5.12054302e-02 -3.28625701e-02  1.08763337e-01  6.08926341e-02\n",
      "  3.30789853e-03  5.53820133e-02  8.43200758e-02  1.27087375e-02\n",
      "  3.84465531e-02  6.52325749e-02 -2.94683687e-02  5.08005656e-02\n",
      " -2.09348090e-02  1.46135688e-01  2.25561615e-02 -1.77227744e-08\n",
      " -5.02672680e-02 -2.79207889e-04 -1.00328542e-01  2.42811106e-02\n",
      " -7.54043162e-02 -3.79139706e-02  3.96049805e-02  3.10079772e-02\n",
      " -9.05703753e-03 -6.50411844e-02  4.05453295e-02  4.83390093e-02\n",
      " -4.56962362e-02  4.76004276e-03  2.64364691e-03  9.35614184e-02\n",
      " -4.02599089e-02  3.27401981e-02  1.18298540e-02  5.54344766e-02\n",
      "  1.48052216e-01  7.21189603e-02  2.76983832e-04  1.68651547e-02\n",
      "  8.34881421e-03 -8.76154006e-03 -1.33650014e-02  6.14236593e-02\n",
      "  1.57167800e-02  6.94960877e-02  1.08621884e-02  6.08018450e-02\n",
      " -5.33421189e-02 -3.47924717e-02 -3.36271934e-02  6.93906993e-02\n",
      "  1.22987861e-02 -1.45237371e-01 -2.06971611e-03 -4.61133048e-02\n",
      "  3.72749381e-03 -5.59355132e-03 -1.00659862e-01 -4.45953086e-02\n",
      "  5.40921353e-02  4.98892786e-03  1.49534615e-02 -8.26059505e-02\n",
      "  6.26630560e-02 -5.01911854e-03 -4.81857695e-02 -3.53991166e-02\n",
      "  9.03388672e-03 -2.42337715e-02  5.66267297e-02  2.51529235e-02\n",
      " -1.70709286e-02 -1.24779856e-02  3.19518261e-02  1.38421012e-02\n",
      " -1.55814830e-02  1.00178301e-01  1.23657264e-01 -4.22967002e-02]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#Our sentences we like to encode\n",
    "sentences = ['This framework generates embeddings for each input sentence']\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, sentence_embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"embedding shape: \", embedding.shape)    \n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32dc696-1882-443d-8dd3-b7049a93b446",
   "metadata": {},
   "source": [
    "## 3.2. 문장 유사도 보여주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cc5940a-cf4a-481b-ac3d-ffc73c56fc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine-Similarity: tensor([[0.6153]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "emb1 = model.encode(\"This is a red cat with a hat.\") # 384\n",
    "emb2 = model.encode(\"Have you seen my red cat?\") # 384\n",
    "\n",
    "\n",
    "cos_sim = util.cos_sim(emb1, emb2)\n",
    "print(\"Cosine-Similarity:\", cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111401e4-941d-4d76-8a1b-4d4a957f4b49",
   "metadata": {},
   "source": [
    "아래 sentences 에서 유사한 것 끼리 Pair 를 구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8196e9b2-1fd0-48c1-9540-3aa4537ba32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 most similar pairs:\n",
      "A man is eating food. \t A man is eating a piece of bread. \t 0.7553\n",
      "A man is riding a horse. \t A man is riding a white horse on an enclosed ground. \t 0.7369\n",
      "A monkey is playing drums. \t Someone in a gorilla costume is playing a set of drums. \t 0.6433\n",
      "A woman is playing violin. \t Someone in a gorilla costume is playing a set of drums. \t 0.2564\n",
      "A man is eating food. \t A man is riding a horse. \t 0.2474\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "sentences = ['A man is eating food.',\n",
    "          'A man is eating a piece of bread.',\n",
    "          'The girl is carrying a baby.',\n",
    "          'A man is riding a horse.',\n",
    "          'A woman is playing violin.',\n",
    "          'Two men pushed carts through the woods.',\n",
    "          'A man is riding a white horse on an enclosed ground.',\n",
    "          'A monkey is playing drums.',\n",
    "          'Someone in a gorilla costume is playing a set of drums.'\n",
    "          ]\n",
    "\n",
    "#Encode all sentences\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Compute cosine similarity between all pairs\n",
    "cos_sim = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "#Add all pairs to a list with their cosine similarity score\n",
    "all_sentence_combinations = []\n",
    "for i in range(len(cos_sim)-1):\n",
    "    for j in range(i+1, len(cos_sim)):\n",
    "        all_sentence_combinations.append([cos_sim[i][j], i, j])\n",
    "\n",
    "#Sort list by the highest cosine similarity score\n",
    "all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print(\"Top-5 most similar pairs:\")\n",
    "for score, i, j in all_sentence_combinations[0:5]:\n",
    "    print(\"{} \\t {} \\t {:.4f}\".format(sentences[i], sentences[j], cos_sim[i][j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5befef5-f59f-4dbd-9c0a-0333416d331a",
   "metadata": {},
   "source": [
    "## 3.3. 시멘틱 서치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4da6b95-efc3-4137-a933-8a18ea13d5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: tensor([[0.5472, 0.6330]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "query_embedding = model.encode('How big is London')\n",
    "passage_embedding = model.encode(['London has 9,787,426 inhabitants at the 2011 census',\n",
    "                                  'London is known for its finacial district'])\n",
    "\n",
    "print(\"Similarity:\", util.dot_score(query_embedding, passage_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9ce500f-3faa-4655-9e1a-ab7870254ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: \n",
      " [ 7.152368 -6.287043]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "model = CrossEncoder('cross-encoder/ms-marco-TinyBERT-L-2-v2', max_length=512)\n",
    "scores = model.predict([('Query1', 'Paragraph1'), ('Query1', 'Paragraph2')])\n",
    "\n",
    "#For Example\n",
    "scores = model.predict([('How many people live in Berlin?', 'Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.'), \n",
    "                        ('How many people live in Berlin?', 'Berlin is well known for its museums.')])\n",
    "print(\"scores: \\n\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517611c8-2c11-4fc0-a745-94d32ac61ea6",
   "metadata": {},
   "source": [
    "## 3.4. Using Hugging Face models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94e5c32d-5fed-4d5e-8862-eaa7616fb993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/transformers/configuration_utils.py:369: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'corpus_id': 0, 'score': 0.5646325945854187}, {'corpus_id': 2, 'score': 0.5142339468002319}, {'corpus_id': 1, 'score': 0.4730038344860077}]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "question = \"<Q>How many models can I host on HuggingFace?\"\n",
    "answer_1 = \"<A>All plans come with unlimited private models and datasets.\"\n",
    "answer_2 = \"<A>AutoNLP is an automatic way to train and deploy state-of-the-art NLP models, seamlessly integrated with the Hugging Face ecosystem.\"\n",
    "answer_3 = \"<A>Based on how much training data and model variants are created, we send you a compute cost and payment link - as low as $10 per job.\"\n",
    "\n",
    "model = SentenceTransformer('clips/mfaq')\n",
    "query_embedding = model.encode(question)\n",
    "corpus_embeddings = model.encode([answer_1, answer_2, answer_3])\n",
    "\n",
    "print(util.semantic_search(query_embedding, corpus_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95277aa-4983-4df2-85db-7d8ca387b5ae",
   "metadata": {},
   "source": [
    "# 4. 커널 리스타트\n",
    "\n",
    "\n",
    "- 커널 리스타트에 대한 내용이 있습니다. 클릭 후 가장 하단의 \"3.커널 리스타팅\" 을 참조 하세요.\n",
    "    - [리스타트 상세](https://github.com/gonsoomoon-ml/NLP-HuggingFace-On-SageMaker/blob/main/1_NSMC-Classification/2_WarmingUp/0.1.warming_up_yelp_review.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83c517fa-61ff-45f3-a9a5-3d9ecbcff283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69308836-6a53-464e-bc2f-61761880e35a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8ff495-5a7e-4a71-9eff-ac1f5f4a07f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e80ad12-3cfc-4bfa-a5ce-854f3698751e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
