{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c23209c1-3a95-4036-a5a5-d0c16e10f06b",
   "metadata": {},
   "source": [
    "# Warming Up - Coumputing Sentence Embeddings\n",
    "\n",
    "---\n",
    "\n",
    "## 참조\n",
    "- [Computing Sentence Embeddings](https://www.sbert.net/examples/applications/computing-embeddings/README.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a9d4db-1c80-4978-ac93-2f8ca2f018be",
   "metadata": {},
   "source": [
    "# 1. 배경"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eca7107-28bc-4411-aa28-5782ee0cfbf7",
   "metadata": {},
   "source": [
    "Sentence Embedding 에 대한 여러 테크닉을 배우는 노트북 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d959cd8c-2261-451e-ade9-8ba196dcdefd",
   "metadata": {},
   "source": [
    "# 2. 사용 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff737666-b3e6-488e-a38b-5bf117190298",
   "metadata": {},
   "source": [
    "## 2.1. Input Sequence Length\n",
    "\n",
    "Transformer models like BERT / RoBERTa / DistilBERT etc. the runtime and the memory requirement grows quadratic with the input length. This limits transformers to inputs of certain lengths. A common value for BERT & Co. are 512 word pieces, which corresponde to about 300-400 words (for English). Longer texts than this are truncated to the first x word pieces.\n",
    "\n",
    "By default, the provided methods use a limit fo 128 word pieces, longer inputs will be truncated. You can get and set the maximal sequence length like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda7b2db-247a-4826-a8bf-41a6c532ad1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sequence Length: 256\n",
      "Max Sequence Length: 200\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"Max Sequence Length:\", model.max_seq_length)\n",
    "\n",
    "#Change the length to 200\n",
    "model.max_seq_length = 200\n",
    "\n",
    "print(\"Max Sequence Length:\", model.max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32dc696-1882-443d-8dd3-b7049a93b446",
   "metadata": {},
   "source": [
    "## 2.2. Storing & Loading Embeddings\n",
    "\n",
    "The easiest method is to use pickle to store pre-computed embeddings on disc and to load it from disc. This can especially be useful if you need to encode large set of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83c517fa-61ff-45f3-a9a5-3d9ecbcff283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stored_sentences: \n",
      " ['This framework generates embeddings for each input sentence', 'Sentences are passed as a list of string.', 'The quick brown fox jumps over the lazy dog.']\n",
      "stored_embeddings shape: \n",
      " (3, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.', \n",
    "    'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Store sentences & embeddings on disc\n",
    "with open('embeddings.pkl', \"wb\") as fOut:\n",
    "    pickle.dump({'sentences': sentences, 'embeddings': embeddings}, fOut, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#Load sentences & embeddings from disc\n",
    "with open('embeddings.pkl', \"rb\") as fIn:\n",
    "    stored_data = pickle.load(fIn)\n",
    "    stored_sentences = stored_data['sentences']\n",
    "    stored_embeddings = stored_data['embeddings']\n",
    "    print(\"stored_sentences: \\n\", stored_sentences)\n",
    "    print(\"stored_embeddings shape: \\n\", stored_embeddings.shape)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa9ceeb-a5f3-4d2f-9508-2ee6bdb2698d",
   "metadata": {},
   "source": [
    "## 2.3. Multi-Process / Multi-GPU Encoding\n",
    "\n",
    "You can encode input texts with more than one GPU (or with multiple processes on a CPU machine). For an example, see: computing_embeddings_mutli_gpu.py.\n",
    "\n",
    "아래와 같이 4개의 GPU를 다 사용함.\n",
    "\n",
    "![multiple_embedding.png](img/multiple_embedding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40a57900-faaa-4abd-a9a5-0b305edcb0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c0353d7-9a6a-46d4-ad1c-29ee8524a132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Embeddings computed. Shape: (50000, 384)\n",
      "CPU times: user 640 ms, sys: 338 ms, total: 978 ms\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "#Create a large list of 100k sentences\n",
    "sentences = [\"This is sentence {}\".format(i) for i in range(50000)]\n",
    "\n",
    "#Define the model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#Start the multi-process pool on all available CUDA devices\n",
    "pool = model.start_multi_process_pool()\n",
    "\n",
    "#Compute the embeddings using the multi-process pool\n",
    "emb = model.encode_multi_process(sentences, pool)\n",
    "print(\"Embeddings computed. Shape:\", emb.shape)\n",
    "\n",
    "#Optional: Stop the proccesses in the pool\n",
    "model.stop_multi_process_pool(pool)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e58ddc-7c46-4006-abb9-bb596b7a088b",
   "metadata": {},
   "source": [
    "## 2.4. Sentence Embeddings with Transformers (영어)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30369eb2-4faf-4dd7-9e49-a158b533bdbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f18a2e0b64f4da6a98df99e54e7f681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757471f86cbd429c95d6a8c0b18e636b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f24d22d2a524955b05b75411ba731bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601241dd7acc464d80a7579f9574edd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f21ed00da0f4ca59f3848a67765f8e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139e83f8a4664f8e9a70776236d822b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/86.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_embeddings shape: \n",
      " torch.Size([3, 384])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "\n",
    "\n",
    "#Sentences we want sentence embeddings for\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "             'Sentences are passed as a list of string.',\n",
    "             'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "#Load AutoModel from huggingface model repository\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "#Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "#Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "#Perform pooling. In this case, mean pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "print(\"sentence_embeddings shape: \\n\", sentence_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c917eaf5-435a-4b06-aa64-9a43c2def77b",
   "metadata": {},
   "source": [
    "## 2.5. Sentence Embeddings with Transformers (한글)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be7070cd-1818-498f-b35a-8c20f7cd3481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "from transformers import (\n",
    "    ElectraModel, \n",
    "    ElectraTokenizer, \n",
    "    ElectraForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    set_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ba6a2fb-2bc7-43fd-8df8-f5056c5fe4a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf18ba0b192b4afc82198ef36befc200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/54.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-small-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_embeddings shape: \n",
      " torch.Size([3, 256])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "\n",
    "sentences = ['엄마가 라면을 준비 해주셨는데, 너무 맛 있었다',\n",
    "             '라면의 면발이 꼬들꼬들 하여서 입맛에 딱 좋았다',\n",
    "             '하지만 라면이 좀 매워서, 다음 부터는 순한 맛으로 해달라고 애기 했다']\n",
    "\n",
    "#Load AutoModel from huggingface model repository\n",
    "# tokenizer = AutoTokenizer.from_pretrained('monologg/koelectra-small-v3-discriminator')\n",
    "# model = AutoModel.from_pretrained(\"monologg/koelectra-small-v3-discriminator\")\n",
    "\n",
    "tokenizer = ElectraTokenizer.from_pretrained('monologg/koelectra-small-v3-discriminator')\n",
    "model = ElectraModel.from_pretrained(\"monologg/koelectra-small-v3-discriminator\")\n",
    "\n",
    "\n",
    "\n",
    "#Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "#Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "#Perform pooling. In this case, mean pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "print(\"sentence_embeddings shape: \\n\", sentence_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce7e455-0a35-437b-bb0a-9daf6f894d6e",
   "metadata": {},
   "source": [
    "# 3. 커널 리스타트\n",
    "\n",
    "\n",
    "- 커널 리스타트에 대한 내용이 있습니다. 클릭 후 가장 하단의 \"3.커널 리스타팅\" 을 참조 하세요.\n",
    "    - [리스타트 상세](https://github.com/gonsoomoon-ml/NLP-HuggingFace-On-SageMaker/blob/main/1_NSMC-Classification/2_WarmingUp/0.1.warming_up_yelp_review.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "026183db-55ea-402e-916f-d09d673d8b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e76aab-cf95-4251-b691-15fe0584b60c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
