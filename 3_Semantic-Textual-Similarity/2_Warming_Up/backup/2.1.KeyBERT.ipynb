{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f449047-280d-461a-aaa8-4ffd83cfc90b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Warming Up - Keyword BERT\n",
    "\n",
    "\n",
    "### 참고\n",
    "- 5) 한국어 키버트(Korean KeyBERT)를 이용한 키워드 추출\n",
    "    - https://wikidocs.net/159468"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5359730f-ddfd-43e2-8575-51b78f123b56",
   "metadata": {},
   "source": [
    "# 1. 환경 셋업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "197e3de3-b7fc-4d1b-9f0b-66f2d5302ab9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95108063-c492-4ebf-9bd7-3750552f2a6a",
   "metadata": {},
   "source": [
    "# 2. 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "619c454e-365d-4983-848c-f00c9a0f165b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def max_sum_sim(doc_embedding, candidate_embeddings, words, top_n, nr_candidates):\n",
    "    # 문서와 각 키워드들 간의 유사도\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "\n",
    "    # 각 키워드들 간의 유사도\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings, \n",
    "                                            candidate_embeddings)\n",
    "\n",
    "    # 코사인 유사도에 기반하여 키워드들 중 상위 top_n개의 단어를 pick.\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [candidates[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # 각 키워드들 중에서 가장 덜 유사한 키워드들간의 조합을 계산\n",
    "    min_sim = np.inf\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "\n",
    "    return [words_vals[idx] for idx in candidate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82f0e771-dee1-4d85-8ea6-f4b5ef6ef8ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
    "\n",
    "    # 문서와 각 키워드들 간의 유사도가 적혀있는 리스트\n",
    "    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
    "\n",
    "    # 각 키워드들 간의 유사도\n",
    "    word_similarity = cosine_similarity(candidate_embeddings)\n",
    "\n",
    "    # 문서와 가장 높은 유사도를 가진 키워드의 인덱스를 추출.\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # keywords_idx = [2]\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "\n",
    "    # 가장 높은 유사도를 가진 키워드의 인덱스를 제외한 문서의 인덱스들\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # ==> candidates_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10 ... 중략 ...]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    # 최고의 키워드는 이미 추출했으므로 top_n-1번만큼 아래를 반복.\n",
    "    # ex) top_n = 5라면, 아래의 loop는 4번 반복됨.\n",
    "    for _ in range(top_n - 1):\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # MMR을 계산\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # keywords & candidates를 업데이트\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fd58a2-567c-4c30-9d69-88de9867cc6a",
   "metadata": {},
   "source": [
    "# 3. 한글 예시 (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924af69e-16a6-41f4-ad2e-5c1d3377476d",
   "metadata": {},
   "source": [
    "## 3.1. 일부 품사 (예: 명사, 형용사) 만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d409e74-e336-4fa8-ba07-0fba1a63fe6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "드론 활용 범위도 점차 확대되고 있다. 최근에는 미세먼지 관리에 드론이 활용되고 있다.\n",
    "서울시는 '미세먼지 계절관리제' 기간인 지난달부터 오는 3월까지 4개월간 드론에 측정장치를 달아 미세먼지 집중 관리를 실시하고 있다.\n",
    "드론은 산업단지와 사업장 밀집지역을 날아다니며 미세먼지 배출 수치를 점검하고, 현장 모습을 영상으로 담는다.\n",
    "영상을 통해 미세먼지 방지 시설을 제대로 가동하지 않는 업체와 무허가 시설에 대한 단속이 한층 수월해질 전망이다.\n",
    "드론 활용에 가장 적극적인 소방청은 광범위하고 복합적인 재난 대응 차원에서 드론과 관련 전문인력 보강을 꾸준히 이어가고 있다.\n",
    "지난해 말 기준 소방청이 보유한 드론은 총 304대, 드론 조종 자격증을 갖춘 소방대원의 경우 1,860명이다.\n",
    "이 중 실기평가지도 자격증까지 갖춘 ‘드론 전문가’ 21명도 배치돼 있다.\n",
    "소방청 관계자는 \"소방드론은 재난현장에서 영상정보를 수집, 산악ㆍ수난 사고 시 인명수색·구조활동,\n",
    "유독가스·폭발사고 시 대원안전 확보 등에 활용된다\"며\n",
    "\"향후 화재진압, 인명구조 등에도 드론을 활용하기 위해 연구개발(R&D)을 하고 있다\"고 말했다.\n",
    "\"\"\"\n",
    "\n",
    "doc = '주꾸미 오징어 문어 요런거 느므느므 좋아하는데 혼자 사는 싱글족이라 주꾸미 땡겨도 막상 근처에 갈데가 없고,\\\n",
    "       볶음류가 은근 2인분 이상 주문해야되는곳도 많아서ㅠㅠ 요즘 한끼 먹기 좋게 포장된 주꾸미를 열심히 섭렵 중에 있답니닷! ^^ \\\n",
    "       한끼용이지만 마치 두끼인것처럼 든든하게 먹기 위해서 좋아하는거 맘껏 다 준비해서 만들었어요ㅎㅎㅎ\\\n",
    "       1. 중국식 넓적 당면 불려 두고 2. 함께 주문한 왕통통 새우도 넣고 \\\n",
    "       3. 볶음류에 깻잎 또 필수 중에 필수죠ㅋㅋ 4. 거기다가 매운고추 더 뿌려서 \\\n",
    "       5. 고소함 띠드 한장 싸악 올려서 먹음 채고!!! 채고!!! 오늘 받자마자 저녁으로 만들어 먹어보니 \\\n",
    "       홍대주꾸미가 다른 주꾸미 보다 좋았던게 익는 순간 쪼그라 들면서 반토막나는데가 진짜 너무 많았거든요? ㅠㅠ \\\n",
    "       실망 실망 대실망ㅠㅠ 근데 여긴 사이즈가 변함없이 통통하니 쫄깃쫄깃 씹는 재미,식감을 즐기기도 너무 좋더라구요. >.< \\\n",
    "       딱 좋아요! 맘에 들었어요! 쟁여두고 사먹을 아이템으로 등극했어요~*^^*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0239105-6285-45d6-bc8e-a5a3f89701a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'Noun'\n",
    "\n",
    "word in \"Hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59a9e727-1d30-4ed7-a484-15cee50ab934",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taggings Sample: \n",
      " [('주꾸미', 'Noun'), ('오징어', 'Noun'), ('문어', 'Noun'), ('요런', 'Modifier'), ('거', 'Noun'), ('느므느므', 'Noun'), ('좋아하는데', 'Adjective'), ('혼자', 'Noun'), ('사는', 'Verb'), ('싱글', 'Noun'), ('족', 'Noun'), ('이라', 'Josa'), ('주꾸미', 'Noun'), ('땡겨도', 'Verb'), ('막상', 'Noun'), ('근처', 'Noun'), ('에', 'Josa'), ('갈데가', 'Verb'), ('없고', 'Adjective'), (',', 'Punctuation'), ('볶음', 'Noun'), ('류', 'Noun'), ('가', 'Josa'), ('은근', 'Noun'), ('2', 'Number'), ('인분', 'Noun'), ('이상', 'Noun'), ('주문', 'Noun'), ('해야', 'Verb'), ('되는', 'Verb'), ('곳도', 'Noun'), ('많아서', 'Adjective'), ('ㅠㅠ', 'KoreanParticle'), ('요즘', 'Noun'), ('한', 'Determiner'), ('끼', 'Noun'), ('먹기', 'Noun'), ('좋게', 'Adjective'), ('포장', 'Noun'), ('된', 'Verb'), ('주꾸미', 'Noun'), ('를', 'Josa'), ('열심히', 'Adverb'), ('섭렵', 'Noun'), ('중', 'Noun'), ('에', 'Josa'), ('있답니', 'Adjective'), ('닷', 'Noun'), ('!', 'Punctuation'), ('^^', 'Punctuation')]\n",
      "POS Samples  :\n",
      " 주꾸미 오징어 문어 거 느므느므 좋아하는데 혼자 사는 싱글 족 주꾸미 땡겨도 막상 근처 갈데가 없고 볶음 류 은근 2 인분 이상 주문 해야 되는 곳도 많아서 요즘 끼 먹기 좋게 포장 된 주꾸미 섭렵 중 있답니 닷 끼 용이 마치 끼 것 든든하게 먹기 위해 좋아하는거 맘껏 준비 해서 만들었어요 1 중국 넓 적 당면 불려 두고 2 주문 왕 통통 새우도 넣고 3 볶음 류 깻잎 또 필수 중 필수 4 거기 다가 매운 고추 더 뿌려서 5 고소함 띠드 한장 싸 악 올려서 먹음 채고 채고 오늘 받자마자 저녁 만들어 먹어 보니 홍대 주꾸미 다른 주꾸미 보다 좋았던게 익는 순간 쪼그 들면서 반토막 는데가 진짜 많았거든요 실망 실망 실망 여긴 사이즈 변함 통통하니 쫄깃쫄깃 씹는 재미 식감 즐기 기도 좋더라구요 좋아요 맘 들었어요 쟁여두 사먹을 아이템 등 했어요\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "tokenized_doc = okt.pos(doc)\n",
    "tokenized_POS = ' '.join([word[0] for word in tokenized_doc if word[1] in \"Noun Adjective Number Verb\"])\n",
    "# tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'Noun'] or word[1]=='Adjective')\n",
    "# tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'Noun'])\n",
    "\n",
    "print('Taggings Sample: \\n',tokenized_doc[:50])\n",
    "print('POS Samples  :\\n',tokenized_POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9db0e20-940d-4cbd-85a0-c3fdb428ed8c",
   "metadata": {},
   "source": [
    "## 3.2. Bi-gram (두개 연속 단어), Tri-gram (세개 연속 단어) 를 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaa2deb1-0282-4e06-9da0-85c16aba6890",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of bigram & trigram : 204\n",
      "\n",
      "bigram and trigram Samples :\n",
      " ['갈데가 없고' '갈데가 없고 볶음' '거기 다가' '거기 다가 매운' '고소함 띠드' '고소함 띠드 한장' '고추 뿌려서'\n",
      " '고추 뿌려서 고소함' '곳도 많아서' '곳도 많아서 요즘' '근처 갈데가' '근처 갈데가 없고' '기도 좋더라구요'\n",
      " '기도 좋더라구요 좋아요' '깻잎 필수' '깻잎 필수 필수' '넣고 볶음' '넣고 볶음 깻잎' '느므느므 좋아하는데'\n",
      " '느므느므 좋아하는데 혼자' '는데가 진짜' '는데가 진짜 많았거든요' '다가 매운' '다가 매운 고추' '다른 주꾸미'\n",
      " '다른 주꾸미 보다' '당면 불려' '당면 불려 두고' '되는 곳도' '되는 곳도 많아서' '두고 주문' '두고 주문 통통'\n",
      " '든든하게 먹기' '든든하게 먹기 위해' '들면서 반토막' '들면서 반토막 는데가' '들었어요 쟁여두' '들었어요 쟁여두 사먹을'\n",
      " '땡겨도 막상' '땡겨도 막상 근처' '띠드 한장' '띠드 한장 올려서' '마치 든든하게' '마치 든든하게 먹기' '막상 근처'\n",
      " '막상 근처 갈데가' '만들어 먹어' '만들어 먹어 보니' '만들었어요 중국' '만들었어요 중국 당면' '많아서 요즘'\n",
      " '많아서 요즘 먹기' '많았거든요 실망' '많았거든요 실망 실망' '맘껏 준비' '맘껏 준비 해서' '매운 고추'\n",
      " '매운 고추 뿌려서' '먹기 위해' '먹기 위해 좋아하는거' '먹기 좋게' '먹기 좋게 포장' '먹어 보니' '먹어 보니 홍대'\n",
      " '먹음 채고' '먹음 채고 채고' '문어 느므느므' '문어 느므느므 좋아하는데' '반토막 는데가' '반토막 는데가 진짜'\n",
      " '받자마자 저녁' '받자마자 저녁 만들어' '변함 통통하니' '변함 통통하니 쫄깃쫄깃' '보니 홍대' '보니 홍대 주꾸미'\n",
      " '보다 좋았던게' '보다 좋았던게 익는' '볶음 깻잎' '볶음 깻잎 필수' '볶음 은근' '볶음 은근 인분' '불려 두고'\n",
      " '불려 두고 주문' '뿌려서 고소함' '뿌려서 고소함 띠드' '사는 싱글' '사는 싱글 주꾸미' '사먹을 아이템'\n",
      " '사먹을 아이템 했어요' '사이즈 변함' '사이즈 변함 통통하니' '새우도 넣고' '새우도 넣고 볶음' '섭렵 있답니'\n",
      " '섭렵 있답니 용이' '순간 쪼그' '순간 쪼그 들면서' '식감 즐기' '식감 즐기 기도']\n"
     ]
    }
   ],
   "source": [
    "n_gram_range = (2, 3)\n",
    "\n",
    "count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_POS])\n",
    "candidates = count.get_feature_names_out()\n",
    "\n",
    "print('# of bigram & trigram :',len(candidates))\n",
    "print('\\nbigram and trigram Samples :\\n',candidates[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6067de-c856-4e95-b9d6-99227cdee690",
   "metadata": {},
   "source": [
    "## 3.3. Pre-Trained 로딩 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fbd8179-612a-451c-80b2-7154956d38fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7ead72e27a435db07f912613a7f10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ab895/.gitattributes:   0%|          | 0.00/574 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5732efd19fc4550b8dff5e60f7480be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1ef2b4f5d04667bb5fe9a6f809204f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)f9e99ab895/README.md:   0%|          | 0.00/4.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43da6f343aeb4bf29bfcdedb3e415d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e99ab895/config.json:   0%|          | 0.00/731 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd75478c155147dcb2b7242bd3fec719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c15bc6391b6493a9c9b9112fca7c208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3137c5b11c6343d18e3bdd9209dd9b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3751bbdd6de4195aab8ddf512e04ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b8cb3206e046e3bd8a76e1a231dedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b56237b772429fb2827e2ae7bb6010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"tokenizer.json\";:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9a8c5566b5449c99fad4a6bd247cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/527 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c7cb90f95b4b4e8b39e1c726b39877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)99ab895/modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25153664-89a3-47b1-977f-e4a415f8008c",
   "metadata": {},
   "source": [
    "## 3.4. 자연어를 모델에 입력하여 모델 Output embedding 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a861751-dc54-4a20-8e1b-071e60b15f2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "doc_embedding = model.encode([doc])\n",
    "candidate_embeddings = model.encode(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eef342cd-baa7-4de0-99e5-4e91ab930f32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_embedding shape:  (1, 768)\n",
      "candidate_embeddings shape:  (204, 768)\n"
     ]
    }
   ],
   "source": [
    "print(\"doc_embedding shape: \", doc_embedding.shape)\n",
    "print(\"candidate_embeddings shape: \", candidate_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376613cd-fbc8-4b31-9b7f-84cf2fba642b",
   "metadata": {},
   "source": [
    "# 3.5. 원문 (Doc) 과 Bi-gram 및 Tri-gram 의 유사한 것 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0958357-9a91-4965-b622-804fd9374878",
   "metadata": {},
   "source": [
    "### 3.5.1 Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "738d25ee-d829-40f6-b813-06c0155da511",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['볶음 깻잎', '넣고 볶음 깻잎', '새우도 넣고 볶음', '볶음 은근', '볶음 은근 인분']\n"
     ]
    }
   ],
   "source": [
    "top_n = 5\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c175586-d022-4d20-84f7-b7ae82607a9a",
   "metadata": {},
   "source": [
    "### 3.5.2 Max Sum Similarity\n",
    "- \"후보 간의 유사성을 최소화하면서 문서와의 후보 유사성을 극대화하고자 하는 것입니다.\"\n",
    "    - https://wikidocs.net/159468"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4d42c0d-43cc-4987-9d1c-60d9190e6d5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['고추 뿌려서 고소함', '매운 고추', '많아서 요즘 먹기', '새우도 넣고 볶음', '볶음 은근']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4eb973-eb5a-40cb-aab0-5ceff7c1be5d",
   "metadata": {},
   "source": [
    "### 3.5.3 Maximal Marginal Relevance\n",
    "- \"MMR은 텍스트 요약 작업에서 중복을 최소화하고 결과의 다양성을 극대화하기 위해 노력합니다. 참고 할 수 있는 자료로 EmbedRank(https://arxiv.org/pdf/1801.04470.pdf) 라는 키워드 추출 알고리즘은 키워드/키프레이즈를 다양화하는 데 사용할 수 있는 MMR을 구현했습니다. 먼저 문서와 가장 유사한 키워드/키프레이즈를 선택합니다. 그런 다음 문서와 유사하고 이미 선택된 키워드/키프레이즈와 유사하지 않은 새로운 후보를 반복적으로 선택합니다.\"\n",
    "    - https://wikidocs.net/159468"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8d70d03-8ffa-410a-9dba-7e17f18c7a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['볶음 은근 인분', '많아서 요즘 먹기', '새우도 넣고 볶음', '중국 당면 불려', '넣고 볶음 깻잎']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab8a4fc5-3157-42be-9b61-f89a1a434f89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['볶음 은근 인분', '실망 실망', '중국 당면', '좋아하는데 혼자 사는', '다른 주꾸미']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787321e2-4258-4e3b-9f7e-8d29a0e9b0d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df0f2cd-6a72-489c-88d5-bc04fe7e952b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeee15b-74eb-45bc-8552-082e2fb1a2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c31e7c6-154b-48db-b7b2-bd2d8a9ea59e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f1f733-e787-4cba-a49a-9fd8df7d8ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
