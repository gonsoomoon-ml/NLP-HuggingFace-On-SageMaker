{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c23209c1-3a95-4036-a5a5-d0c16e10f06b",
   "metadata": {},
   "source": [
    "# [모듈] Semantic Textual Similarity & Semantic Search\n",
    "\n",
    "### 참고\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a9d4db-1c80-4978-ac93-2f8ca2f018be",
   "metadata": {},
   "source": [
    "# 1. 배경"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d959cd8c-2261-451e-ade9-8ba196dcdefd",
   "metadata": {},
   "source": [
    "# 2. 사용 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff737666-b3e6-488e-a38b-5bf117190298",
   "metadata": {},
   "source": [
    "## 2.1. Training Data\n",
    "- https://www.sbert.net/docs/training/overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb1db7b8-f5d1-4d59-b5a1-ba01f2bbcc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39607035bd446f2a3525d955396bda9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac14ad56356f49f186a97858aa487857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de584e37163e4a02b7641f0e32333dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.99k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c0bd730a7047218bc32d551638d65b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/550 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae074b09e0549cc87b14e321bc53282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6952fb4082e94fd99df834aa84d7e1ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0018e341d11d4f15b2496e3db7c5138b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5832230d2a674c82adf437fb2e8ea561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ffdbcd97eb46d79b020f4727dbbd27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31aac7cfef54e8783eb3eef2716d115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/450 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2eac791f7d47008d6e3b3760fe4359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d5e255b1184af0aba17f40b37c30c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b676b343fe9b4e299b8868b5a63032d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab7bdf0c0c384fb68c03a8ba7b5a2306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Define the model. Either from scratch of by loading a pre-trained model\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "#Define your train examples. You need more than just two examples...\n",
    "train_examples = [InputExample(texts=['My first sentence', 'My second sentence'], label=0.8),\n",
    "    InputExample(texts=['Another pair', 'Unrelated sentence'], label=0.3)]\n",
    "\n",
    "#Define your train dataset, the dataloader and the train loss\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "#Tune the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "162067d8-dfed-45b3-b1ad-90a7bd3dafd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111d3d02370e42588c160d8b5835293e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2448e9870a614aaea94e6fa8b7279849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import evaluation\n",
    "sentences1 = ['This list contains the first column', 'With your sentences', 'You want your model to evaluate on']\n",
    "sentences2 = ['Sentences contains the other column', 'The evaluator matches sentences1[i] with sentences2[i]', 'Compute the cosine similarity and compares it to scores[i]']\n",
    "scores = [0.3, 0.6, 0.2]\n",
    "\n",
    "evaluator = evaluation.EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)\n",
    "\n",
    "# ... Your other code to load training data\n",
    "\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100, evaluator=evaluator, evaluation_steps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0172bb-8dd2-4671-a22b-9a1378500004",
   "metadata": {},
   "source": [
    "## 2.2. Continue Training on Other Data\n",
    "- https://www.sbert.net/docs/training/overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aac8110-25fd-47b2-801c-5ff19275306e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dcfe4a7ed1f4426b9eec719b90d1675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/392k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-11 14:05:29 - Load pretrained SentenceTransformer: nli-distilroberta-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc22cbab3dc40a8b88951d4b0cef121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/736 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58000961dd594125bde67cace27219f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c84fe0a6c64ca79e98fb9c57205533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.71k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2757b8b544bb4bfc91cee48abdd990b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/679 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38c5b82613e414a953f0b651e260435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867a2356a95d42a79299cfa74ddbe960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616f79466aa943ebaea38aa0ac8a0e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81fd3ba4e9a545c1885efd9cda70c3e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b07c22b1a3b43e8a9ecbd792c25481d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97e53b7a7c349f0ae8d087a4d331294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b9dfc92ce54a50994d19f0e1133c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a333faecfa463daf29878f12359c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405162f20446415a945e59ef8cca33bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-11 14:05:39 - Use pytorch device: cuda\n",
      "2022-08-11 14:05:39 - Read STSbenchmark train dataset\n",
      "2022-08-11 14:05:39 - Read STSbenchmark dev dataset\n",
      "2022-08-11 14:05:39 - Warmup-steps: 144\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9e464c73f84cdbaa639fa2575ebd6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb68cf12d8c46e2903b6a294e77da9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-11 14:06:01 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset after epoch 0:\n",
      "2022-08-11 14:06:02 - Cosine-Similarity :\tPearson: 0.8814\tSpearman: 0.8820\n",
      "2022-08-11 14:06:02 - Manhattan-Distance:\tPearson: 0.8482\tSpearman: 0.8508\n",
      "2022-08-11 14:06:02 - Euclidean-Distance:\tPearson: 0.8499\tSpearman: 0.8530\n",
      "2022-08-11 14:06:02 - Dot-Product-Similarity:\tPearson: 0.8242\tSpearman: 0.8260\n",
      "2022-08-11 14:06:02 - Save model to output/training_stsbenchmark_continue_training-nli-distilroberta-base-v2-2022-08-11_14-05-29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a65d49d84648099c5532023f6f09bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-11 14:06:24 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset after epoch 1:\n",
      "2022-08-11 14:06:26 - Cosine-Similarity :\tPearson: 0.8884\tSpearman: 0.8888\n",
      "2022-08-11 14:06:26 - Manhattan-Distance:\tPearson: 0.8621\tSpearman: 0.8647\n",
      "2022-08-11 14:06:26 - Euclidean-Distance:\tPearson: 0.8641\tSpearman: 0.8673\n",
      "2022-08-11 14:06:26 - Dot-Product-Similarity:\tPearson: 0.8408\tSpearman: 0.8418\n",
      "2022-08-11 14:06:26 - Save model to output/training_stsbenchmark_continue_training-nli-distilroberta-base-v2-2022-08-11_14-05-29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf5b226021c4fcf9fb85a2dccd22f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-11 14:06:48 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset after epoch 2:\n",
      "2022-08-11 14:06:50 - Cosine-Similarity :\tPearson: 0.8894\tSpearman: 0.8895\n",
      "2022-08-11 14:06:50 - Manhattan-Distance:\tPearson: 0.8684\tSpearman: 0.8725\n",
      "2022-08-11 14:06:50 - Euclidean-Distance:\tPearson: 0.8698\tSpearman: 0.8745\n",
      "2022-08-11 14:06:50 - Dot-Product-Similarity:\tPearson: 0.8510\tSpearman: 0.8517\n",
      "2022-08-11 14:06:50 - Save model to output/training_stsbenchmark_continue_training-nli-distilroberta-base-v2-2022-08-11_14-05-29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0270e3db770c41ed8037c44601cf9c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-11 14:07:12 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset after epoch 3:\n",
      "2022-08-11 14:07:13 - Cosine-Similarity :\tPearson: 0.8889\tSpearman: 0.8888\n",
      "2022-08-11 14:07:13 - Manhattan-Distance:\tPearson: 0.8689\tSpearman: 0.8728\n",
      "2022-08-11 14:07:13 - Euclidean-Distance:\tPearson: 0.8703\tSpearman: 0.8747\n",
      "2022-08-11 14:07:13 - Dot-Product-Similarity:\tPearson: 0.8510\tSpearman: 0.8510\n",
      "2022-08-11 14:07:13 - Load pretrained SentenceTransformer: output/training_stsbenchmark_continue_training-nli-distilroberta-base-v2-2022-08-11_14-05-29\n",
      "2022-08-11 14:07:14 - Use pytorch device: cuda\n",
      "2022-08-11 14:07:14 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-test dataset:\n",
      "2022-08-11 14:07:16 - Cosine-Similarity :\tPearson: 0.8631\tSpearman: 0.8629\n",
      "2022-08-11 14:07:16 - Manhattan-Distance:\tPearson: 0.8369\tSpearman: 0.8383\n",
      "2022-08-11 14:07:16 - Euclidean-Distance:\tPearson: 0.8400\tSpearman: 0.8415\n",
      "2022-08-11 14:07:16 - Dot-Product-Similarity:\tPearson: 0.8228\tSpearman: 0.8167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.862920923634329"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This example loads the pre-trained SentenceTransformer model 'nli-distilroberta-base-v2' from the server.\n",
    "It then fine-tunes this model for some epochs on the STS benchmark dataset.\n",
    "Note: In this example, you must specify a SentenceTransformer model.\n",
    "If you want to fine-tune a huggingface/transformers model like bert-base-uncased, see training_nli.py and training_stsbenchmark.py\n",
    "\"\"\"\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout\n",
    "\n",
    "#Check if dataset exsist. If not, download and extract  it\n",
    "sts_dataset_path = 'datasets/stsbenchmark.tsv.gz'\n",
    "\n",
    "if not os.path.exists(sts_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/stsbenchmark.tsv.gz', sts_dataset_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the dataset\n",
    "model_name = 'nli-distilroberta-base-v2'\n",
    "train_batch_size = 16\n",
    "num_epochs = 1\n",
    "model_save_path = 'output/training_stsbenchmark_continue_training-'+model_name+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Convert the dataset to a DataLoader ready for training\n",
    "logging.info(\"Read STSbenchmark train dataset\")\n",
    "\n",
    "train_samples = []\n",
    "dev_samples = []\n",
    "test_samples = []\n",
    "with gzip.open(sts_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        score = float(row['score']) / 5.0  # Normalize score to range 0 ... 1\n",
    "        inp_example = InputExample(texts=[row['sentence1'], row['sentence2']], label=score)\n",
    "\n",
    "        if row['split'] == 'dev':\n",
    "            dev_samples.append(inp_example)\n",
    "        elif row['split'] == 'test':\n",
    "            test_samples.append(inp_example)\n",
    "        else:\n",
    "            train_samples.append(inp_example)\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "\n",
    "# Development set: Measure correlation between cosine score and gold labels\n",
    "logging.info(\"Read STSbenchmark dev dataset\")\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, name='sts-dev')\n",
    "\n",
    "\n",
    "# Configure the training. We skip evaluation in this example\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n",
    "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path)\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#\n",
    "# Load the stored model and evaluate its performance on STS benchmark dataset\n",
    "#\n",
    "##############################################################################\n",
    "\n",
    "model = SentenceTransformer(model_save_path)\n",
    "test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, name='sts-test')\n",
    "test_evaluator(model, output_path=model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33d0134-f39e-4fce-b95a-bcf642abfedd",
   "metadata": {},
   "source": [
    "## 2.3. Multitask Training\n",
    "- https://www.sbert.net/docs/training/overview.html\n",
    "- https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/other/training_multi-task.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6028dba9-618f-4525-9a1c-e7fa73b04676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f281d18b53421a82dfd93477657b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/40.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3271ca74909443eeac1320378fa85942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846c817e40f44cf2b810c1349e685f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e14b559c5b4933b26ff43f99e1e69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07769f68362940339d537d278c772281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ecd608528f4b16aaaad8bf0867c54a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-11 14:07:28 - Use pytorch device: cuda\n",
      "2022-08-11 14:07:28 - Read AllNLI train dataset\n",
      "2022-08-11 14:07:37 - Softmax loss: #Vectors concatenated: 3\n",
      "2022-08-11 14:07:37 - Read STSbenchmark train dataset\n",
      "2022-08-11 14:07:37 - Read STSbenchmark dev dataset\n",
      "2022-08-11 14:07:37 - Warmup-steps: 144\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bbc9318e1d749e5bab9108f12299244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2683d0e2b8a64b9483e4d3c8dc1fa362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-11 14:08:56 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset after epoch 0:\n",
      "2022-08-11 14:08:59 - Cosine-Similarity :\tPearson: 0.8356\tSpearman: 0.8376\n",
      "2022-08-11 14:08:59 - Manhattan-Distance:\tPearson: 0.7746\tSpearman: 0.7817\n",
      "2022-08-11 14:08:59 - Euclidean-Distance:\tPearson: 0.7749\tSpearman: 0.7823\n",
      "2022-08-11 14:08:59 - Dot-Product-Similarity:\tPearson: 0.7151\tSpearman: 0.7318\n",
      "2022-08-11 14:08:59 - Save model to output/training_multi-task_bert-base-uncased-2022-08-11_14-07-16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77086c8fe0e34795935641fb6b352d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is an example how to train SentenceTransformers in a multi-task setup.\n",
    "The system trains BERT on the AllNLI and on the STSbenchmark dataset.\n",
    "\"\"\"\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers import LoggingHandler, SentenceTransformer, util\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.readers import *\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import gzip\n",
    "import csv\n",
    "import os\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout\n",
    "\n",
    "# Read the dataset\n",
    "model_name = 'bert-base-uncased'\n",
    "batch_size = 16\n",
    "model_save_path = 'output/training_multi-task_'+model_name+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "#Check if dataset exsist. If not, download and extract  it\n",
    "nli_dataset_path = 'datasets/AllNLI.tsv.gz'\n",
    "sts_dataset_path = 'datasets/stsbenchmark.tsv.gz'\n",
    "\n",
    "if not os.path.exists(nli_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/AllNLI.tsv.gz', nli_dataset_path)\n",
    "\n",
    "if not os.path.exists(sts_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/stsbenchmark.tsv.gz', sts_dataset_path)\n",
    "\n",
    "\n",
    "\n",
    "# Use BERT for mapping tokens to embeddings\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False)\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "\n",
    "# Convert the dataset to a DataLoader ready for training\n",
    "logging.info(\"Read AllNLI train dataset\")\n",
    "label2int = {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}\n",
    "train_nli_samples = []\n",
    "with gzip.open(nli_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        if row['split'] == 'train':\n",
    "            label_id = label2int[row['label']]\n",
    "            train_nli_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=label_id))\n",
    "\n",
    "\n",
    "train_dataloader_nli = DataLoader(train_nli_samples, shuffle=True, batch_size=batch_size)\n",
    "train_loss_nli = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=len(label2int))\n",
    "\n",
    "logging.info(\"Read STSbenchmark train dataset\")\n",
    "train_sts_samples = []\n",
    "dev_sts_samples = []\n",
    "test_sts_samples = []\n",
    "with gzip.open(sts_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        score = float(row['score']) / 5.0  # Normalize score to range 0 ... 1\n",
    "        inp_example = InputExample(texts=[row['sentence1'], row['sentence2']], label=score)\n",
    "\n",
    "        if row['split'] == 'dev':\n",
    "            dev_sts_samples.append(inp_example)\n",
    "        elif row['split'] == 'test':\n",
    "            test_sts_samples.append(inp_example)\n",
    "        else:\n",
    "            train_sts_samples.append(inp_example)\n",
    "\n",
    "\n",
    "train_dataloader_sts = DataLoader(train_sts_samples, shuffle=True, batch_size=batch_size)\n",
    "train_loss_sts = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "\n",
    "logging.info(\"Read STSbenchmark dev dataset\")\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_sts_samples, name='sts-dev')\n",
    "\n",
    "# Configure the training\n",
    "num_epochs = 1\n",
    "\n",
    "warmup_steps = math.ceil(len(train_dataloader_sts) * num_epochs * 0.1) #10% of train data for warm-up\n",
    "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "\n",
    "# Here we define the two train objectives: train_dataloader_nli with train_loss_nli (i.e., SoftmaxLoss for NLI data)\n",
    "# and train_dataloader_sts with train_loss_sts (i.e., CosineSimilarityLoss for STSbenchmark data)\n",
    "# You can pass as many (dataloader, loss) tuples as you like. They are iterated in a round-robin way.\n",
    "train_objectives = [(train_dataloader_nli, train_loss_nli), (train_dataloader_sts, train_loss_sts)]\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=train_objectives,\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path\n",
    "          )\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#\n",
    "# Load the stored model and evaluate its performance on STS benchmark dataset\n",
    "#\n",
    "##############################################################################\n",
    "\n",
    "model = SentenceTransformer(model_save_path)\n",
    "test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_sts_samples, name='sts-test')\n",
    "test_evaluator(model, output_path=model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c129ea43-3dc9-4e36-9ff5-bc2996c5c660",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. 커널 리스타트\n",
    "\n",
    "\n",
    "- 커널 리스타트에 대한 내용이 있습니다. 클릭 후 가장 하단의 \"3.커널 리스타팅\" 을 참조 하세요.\n",
    "    - [리스타트 상세](https://github.com/gonsoomoon-ml/NLP-HuggingFace-On-SageMaker/blob/main/1_NSMC-Classification/2_WarmingUp/0.1.warming_up_yelp_review.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4841e65a-772b-423f-9de2-885a5164a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff8c26-803e-42f7-bc56-cafab2d257f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
