{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8e350e8-d8fc-4a24-96b1-5fdcd52d9033",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Warming Up - PLM + 그룹 Custom Classifiers 만들기\n",
    "\n",
    "이 노트북은 크게 2개의 모델을 생성하여 파이프라인으로 연결를 결과로 만듧니다.\n",
    "- 첫 번재 모델\n",
    "    - BERT Pretrained Model (PLM)\n",
    "- 두 번째 모델\n",
    "    - 4 개의 Classifiers 로 구성된 모델 \n",
    "        - Classifiers_01, Classifiers02, Classifiers_03, Classifiers_04\n",
    "- 추론\n",
    "    - PLM --> Classifiers 로 이루어 지며, 최종 4개의 Classifier 의 모델 결과가 제공 됨.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 참조: \n",
    "- 딥러닝으로 리뷰에서 제품 속성 정보 추출하기\n",
    "    * http://blog.hwahae.co.kr/all/tech/tech-tech/5967/\n",
    "- A Visual Guide to Using BERT for the First Time\n",
    "    - http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n",
    "- PyTorch 101, Part 3: Going Deep with PyTorch\n",
    "    - https://blog.paperspace.com/pytorch-101-advanced/\n",
    "- Pytorch freeze part of the layers\n",
    "    - https://jimmy-shen.medium.com/pytorch-freeze-part-of-the-layers-4554105e03a6\n",
    "- BERT Fine-Tuning Tutorial with PyTorch\n",
    "    - https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
    "- How many layers of my BERT model should I freeze?\n",
    "    - https://raphaelb.org/posts/freezing-bert/\n",
    "- Add dense layer on top of Huggingface BERT model\n",
    "    - https://pyquestions.com/add-dense-layer-on-top-of-huggingface-bert-model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edae9044-ee12-4674-b344-c58323a1ddee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. 환경 셋업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d65b01ce-944d-4897-be9e-45bd6c8ea29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "from datasets import load_dataset,Dataset,DatasetDict\n",
    "from transformers import DataCollatorWithPadding,AutoModelForSequenceClassification, Trainer, TrainingArguments,AutoTokenizer,AutoModel,AutoConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad78dd5-fbd9-4327-bdda-f9bca048bb9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd05897d-07f7-4e52-ae01-f2da741e6f1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-5fa7fc59288bdff3\n",
      "Reusing dataset json (/home/ec2-user/.cache/huggingface/datasets/json/default-5fa7fc59288bdff3/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a303436e2a424d0e8424b0bda6e944ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "data=load_dataset(\"json\",data_files=\"download_data/news-headlines-dataset-for-sarcasm-detection.zip\")\n",
    "data=data.rename_column(\"is_sarcastic\",\"label\")\n",
    "\n",
    "data=data.remove_columns(['article_link'])\n",
    "\n",
    "data.set_format('pandas')\n",
    "data=data['train'][:]\n",
    "\n",
    "data.drop_duplicates(subset=['headline'],inplace=True)\n",
    "data=data.reset_index()[['headline','label']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a30c77-427c-4654-97f3-89b6b84b7d36",
   "metadata": {},
   "source": [
    "## 1.1 데이터 확인\n",
    "- is_sarcastic (풍자) 에 따라 레이블이 1 과 0 임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c083af7-9f3a-473c-8507-0d466cac6b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28498</th>\n",
       "      <td>tyson holds contest to let fans submit new ide...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28499</th>\n",
       "      <td>increasingly cocky bernie sanders announces he...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28500</th>\n",
       "      <td>cash-strapped zuckerberg forced to sell 11 mil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28501</th>\n",
       "      <td>grocery store bar actually has great little ha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28502</th>\n",
       "      <td>study: 83% of marathon spectators only attend ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28503 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                headline  label\n",
       "0      former versace store clerk sues over secret 'b...      0\n",
       "1      the 'roseanne' revival catches up to our thorn...      0\n",
       "2      mom starting to fear son's web series closest ...      1\n",
       "3      boehner just wants wife to listen, not come up...      1\n",
       "4      j.k. rowling wishes snape happy birthday in th...      0\n",
       "...                                                  ...    ...\n",
       "28498  tyson holds contest to let fans submit new ide...      1\n",
       "28499  increasingly cocky bernie sanders announces he...      1\n",
       "28500  cash-strapped zuckerberg forced to sell 11 mil...      1\n",
       "28501  grocery store bar actually has great little ha...      1\n",
       "28502  study: 83% of marathon spectators only attend ...      1\n",
       "\n",
       "[28503 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98930f86-4081-4de4-abb4-ac7efe67ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=Dataset.from_pandas(data)\n",
    "\n",
    "# 80% train, 20% test + validation\n",
    "train_testvalid = data.train_test_split(test_size=0.2,seed=15)\n",
    "\n",
    "# Split the 10% test + valid in half test, half valid\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5,seed=15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81d67698-d937-4379-8130-12e845b69dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['headline', 'label'],\n",
       "         num_rows: 22802\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['headline', 'label'],\n",
       "         num_rows: 5701\n",
       "     })\n",
       " }),\n",
       " DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['headline', 'label'],\n",
       "         num_rows: 2850\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['headline', 'label'],\n",
       "         num_rows: 2851\n",
       "     })\n",
       " }))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_testvalid, test_valid "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba344d10-16fb-46b4-9a99-4d9498f952e0",
   "metadata": {},
   "source": [
    "## 1.2 최종 사전 형태의 데이터 세트 (훈련, 검증, 테스트 셋) 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afd86d79-39df-4eca-9304-d153e9c93ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['headline', 'label'],\n",
       "        num_rows: 22802\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['headline', 'label'],\n",
       "        num_rows: 2851\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['headline', 'label'],\n",
       "        num_rows: 2850\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gather everyone if you want to have a single DatasetDict\n",
    "data = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdc155fe-c505-416d-8c06-af4e3fd32d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "headline: string\n",
       "label: int64\n",
       "----\n",
       "headline: [[\"former versace store clerk sues over secret 'black code' for minority shoppers\",\"the 'roseanne' revival catches up to our thorny political mood, for better and worse\",\"mom starting to fear son's web series closest thing she will have to grandchild\",\"boehner just wants wife to listen, not come up with alternative debt-reduction ideas\",\"j.k. rowling wishes snape happy birthday in the most magical way\"]]\n",
       "label: [[0,0,1,1,0]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"].data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadb5211-c576-4eaa-b55d-7dddfc0a8d27",
   "metadata": {},
   "source": [
    "## 1.3. 토큰나이저 로딩 및 BERT 인코딩으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eac52622-bd95-4aff-98ef-77cebc81f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'bert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.model_max_len=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e738b358-bd83-4dfe-a534-d1cf8710cd83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e86d9860e09459188752f5f8325bb59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba0029a85a446959711e7375bb03711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb8c2bb4bcd415a95933fabe8f30a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['headline', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 22802\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['headline', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2851\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['headline', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2850\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "  return tokenizer(batch[\"headline\"], truncation=True,max_length=512)\n",
    "\n",
    "tokenized_dataset = data.map(tokenize, batched=True)\n",
    "#sample_dataset = data.map(tokenize, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac537ef-c266-4194-a1de-f47653816f8f",
   "metadata": {},
   "source": [
    "## 1.4. 데이터 로더 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba0b944f-dfe0-4d6d-9865-67c4c5a7a712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorWithPadding(tokenizer=PreTrainedTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}), padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset.set_format(\"torch\",columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e32e6f9-b344-47d4-8f0d-dbaf36a91acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_batch_size = 50\n",
    "eval_batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"train\"], shuffle=True, batch_size=train_batch_size, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"valid\"], batch_size=eval_batch_size, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1739c834-f504-4741-91b9-4c6b75281427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   170,  1440,  1120,  9304,  9753,  1162, 18254,   112,  1297,\n",
       "          1105,  1578,  1112,  1103,  2851,  3587,  1851,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,   176, 25690,  9037,  1106,  2194,  2919,  6486,  5914,  1114,\n",
       "          1714,   188,  1204,  1181,  2774, 21195,  1254,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  4267,  1116,  4695,  1231, 15189,  1116,  1900, 17462,  1279,\n",
       "          2560,  1179,  1112,  1226,  1104,  1419,   118,  2043,  4684,  2019,\n",
       "          9712, 26348,   185,  1174,  4184, 20473,  1465,   102],\n",
       "        [  101,   192,  7745,  5834,  1940,  1116, 13830, 10517,  1299,  4482,\n",
       "          1158,  1146,  1113, 20188,  1104,  6831,  1111,  1981, 15841,  3842,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  5871, 24138,  1180, 12999,  1154,   112,  6866,   112,  2963,\n",
       "         16884,  1114,  7166,   131,  6962,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  3787,  5166,  1106,  3073,   118,  1402, 17423,  1204,  1634,\n",
       "          1104,  6646,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  1653, 28117,  3101, 23864,  2230, 21306, 15185,  1268,  1146,\n",
       "          1175,  1114,  1168,  3117,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  1143, 27511,   176, 23464,  3632,  1146,  1219,  3049,  3654,\n",
       "         26103,  4134,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,   170,  3186,   112,   188,   189,  6439, 10965,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  1969,   118,  1214,   118,  1385,  1144,   188,  8508,  3781,\n",
       "          1716,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  1150, 14505,  1111,  1167,   112, 20796,  1732,  2008,   112,\n",
       "          8050, 23838,  4198,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,   185, 18318,   182, 19495,  3740,  4695, 15077,  1103,  3222,\n",
       "          2897,  1464,   112,  4963,   112,  1118, 26063,  2718,  1113,  1186,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  4208,   117,   171,  4867,  1195, 19202,  1197,  1108,  3950,\n",
       "          1111,   189,  4894, 21793,  1120,   171,  4867, 24078,  6906, 17876,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  1293,  1195,  1245,  1103,  1119, 21704,  2556,  3668,  1468,\n",
       "          1107,   170,  1432,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  1143,  4661,  1883,   112,   188,  1710,  7425,  1118,  2848,\n",
       "           118, 12338,   170,  2087,  1181,  1107,   176, 14170,  1352,  1728,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  8524, 18241,  1106,  1260, 17532, 14120, 24820,   112,   188,\n",
       "         10497,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]]), 'labels': tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(eval_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165ea065-8111-4fdb-a03d-2e982e2ddebe",
   "metadata": {},
   "source": [
    "# 2.모델 정의 및 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea28a4c-bb65-4fe1-a85a-01e6396191fd",
   "metadata": {},
   "source": [
    "## 2.1. Pre-Trained Model 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1202dfa8-5989-4f09-8b0b-871c51185e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "plm = AutoModel.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62d47c7-c9b1-4783-b393-46aff52ab1bb",
   "metadata": {},
   "source": [
    "## 2.2. BERT 아키텍쳐 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c870af1-4eef-40b0-acdc-e853eebf4eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_module(model):\n",
    "    for name, child in model.named_children():\n",
    "        print(\"name :\", name)\n",
    "        #print(\"child: \\n\", child)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21fe8f79-2277-4ab9-8603-ec6639d49225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : embeddings\n",
      "name : encoder\n",
      "name : pooler\n"
     ]
    }
   ],
   "source": [
    "show_module(plm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e155afb-f150-4b91-8278-32c34651eef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(plm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec64eadd-2545-40c4-a276-5d4bbaccde0d",
   "metadata": {},
   "source": [
    "## 2.3. Custom Classifier 추가 하여 Custom Model 생성 하기\n",
    "- PLM + Classifier 로 구성됨.\n",
    "- PLM 레이어는 훈련을 안하기 위해 파라이터 Freezing 을 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dd03189-b6f2-42d7-981e-d42b77ed47b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBERTModel(nn.Module):\n",
    "    '''\n",
    "    plm 파라미터는 freezing 하여 훈련을 하지 않음.\n",
    "    '''\n",
    "    def __init__(self,model ,num_labels): \n",
    "        super(CustomBERTModel,self).__init__() \n",
    "        self.num_labels = num_labels \n",
    "\n",
    "        self.plm = model\n",
    "        # self.dropout = nn.Dropout(0.1) \n",
    "        # self.classifier = nn.Linear(768,num_labels) # load and initialize weights\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.1),            \n",
    "            nn.Linear(768,256),        \n",
    "            nn.Dropout(0.1),                        \n",
    "            nn.Linear(256,num_labels) \n",
    "        )\n",
    "                \n",
    "        self.freeze_plm()        \n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None,labels=None):\n",
    "        #Extract outputs from the body\n",
    "        outputs = self.plm(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        #print(\"outputs shape: \", outputs[0].shape)\n",
    "        \n",
    "        #sequence_output = self.dropout(outputs[0]) # outputs[0]=last hidden state\n",
    "        # print(\"sequence_output shape: \", sequence_output.shape)\n",
    "        \n",
    "        cls_vector = outputs[0][:,0,:].view(-1,768) # outputs[0] 은 last_hidden_state, outputs[1] 은 pooled_output_state\n",
    "        # print(\"cls_vector shape: \", cls_vector.shape)        \n",
    "        \n",
    "        logits = self.classifier(cls_vector) \n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states,attentions=outputs.attentions)\n",
    "\n",
    "    def freeze_plm(self):\n",
    "        \"\"\"\n",
    "        Freezes the parameters of BERT so when BertWithCustomNNClassifier is trained\n",
    "        only the wieghts of the custom classifier are modified.\n",
    "        \"\"\"\n",
    "        for param in self.plm.parameters():\n",
    "            param.requires_grad=False\n",
    "        #     print(param.requires_grad)\n",
    "        \n",
    "    \n",
    "    def unfreeze_plm(self):\n",
    "        \"\"\"\n",
    "        Unfreezes the parameters of BERT so when BertWithCustomNNClassifier is trained\n",
    "        both the wieghts of the custom classifier and of the underlying BERT are modified.\n",
    "        \"\"\"\n",
    "        for param in self.plm.parameters():\n",
    "            param.requires_grad=True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05928664-cf83-42c1-aee9-27dd047b82b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "custom_model=CustomBERTModel(model = plm ,num_labels=2).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8ed72f-60bb-4507-b1f6-a6e714d7fd2c",
   "metadata": {},
   "source": [
    "최상위 모듈(레이어)  확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f330da11-8745-488a-a234-198abae665af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : plm\n",
      "name : classifier\n"
     ]
    }
   ],
   "source": [
    "show_module(custom_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9f90c0-5207-43ba-8ed3-888411678d8a",
   "metadata": {},
   "source": [
    "파라미터 Freezing 여부 확인. plm == freezing , classifier == trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e235cdbc-b8a8-409e-9bd2-64271f15b8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.1.weight\n",
      "classifier.1.bias\n",
      "classifier.3.weight\n",
      "classifier.3.bias\n"
     ]
    }
   ],
   "source": [
    "def show_trainable_layer(model):\n",
    "    # requires_grad == true 만 출력\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad: print(name) \n",
    "\n",
    "show_trainable_layer(custom_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908100ea-bf3b-424f-98cf-43151532c22a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.4. Custome Model 아키텍쳐 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8901768d-e36a-4bd4-bcaa-2dc7c9755577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomBERTModel(\n",
      "  (plm): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.1, inplace=False)\n",
      "    (1): Linear(in_features=768, out_features=256, bias=True)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(custom_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947cf866-7801-42ca-96b7-dd3db93c34ed",
   "metadata": {},
   "source": [
    "# 3. 훈련 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e53f52d-057f-4791-8305-9fc1d47d389d",
   "metadata": {},
   "source": [
    "## 3.1. 모델 평가 지표 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7c553d0-627c-4454-9311-dc31cea1ba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "# metric = load_metric(\"f1\")\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf03f4f-3a9d-4c6e-b674-6153bbeb9272",
   "metadata": {},
   "source": [
    "## 3.2. 옵티마이저, 스케줄러, 훈련 루프 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99abb865-3cbb-4de8-a415-e5771b4cc1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW,get_scheduler\n",
    "\n",
    "def create_optimizer_scheduler(num_epochs, model, train_dataloader):\n",
    "    # plm freezing optimizer\n",
    "    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
    "    # optimizer withoud frezzing\n",
    "    # optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    \n",
    "    def get_lr_scheduler(num_epochs, train_dataloader, optimizer):\n",
    "        num_training_steps = num_epochs * len(train_dataloader)\n",
    "        lr_scheduler = get_scheduler(\n",
    "            \"linear\",\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=num_training_steps,\n",
    "        )\n",
    "\n",
    "        print(num_training_steps)\n",
    "\n",
    "        return lr_scheduler, num_training_steps\n",
    "    \n",
    "    lr_scheduler, num_training_steps = get_lr_scheduler(num_epochs, train_dataloader, optimizer)\n",
    "    \n",
    "    return optimizer, lr_scheduler, num_training_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "967963b0-ab70-4ab9-856d-7eb11bc0f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(num_epochs, model, train_dataloader, progress_bar_train, \\\n",
    "               eval_dataloader, progress_bar_eval, metric, optimizer, lr_scheduler):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar_train.update(1)\n",
    "                \n",
    "        # 모델 평가\n",
    "        model.eval()\n",
    "        for batch in eval_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = custom_model(**batch)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "            progress_bar_eval.update(1)\n",
    "            \n",
    "\n",
    "        print(metric.compute())\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4182abae-475d-4786-8023-d65f27c94f11",
   "metadata": {},
   "source": [
    "# 3.3. 훈련 루프 실행 및 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8679a041-6d01-42ad-a800-36f90ff3defc",
   "metadata": {},
   "source": [
    "훈련 루프에 입력이 될 Batch 확인 함. 'eval_dataloader' 를 'train_dataloader' 로 바꾸어서 보시면 됩니다.\n",
    "레코드가 많이 출력이 되어서 eval_dataloader 로 확인 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ecf33b9-2878-4df2-b9ed-96b882a12354",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,   170,  1440,  1120,  9304,  9753,  1162, 18254,   112,  1297,\n",
      "          1105,  1578,  1112,  1103,  2851,  3587,  1851,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   176, 25690,  9037,  1106,  2194,  2919,  6486,  5914,  1114,\n",
      "          1714,   188,  1204,  1181,  2774, 21195,  1254,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  4267,  1116,  4695,  1231, 15189,  1116,  1900, 17462,  1279,\n",
      "          2560,  1179,  1112,  1226,  1104,  1419,   118,  2043,  4684,  2019,\n",
      "          9712, 26348,   185,  1174,  4184, 20473,  1465,   102],\n",
      "        [  101,   192,  7745,  5834,  1940,  1116, 13830, 10517,  1299,  4482,\n",
      "          1158,  1146,  1113, 20188,  1104,  6831,  1111,  1981, 15841,  3842,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  5871, 24138,  1180, 12999,  1154,   112,  6866,   112,  2963,\n",
      "         16884,  1114,  7166,   131,  6962,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  3787,  5166,  1106,  3073,   118,  1402, 17423,  1204,  1634,\n",
      "          1104,  6646,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1653, 28117,  3101, 23864,  2230, 21306, 15185,  1268,  1146,\n",
      "          1175,  1114,  1168,  3117,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1143, 27511,   176, 23464,  3632,  1146,  1219,  3049,  3654,\n",
      "         26103,  4134,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   170,  3186,   112,   188,   189,  6439, 10965,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1969,   118,  1214,   118,  1385,  1144,   188,  8508,  3781,\n",
      "          1716,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1150, 14505,  1111,  1167,   112, 20796,  1732,  2008,   112,\n",
      "          8050, 23838,  4198,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,   185, 18318,   182, 19495,  3740,  4695, 15077,  1103,  3222,\n",
      "          2897,  1464,   112,  4963,   112,  1118, 26063,  2718,  1113,  1186,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  4208,   117,   171,  4867,  1195, 19202,  1197,  1108,  3950,\n",
      "          1111,   189,  4894, 21793,  1120,   171,  4867, 24078,  6906, 17876,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1293,  1195,  1245,  1103,  1119, 21704,  2556,  3668,  1468,\n",
      "          1107,   170,  1432,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1143,  4661,  1883,   112,   188,  1710,  7425,  1118,  2848,\n",
      "           118, 12338,   170,  2087,  1181,  1107,   176, 14170,  1352,  1728,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  8524, 18241,  1106,  1260, 17532, 14120, 24820,   112,   188,\n",
      "         10497,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]], device='cuda:0'), 'labels': tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a7f935f-337c-4d48-b26c-3f29c465f8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1561bdbf328f42629a2873e2d950fae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed428f403194c80a28e6a90ff89aab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7410526315789474}\n",
      "{'accuracy': 0.7519298245614036}\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "num_epochs = 2\n",
    "optimizer, lr_scheduler, num_training_steps = create_optimizer_scheduler(num_epochs, custom_model, train_dataloader)\n",
    "\n",
    "progress_bar_train = tqdm(range(num_training_steps))\n",
    "progress_bar_eval = tqdm(range(num_epochs * len(eval_dataloader)))\n",
    "\n",
    "custom_model_01 = train_loop(num_epochs, custom_model, train_dataloader, progress_bar_train, \\\n",
    "               eval_dataloader, progress_bar_eval, metric, optimizer, lr_scheduler)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ec30a-60f1-47db-8dd7-36ff39557e01",
   "metadata": {},
   "source": [
    "# 5. 테스트 데이터 로 모델 평가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "462f65e8-aef2-4867-83b4-4a2d3a95ca2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7649947386881796}\n"
     ]
    }
   ],
   "source": [
    "test_batch_size = 32\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"test\"], batch_size= test_batch_size, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "def evaL_model(model, test_dataloader, metric):\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "    print(metric.compute())\n",
    "    \n",
    "evaL_model(custom_model_01, test_dataloader, metric)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf66b786-7686-49a3-9190-f0b45ef8faec",
   "metadata": {},
   "source": [
    "# 6. 모델 분리 후 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "586d1c24-8064-4c17-ba85-27ea003445c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : plm\n",
      "name : classifier\n"
     ]
    }
   ],
   "source": [
    "show_module(custom_model_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702b8c94-ed34-402c-b3c2-0874d4e025cc",
   "metadata": {},
   "source": [
    "## 6.1. 추론 PLM Model 생성 \n",
    "- custom_model (PLM + Classifier) 에서 PLM 만을 분리 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7fe3ec7-0b71-4a3f-bc78-4fe14eebddd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLM: \n",
      "name : plm\n"
     ]
    }
   ],
   "source": [
    "class PLModel(nn.Module):\n",
    "    def __init__(self, base_model, num_labels): \n",
    "        super(PLModel,self).__init__() \n",
    "\n",
    "        self.plm = base_model.plm\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None,labels=None):\n",
    "        #Extract outputs from the body\n",
    "\n",
    "        #Add custom layers\n",
    "        outputs = self.plm(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        \n",
    "        cls_vector = outputs[0][:,0,:].view(-1,768)\n",
    "        # print(\"cls_vector shape: \", cls_vector.shape)                \n",
    "\n",
    "        return cls_vector\n",
    "\n",
    "PL_Model=PLModel(base_model = custom_model_01 ,num_labels=2).to(device)\n",
    "print(\"PLM: \")\n",
    "show_module(PL_Model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddec29e5-3c6f-4bf8-81db-91df3da33d3f",
   "metadata": {},
   "source": [
    "## 6.2. 추천 이진 분류기 모델 \n",
    "- custom_model (PLM + Classifier) 에서 Classifier 만을 분리 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ad572b9-0943-4a82-80a9-f1326f4a9064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifier: \n",
      "\n",
      "name : classifier\n"
     ]
    }
   ],
   "source": [
    "class ClassifierModel(nn.Module):\n",
    "    def __init__(self, base_model, num_labels): \n",
    "        super(ClassifierModel,self).__init__() \n",
    "\n",
    "        self.num_labels = num_labels \n",
    "        #self.dropout = nn.Dropout(0.1)     \n",
    "        self.classifier = base_model.classifier    \n",
    "        \n",
    "\n",
    "    def forward(self, cls_vector=None, labels=None):\n",
    "\n",
    "        #Add custom layers\n",
    "        #cls_vector = self.dropout(cls_vector) #outputs[0]=last hidden state\n",
    "        logits = self.classifier(cls_vector)\n",
    "        \n",
    "        # logits = self.classifier(sequence_output[:,0,:].view(-1,768)) # calculate losses\n",
    "\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "classifier_01 = ClassifierModel(base_model = custom_model_01 ,num_labels=2).to(device)\n",
    "print(\"\\nClassifier: \\n\")\n",
    "show_module(classifier_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde78ad2-405e-4eea-86cc-469b634df10f",
   "metadata": {},
   "source": [
    "## 6.3. PLM 모델 추론\n",
    "- BERT Encoding 을 입력하여 PLM 모델을 통해서 (Batch_Size, 25, 768) 벡터를 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b755eb7-ca9b-417f-9494-404fa3b42ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size:  90\n",
      "one batch shape:  torch.Size([32, 768])\n"
     ]
    }
   ],
   "source": [
    "def inference_plm(model, sample_dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    output_list = []\n",
    "    for i, batch in enumerate(sample_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            output_list.append(outputs)\n",
    "        \n",
    "        # if i == 10:\n",
    "        #     break\n",
    "            \n",
    "    return output_list\n",
    "\n",
    "    \n",
    "plm_vector = inference_plm(PL_Model, test_dataloader)\n",
    "print(\"batch size: \" , len(plm_vector))\n",
    "print(\"one batch shape: \" , plm_vector[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa797a7-e722-460c-93ac-d79759acd59a",
   "metadata": {},
   "source": [
    "## 6.4. 이진 분류기 추론\n",
    "PLM 모델을 통해서 (Batch_Size, 25, 768) 벡터를 입력으로 하여 Classifier 로 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79e27208-7c48-4d8f-8a9b-51af99c66e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25308e41-7a7d-4f27-a1f5-a2ff7696f8cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7649947386881796}\n"
     ]
    }
   ],
   "source": [
    "def inference_classifier(model, plm_vector, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    output_list = []\n",
    "    for batch, reference in zip(plm_vector, test_loader):\n",
    "        # print(reference['labels'])\n",
    "        # batch = batch[0].to(device)\n",
    "\n",
    "        #print(\"batch shape: \", batch.shape)\n",
    "        # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch)\n",
    "            # print(\"outputs: \", outputs.shape)\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "            #print(predictions)\n",
    "            metric.add_batch(predictions=predictions, references=reference[\"labels\"])\n",
    "            \n",
    "            output_list.append(outputs)\n",
    "\n",
    "        \n",
    "    print(metric.compute())        \n",
    "                \n",
    "    return output_list\n",
    "\n",
    "output_list = inference_classifier(classifier_01, plm_vector, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccb0800-a043-4d22-8ec0-cbf119ee521d",
   "metadata": {},
   "source": [
    "# 7. 두번째 모델 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af95376b-661d-4cb2-b0f8-d61a87413601",
   "metadata": {},
   "source": [
    "## 7.1. plm plus custom classifier 로 두번째 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "566359cb-a046-4153-950a-4c5285455474",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model =CustomBERTModel(model = plm ,num_labels=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f148b7d3-64c3-4cd9-aee3-64cdb0427df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.1.weight\n",
      "classifier.1.bias\n",
      "classifier.3.weight\n",
      "classifier.3.bias\n"
     ]
    }
   ],
   "source": [
    "show_trainable_layer(custom_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a78c85-e29c-43cf-98b1-43ee780c184a",
   "metadata": {},
   "source": [
    "## 7.2. 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0db76d1-350f-42d9-b865-4daf1196069c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "optimizer, lr_scheduler, num_training_steps = create_optimizer_scheduler(num_epochs, custom_model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc659cd2-f326-4728-8838-abce355fecf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5db907b0394da88d1bbdec76a0418a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f53875be7f4c3e9138c37c88aede0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7424561403508771}\n",
      "{'accuracy': 0.755438596491228}\n"
     ]
    }
   ],
   "source": [
    "progress_bar_train = tqdm(range(num_training_steps))\n",
    "progress_bar_eval = tqdm(range(num_epochs * len(eval_dataloader)))\n",
    "\n",
    "custom_model_02 = train_loop(num_epochs, custom_model, train_dataloader, progress_bar_train, \\\n",
    "               eval_dataloader, progress_bar_eval, metric, optimizer, lr_scheduler)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ed69c6-75db-48d2-85d5-f5b6c7b727f6",
   "metadata": {},
   "source": [
    "## 7.3. 테스트 데이터로 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ec150c8-e166-4a94-a7d3-7d7631d02409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7646439845668187}\n"
     ]
    }
   ],
   "source": [
    "evaL_model(custom_model_02, test_dataloader, metric)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b28f7e-32c7-4b64-9c75-2685970935f9",
   "metadata": {},
   "source": [
    "## 7.4. 두번째 모델에서 Classifier 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d867f2b0-b1a1-494b-a679-25d723c7fa08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : classifier\n"
     ]
    }
   ],
   "source": [
    "classifier_02 = ClassifierModel(base_model = custom_model_02 ,num_labels=2).to(device)\n",
    "show_module(classifier_02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15fecef-675b-4155-8126-3f846005a800",
   "metadata": {},
   "source": [
    "## 7.6. 이진 분류기로 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7803161e-ccd6-43ee-b42f-0a2fc58eb843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7646439845668187}\n"
     ]
    }
   ],
   "source": [
    "output_list = inference_classifier(classifier_02, plm_vector, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0df1776-c080-46c7-8e1b-6fe2a14828ab",
   "metadata": {},
   "source": [
    "# 8. 첫번째, 두번째의 모델을 한개의 모델로 통합\n",
    "- Classifier_01 , Classifier_02 를 한개의 모델 안으로 포함 시킴 (Combine_Classifier_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a3803b1-9bca-4277-aa58-ad224ca0e7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : base_classifier\n",
      "name : add_classifier\n"
     ]
    }
   ],
   "source": [
    "class CombineClassifier(nn.Module):\n",
    "    def __init__(self, base_classifier, add_classifier): \n",
    "        super(CombineClassifier,self).__init__() \n",
    "\n",
    "        # self.dropout = nn.Dropout(0.1)     \n",
    "        \n",
    "        self.base_classifier = base_classifier\n",
    "        self.add_classifier = add_classifier\n",
    "\n",
    "    def forward(self, cls_vector=None, labels=None):\n",
    "\n",
    "        #Add custom layers\n",
    "        #print(\"cls_vector shape: \", cls_vector.shape)\n",
    "        x = cls_vector\n",
    "        base_logits = self.base_classifier(x) \n",
    "        add_logits = self.add_classifier(x)\n",
    "\n",
    "\n",
    "        return base_logits, add_logits\n",
    "\n",
    "\n",
    "Combine_Classifier_02 = CombineClassifier(base_classifier = classifier_01 ,add_classifier = classifier_02).to(device)\n",
    "show_module(Combine_Classifier_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e68f816-2f62-416d-ba45-5dcfec5a5cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CombineClassifier(\n",
      "  (base_classifier): ClassifierModel(\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=256, bias=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): Linear(in_features=256, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (add_classifier): ClassifierModel(\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=256, bias=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): Linear(in_features=256, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(Combine_Classifier_02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c207a221-3caf-49ab-9e8e-7a20e06696b6",
   "metadata": {},
   "source": [
    "복수개의 Classifier 가 있는 Combine_Classifier 모델의 추론을 및 평가를 하는 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e99ead7-e0d0-4846-be7d-17001d002631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_classifier2(model, plm_vector, test_dataloader, test_batch_size, verbose=False):\n",
    "    def get_depth(l):\n",
    "        if isinstance(l, list):\n",
    "            return 1 + max(get_depth(item) for item in l)\n",
    "        elif isinstance(l, tuple):\n",
    "            return 1 + max(get_depth(item) for item in l)\n",
    "\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def unflatten_tuple(t, depth):\n",
    "        e_list = []\n",
    "        while True:\n",
    "            if depth ==0:\n",
    "                e_list.append(x)\n",
    "                break\n",
    "            x, y = t\n",
    "            e_list.append(y)\n",
    "\n",
    "            t = x\n",
    "            #print(\"x: \", x)\n",
    "\n",
    "            depth -= 1\n",
    "\n",
    "        e_list.reverse()\n",
    "\n",
    "        return e_list\n",
    "\n",
    "    def get_num_model():\n",
    "        model.eval()\n",
    "\n",
    "        test_batch_num = len(test_dataloader)\n",
    "        total_correct, correct = 0 , 0\n",
    "\n",
    "        output_list = []\n",
    "        for batch, reference in zip(plm_vector, test_dataloader):\n",
    "            # print(reference['labels'])\n",
    "            batch = batch[0].to(device)\n",
    "            # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                probs = model(batch)\n",
    "                # print(\"outputs: \", probs)   \n",
    "                depth = get_depth(probs)    \n",
    "                probs_list = unflatten_tuple(probs, depth)            \n",
    "            break\n",
    "        return len(probs_list), depth\n",
    "    \n",
    "    def eval_model():\n",
    "        #############################\n",
    "        # 정확도 계산 위한 변수 정의\n",
    "        #############################        \n",
    "        test_batch_num = len(test_dataloader) # 총 배치 숫자\n",
    "        num_models , depth = get_num_model() # 총 모델안의 분류기 수\n",
    "        print(\"# of Moddels: \", num_models)\n",
    "        \n",
    "        total_correct = np.zeros((num_models,1)) # 통계를 내기 위해 각 모델마다 할당\n",
    "        \n",
    "        correct = 0 \n",
    "        output_list = []\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for batch, reference in zip(plm_vector, test_dataloader):\n",
    "            # print(reference['labels'])\n",
    "            # batch = batch[0].to(device)\n",
    "            # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                probs = model(batch)\n",
    "                # print(\"outputs: \", probs)   \n",
    "                #depth = get_depth(probs)    \n",
    "                probs_list = unflatten_tuple(probs, depth)            \n",
    "                # print(\"probs_list: \", probs_list)\n",
    "\n",
    "                ground_truth = reference[\"labels\"].to(device)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"Ground_Truth: \\n\", ground_truth, \"\\n\")            \n",
    "                # print(\"outputs: \", outputs.shape)   \n",
    "\n",
    "                # 각 모델 마다 correct 수를 구함.\n",
    "                for i, pred in enumerate(probs_list):\n",
    "\n",
    "                    correct += (pred.argmax(1) == ground_truth).type(torch.float).sum().item()\n",
    "                    total_correct[i] +=correct                \n",
    "                    correct /= test_batch_size\n",
    "\n",
    "                    correct = 0\n",
    "\n",
    "                    if verbose:\n",
    "                        print(f\"From model_0{i+1} - Predicted Label:\")                \n",
    "                        print(pred.argmax(1))\n",
    "\n",
    "                        print(f\"From model_0{i+1} Accuracy: {(100*correct):>0.2f}% \\n\")\n",
    "\n",
    "\n",
    "        # 전체 배치에 대한 모델 마다 정확도 구함.\n",
    "        num_total_payload = test_batch_num * test_batch_size                \n",
    "        for i in range(num_models):\n",
    "            total_correct[i] /= num_total_payload    \n",
    "            #print(\" total_correct[i] : \",         total_correct[i])\n",
    "            print(f\"From model_0{i+1} Accuracy: {(100*total_correct[i][0]):>0.2f}% \\n\")    \n",
    "    \n",
    "    \n",
    "    eval_model()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f334bff4-f08a-4e43-ac6a-c89ad7e5c4d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Moddels:  2\n",
      "From model_01 Accuracy: 75.73% \n",
      "\n",
      "From model_02 Accuracy: 75.69% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_list = inference_classifier2(Combine_Classifier_02, plm_vector, test_dataloader, test_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6fb79b-8194-453f-ab9b-82d1e3bcbd6d",
   "metadata": {},
   "source": [
    "# 9. 세번째 모델 생성 및 통합 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6570d47c-88a7-42c5-9766-f355ff1c4415",
   "metadata": {},
   "source": [
    "## 9.1. 세번째 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4db214fe-f5a7-4208-ba66-b2eff69f78a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7fb06f1a0eb472c8b27ec7255f55df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4cab8a3b514a9793883715fc0e4f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval dataset accuracy in training loop\n",
      "{'accuracy': 0.7421052631578947}\n",
      "{'accuracy': 0.7568421052631579}\n",
      "inference accuracy for plm plus classifer\n",
      "{'accuracy': 0.7653454928095406}\n",
      "classifier architecture: \n",
      "name : classifier\n",
      "inference accuracy for classifer\n",
      "{'accuracy': 0.7653454928095406}\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "# 모델 훈련\n",
    "custom_model =CustomBERTModel(model = plm ,num_labels=2).to(device)\n",
    "optimizer, lr_scheduler, num_training_steps = create_optimizer_scheduler(num_epochs, custom_model, train_dataloader)\n",
    "progress_bar_train = tqdm(range(num_training_steps))\n",
    "progress_bar_eval = tqdm(range(num_epochs * len(eval_dataloader)))\n",
    "\n",
    "\n",
    "def create_bert_model(num_epochs, custom_model):\n",
    "    print(\"eval dataset accuracy in training loop\")\n",
    "    custom_model = train_loop(num_epochs, custom_model, train_dataloader, progress_bar_train, \\\n",
    "                   eval_dataloader, progress_bar_eval, metric, optimizer, lr_scheduler)      \n",
    "\n",
    "    # 모델 평가\n",
    "    print(\"inference accuracy for plm plus classifer\")    \n",
    "    evaL_model(custom_model, test_dataloader, metric)    \n",
    "\n",
    "    # Classifier 추출\n",
    "    classifier = ClassifierModel(base_model = custom_model ,num_labels=2).to(device)\n",
    "\n",
    "    # Classifier 모델 구조 확인\n",
    "    print(\"classifier architecture: \")        \n",
    "    show_module(classifier)\n",
    "\n",
    "    # plm vector 추출\n",
    "    plm_vector = inference_plm(PL_Model, test_dataloader)\n",
    "    # print(\"batch size: \" , len(plm_vector))\n",
    "    # print(\"one batch shape: \" , plm_vector[0][0].shape)\n",
    "\n",
    "    # Classifier 로 추론\n",
    "    print(\"inference accuracy for classifer\")\n",
    "    output_list = inference_classifier(classifier, plm_vector, test_dataloader)\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "    \n",
    "classifier_03 = create_bert_model(num_epochs, custom_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b815551-9774-4f9f-81ab-4822f295fe3b",
   "metadata": {},
   "source": [
    "## 9.2. 세 번째 모델(classifier_03) 을 기존의 모델 (classifier_01, classifier_02) 로 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d4cd6383-61b3-43da-a8f6-4831d84f9152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : base_classifier\n",
      "name : add_classifier\n"
     ]
    }
   ],
   "source": [
    "Combine_Classifier_03 = CombineClassifier(base_classifier=Combine_Classifier_02 ,\n",
    "                                          add_classifier=classifier_03).to(device)\n",
    "show_module(Combine_Classifier_03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cde72e50-3b4a-40a1-a629-77c0d3dcf509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CombineClassifier(\n",
      "  (base_classifier): CombineClassifier(\n",
      "    (base_classifier): ClassifierModel(\n",
      "      (classifier): Sequential(\n",
      "        (0): Dropout(p=0.1, inplace=False)\n",
      "        (1): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=256, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (add_classifier): ClassifierModel(\n",
      "      (classifier): Sequential(\n",
      "        (0): Dropout(p=0.1, inplace=False)\n",
      "        (1): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=256, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (add_classifier): ClassifierModel(\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=256, bias=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): Linear(in_features=256, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(Combine_Classifier_03)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbcf3d1-7d02-4691-aeb9-e49e4e28c9a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9.3. 통합 모델 (classifier_01, classifier_02, classifier_03) 을 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "012a04fd-9138-49ca-9342-2e3d2852bada",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Moddels:  3\n",
      "From model_01 Accuracy: 75.73% \n",
      "\n",
      "From model_02 Accuracy: 75.69% \n",
      "\n",
      "From model_03 Accuracy: 75.76% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_list = inference_classifier2(Combine_Classifier_03, plm_vector, test_dataloader, test_batch_size, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021a1517-4351-464c-bd76-a3febb982e99",
   "metadata": {},
   "source": [
    "# 10. 4번째 모델 생성 및 통합 모델 (classifier_01, classifier_02, classifier_03, classifier_04) 을 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3cc3004d-2a34-492f-bdf1-eec90c35d8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) Create Bert MOdel (plm + classifier)\n",
      "1828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c3558dba914fe9a45c886d1b75ca2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1828 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5340e170361c4267afc3a769ab838864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/716 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval dataset accuracy in training loop\n",
      "{'accuracy': 0.7410526315789474}\n",
      "{'accuracy': 0.7740350877192983}\n",
      "{'accuracy': 0.771578947368421}\n",
      "{'accuracy': 0.775438596491228}\n",
      "inference accuracy for plm plus classifer\n",
      "{'accuracy': 0.7930550683970536}\n",
      "classifier architecture: \n",
      "name : classifier\n",
      "inference accuracy for classifer\n",
      "{'accuracy': 0.7930550683970536}\n",
      "\n",
      "(2) Create a group of four classifiers\n",
      "\n",
      "(3) Look at the architecture\n",
      "name : base_classifier\n",
      "name : add_classifier\n",
      "CombineClassifier(\n",
      "  (base_classifier): CombineClassifier(\n",
      "    (base_classifier): CombineClassifier(\n",
      "      (base_classifier): ClassifierModel(\n",
      "        (classifier): Sequential(\n",
      "          (0): Dropout(p=0.1, inplace=False)\n",
      "          (1): Linear(in_features=768, out_features=256, bias=True)\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Linear(in_features=256, out_features=2, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (add_classifier): ClassifierModel(\n",
      "        (classifier): Sequential(\n",
      "          (0): Dropout(p=0.1, inplace=False)\n",
      "          (1): Linear(in_features=768, out_features=256, bias=True)\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Linear(in_features=256, out_features=2, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (add_classifier): ClassifierModel(\n",
      "      (classifier): Sequential(\n",
      "        (0): Dropout(p=0.1, inplace=False)\n",
      "        (1): Linear(in_features=768, out_features=256, bias=True)\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Linear(in_features=256, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (add_classifier): ClassifierModel(\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=256, bias=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): Linear(in_features=256, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "(4) Inference the group of 4 classifier\n",
      "# of Moddels:  4\n",
      "From model_01 Accuracy: 75.73% \n",
      "\n",
      "From model_02 Accuracy: 75.69% \n",
      "\n",
      "From model_03 Accuracy: 75.76% \n",
      "\n",
      "From model_04 Accuracy: 78.51% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"(1) Create Bert MOdel (plm + classifier)\")\n",
    "\n",
    "num_epochs = 4\n",
    "custom_model =CustomBERTModel(model = plm ,num_labels=2).to(device)\n",
    "optimizer, lr_scheduler, num_training_steps = create_optimizer_scheduler(num_epochs, custom_model, train_dataloader)\n",
    "progress_bar_train = tqdm(range(num_training_steps))\n",
    "progress_bar_eval = tqdm(range(num_epochs * len(eval_dataloader)))\n",
    "classifier_04 = create_bert_model(num_epochs, custom_model)\n",
    "\n",
    "print(\"\\n(2) Create a group of four classifiers\")\n",
    "Combine_Classifier_04 = CombineClassifier(base_classifier=Combine_Classifier_03 ,\n",
    "                                          add_classifier=classifier_04).to(device)\n",
    "print(\"\\n(3) Look at the architecture\")\n",
    "show_module(Combine_Classifier_04)\n",
    "print(Combine_Classifier_04)\n",
    "print(\"\\n(4) Inference the group of 4 classifier\")\n",
    "output_list = inference_classifier2(Combine_Classifier_04, plm_vector, test_dataloader, test_batch_size, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4523038b-fb36-4d06-873d-681551c7d58f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# E. 커널 리스타팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda480f7-c9dd-422f-a6ac-61557b7ff5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import IPython\n",
    "\n",
    "# IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26f24a7-3514-4e2d-94ba-332ef6a0960b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1dcebd-c78c-4f9b-af5b-4ca56769d543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cfd3ef-7500-4991-92b0-003026bfe3cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
